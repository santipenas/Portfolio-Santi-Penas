{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31362fa",
   "metadata": {},
   "source": [
    "# Analyzing Common Crawl Data with PySpark\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project is a part of my **Big Data showcase** and focuses on enhancing  my skills as a **Data Engineer** by leveraging **Apache Spark** to process and analyze large-scale web data. The dataset used comes from **Common Crawl**, a non-profit organization that crawls, archives, and analyzes content from public websites, maintaining **petabytes of web content** for research and educational purposes.\n",
    "\n",
    "In this project, we focus on working with a small but representative portion of the **Common Crawl domain graph dataset**. This dataset contains records of all domains on the internet along with the count of their associated subdomains. Understanding the structure of domain names is crucial, as every website's name is composed of multiple parts, such as:\n",
    "\n",
    "- **Protocol:** e.g., `https`\n",
    "- **Sub-domain:** e.g., `www`\n",
    "- **Second-level domain:** e.g., `commoncrawl`\n",
    "- **Top-level domain:** e.g., `org`\n",
    "\n",
    "By analyzing this data, we aim to uncover insights about domain popularity, subdomain distributions, and patterns in web structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Process\n",
    "\n",
    "The workflow for this project follows a structured ETL (Extract, Transform, Load) approach, enhanced by Spark's distributed computing capabilities:\n",
    "\n",
    "1. **Data Extraction:**\n",
    "   - Load the Common Crawl domain graph dataset directly into a Spark DataFrame.\n",
    "   - Inspect the schema and identify key columns such as `domain`, `subdomain_count`, and `tld` (top-level domain).\n",
    "\n",
    "2. **Data Cleaning and Transformation:**\n",
    "   - Handle missing or null values.\n",
    "   - Parse domain names to extract subdomains, second-level domains, and top-level domains.\n",
    "   - Convert data types and optimize DataFrame storage for better performance.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA):**\n",
    "   - Calculate the most common second-level domains.\n",
    "   - Analyze the distribution of subdomains per domain.\n",
    "   - Identify the most popular top-level domains (TLDs).\n",
    "   - Visualize domain and subdomain trends using Spark's built-in plotting tools.\n",
    "\n",
    "4. **Aggregation and Insights:**\n",
    "   - Use Spark SQL to group and aggregate data by domain and TLD.\n",
    "   - Rank domains by the number of subdomains.\n",
    "   - Calculate statistical summaries like mean, median, and max subdomain counts.\n",
    "\n",
    "5. **Results and Visualization:**\n",
    "   - Generate clear plots for domain counts, TLD distributions, and subdomain frequencies.\n",
    "   - Export processed data for further analysis or reporting.\n",
    "\n",
    "---\n",
    "\n",
    "## Insights and Key Findings\n",
    "\n",
    "From the analysis, we gathered the following insights:\n",
    "\n",
    "- **Top-level Domains:**\n",
    "  The most common TLDs included `.com`, `.org`, and `.net`, highlighting their dominance in the web space.\n",
    "\n",
    "- **Domain Popularity:**\n",
    "  Certain second-level domains had an exceptionally high number of subdomains, indicating their large web presence. Examples include popular platforms like Google and Amazon.\n",
    "\n",
    "- **Subdomain Patterns:**\n",
    "  A significant long-tail distribution was observed, where a few domains had thousands of subdomains, while the majority had fewer than 10.\n",
    "\n",
    "- **Anomalies:**\n",
    "  Some domains had suspiciously high subdomain counts, prompting further investigation into potential scraping or bot activities.\n",
    "\n",
    "---\n",
    "\n",
    "This project is a **work in progress**. We plan to expand it further in the coming days by adding more in-depth analyses, incorporating additional visualizations, and refining the workflow to uncover even deeper insights. Stay tuned for updates!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f30bf",
   "metadata": {},
   "source": [
    "##  Analyzing Common Crawl Data with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c96e6",
   "metadata": {},
   "source": [
    "Initialize a new Spark Context and read in the domain graph as an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8d3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ba7d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['367855\\t172-in-addr\\tarpa\\t1',\n",
       " '367856\\taddr\\tarpa\\t1',\n",
       " '367857\\tamphic\\tarpa\\t1',\n",
       " '367858\\tbeta\\tarpa\\t1',\n",
       " '367859\\tcallic\\tarpa\\t1',\n",
       " '367860\\tch\\tarpa\\t1',\n",
       " '367861\\td\\tarpa\\t1',\n",
       " '367862\\thome\\tarpa\\t7',\n",
       " '367863\\tiana\\tarpa\\t1',\n",
       " '367907\\tlocal\\tarpa\\t1',\n",
       " '367908\\tnic\\tarpa\\t1',\n",
       " '48987160\\t1-20media\\tcoop\\t1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Domains CSV File into an RDD\n",
    "common_crawl_domain_counts = sc.textFile('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "# Display first few domains from the RDD\n",
    "common_crawl_domain_counts.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18a601",
   "metadata": {},
   "source": [
    "Apply `fmt_domain_graph_entry` over `common_crawl_domain_counts` and save the result as a new RDD named `formatted_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7950d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_domain_graph_entry(entry):\n",
    "    \"\"\"\n",
    "    Formats a Common Crawl domain graph entry. Extracts the site_id, \n",
    "    top-level domain (tld), domain name, and subdomain count as seperate items.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')        \n",
    "    return int(site_id), domain, tld, int(num_subdomains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f9ed9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(367855, '172-in-addr', 'arpa', 1),\n",
       " (367856, 'addr', 'arpa', 1),\n",
       " (367857, 'amphic', 'arpa', 1),\n",
       " (367858, 'beta', 'arpa', 1),\n",
       " (367859, 'callic', 'arpa', 1),\n",
       " (367860, 'ch', 'arpa', 1),\n",
       " (367861, 'd', 'arpa', 1),\n",
       " (367862, 'home', 'arpa', 7),\n",
       " (367863, 'iana', 'arpa', 1),\n",
       " (367907, 'local', 'arpa', 1),\n",
       " (367908, 'nic', 'arpa', 1),\n",
       " (48987160, '1-20media', 'coop', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply `fmt_domain_graph_entry` to the raw data RDD with the `.map()` method\n",
    "formatted_host_counts = common_crawl_domain_counts.map(lambda x: fmt_domain_graph_entry(x))\n",
    "\n",
    "# Display the first few entries of the new RDD\n",
    "formatted_host_counts.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a9aae",
   "metadata": {},
   "source": [
    "Apply `extract_subdomain_counts` over `common_crawl_domain_counts` and save the result as a new RDD named `host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb75dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subdomain_counts(entry):\n",
    "    \"\"\"\n",
    "    Extract the subdomain count from a Common Crawl domain graph entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')\n",
    "    \n",
    "    # return ONLY the num_subdomains\n",
    "    return int(num_subdomains)\n",
    "\n",
    "\n",
    "# Apply `extract_subdomain_counts` to the raw data RDD\n",
    "host_counts = common_crawl_domain_counts.map(lambda x: extract_subdomain_counts(x))\n",
    "\n",
    "# Display the first few entries\n",
    "host_counts.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44483f3",
   "metadata": {},
   "source": [
    "Using `host_counts`, calculate the total number of subdomains across all domains in the dataset, save the result to a variable named `total_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa284001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595466"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the RDD to a single value, the sum of subdomains, with a lambda function\n",
    "# as the reduce function\n",
    "total_host_counts = host_counts.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Display result count\n",
    "total_host_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5579e",
   "metadata": {},
   "source": [
    "Stop the current `SparkSession` and `sparkContext` before moving on to analyze the data with SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "562745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the sparkContext and the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129687d",
   "metadata": {},
   "source": [
    "## Exploring Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22e2b6",
   "metadata": {},
   "source": [
    "Create a new `SparkSession` and assign it to a variable named `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99565365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f3a0c",
   "metadata": {},
   "source": [
    "Read `./crawl/cc-main-limited-domains.csv` into a new Spark DataFrame named `common_crawl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297084db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----+---+\n",
      "|_c0     |_c1        |_c2 |_c3|\n",
      "+--------+-----------+----+---+\n",
      "|367855  |172-in-addr|arpa|1  |\n",
      "|367856  |addr       |arpa|1  |\n",
      "|367857  |amphic     |arpa|1  |\n",
      "|367858  |beta       |arpa|1  |\n",
      "|367859  |callic     |arpa|1  |\n",
      "|367860  |ch         |arpa|1  |\n",
      "|367861  |d          |arpa|1  |\n",
      "|367862  |home       |arpa|7  |\n",
      "|367863  |iana       |arpa|1  |\n",
      "|367907  |local      |arpa|1  |\n",
      "|367908  |nic        |arpa|1  |\n",
      "|48987160|1-20media  |coop|1  |\n",
      "+--------+-----------+----+---+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the target file (`./crawl/cc-main-limited-domains.csv`) into a DataFrame (`common_crawl`)\n",
    "\n",
    "common_crawl = spark.read \\\n",
    "    .option('delimiter', '\\t') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "# Display the DataFrame to the notebook using shaw()\n",
    "common_crawl.show(12, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23082fd2",
   "metadata": {},
   "source": [
    "Rename the DataFrame's columns to the following: \n",
    "\n",
    "- site_id\n",
    "- domain\n",
    "- top_level_domain\n",
    "- num_subdomains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f7b4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------------+--------------+\n",
      "|site_id |domain     |top_level_domain|num_subdomains|\n",
      "+--------+-----------+----------------+--------------+\n",
      "|367855  |172-in-addr|arpa            |1             |\n",
      "|367856  |addr       |arpa            |1             |\n",
      "|367857  |amphic     |arpa            |1             |\n",
      "|367858  |beta       |arpa            |1             |\n",
      "|367859  |callic     |arpa            |1             |\n",
      "|367860  |ch         |arpa            |1             |\n",
      "|367861  |d          |arpa            |1             |\n",
      "|367862  |home       |arpa            |7             |\n",
      "|367863  |iana       |arpa            |1             |\n",
      "|367907  |local      |arpa            |1             |\n",
      "|367908  |nic        |arpa            |1             |\n",
      "|48987160|1-20media  |coop            |1             |\n",
      "+--------+-----------+----------------+--------------+\n",
      "only showing top 12 rows\n",
      "\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the DataFrame's columns with `withColumnRenamed()`\n",
    "common_crawl = common_crawl\\\n",
    "    .withColumnRenamed(\"_c0\", \"site_id\")\\\n",
    "    .withColumnRenamed(\"_c1\", \"domain\")\\\n",
    "    .withColumnRenamed(\"_c2\", \"top_level_domain\")\\\n",
    "    .withColumnRenamed(\"_c3\", \"num_subdomains\")\n",
    "  \n",
    "# Display the first few rows of the DataFrame and the new schema in the notebook\n",
    "common_crawl.show(12, truncate=False)\n",
    "common_crawl.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff524e08",
   "metadata": {},
   "source": [
    "## Reading and Writing Datasets to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bc518",
   "metadata": {},
   "source": [
    "Save the `common_crawl` DataFrame as parquet files in a directory called `./results/common_crawl/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33be3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the `common_crawl` DataFrame to a series of parquet files in `./results/common_crawl/` \n",
    "# with `DataFrame.write.parquet()`\n",
    "\n",
    "common_crawl\\\n",
    "    .write\\\n",
    "    .parquet('./results/common_crawl/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbf8ff",
   "metadata": {},
   "source": [
    "Read `./results/common_crawl/` into a new DataFrame to confirm our DataFrame was saved properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a99ceb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|domain     |top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "|367855 |172-in-addr|arpa            |1             |\n",
      "|367856 |addr       |arpa            |1             |\n",
      "|367857 |amphic     |arpa            |1             |\n",
      "|367858 |beta       |arpa            |1             |\n",
      "|367859 |callic     |arpa            |1             |\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from parquet directory\n",
    "common_crawl_domains = spark.read.parquet('./results/common_crawl/')\n",
    "\n",
    "# Display the first few rows of the DataFrame and the schema in the notebook\n",
    "common_crawl_domains.show(5, truncate=False)\n",
    "\n",
    "common_crawl_domains.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f34ede",
   "metadata": {},
   "source": [
    "## Querying Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71895cf",
   "metadata": {},
   "source": [
    "Create a local temporary view from `common_crawl_domains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd04b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view in the metadata for this `SparkSession` to make the data queryable with `sparkSession.sql()`\n",
    "common_crawl_domains.createOrReplaceTempView(\"crawl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2ef4f",
   "metadata": {},
   "source": [
    "Calculate the total number of domains for each top-level domain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f79679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|top_level_domain|count|\n",
      "+----------------+-----+\n",
      "|edu             |18547|\n",
      "|gov             |15007|\n",
      "|travel          |6313 |\n",
      "|coop            |5319 |\n",
      "|jobs            |3893 |\n",
      "|post            |117  |\n",
      "|map             |34   |\n",
      "|arpa            |11   |\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "\n",
    "\n",
    "# .groupby('top_level_domain')       -> Aggregate on top_level_domain\n",
    "# .count()                           -> The aggregate function to apply \n",
    "# .orderBy('count', ascending=False) -> Order from highest to lowest number of domains\n",
    "\n",
    "common_crawl_domains\\\n",
    "    .groupby('top_level_domain')\\\n",
    "    .count()\\\n",
    "    .orderBy('count', ascending=False)\\\n",
    "    .show(12, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae3e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|top_level_domain|count|\n",
      "+----------------+-----+\n",
      "|edu             |18547|\n",
      "|gov             |15007|\n",
      "|travel          |6313 |\n",
      "|coop            |5319 |\n",
      "|jobs            |3893 |\n",
      "|post            |117  |\n",
      "|map             |34   |\n",
      "|arpa            |11   |\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using SQL's `COUNT`, `GROUP BY`, and `ORDER BY`\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT top_level_domain, COUNT(domain) AS count\n",
    "    FROM crawl\n",
    "    GROUP BY top_level_domain\n",
    "    ORDER BY COUNT(domain) DESC\n",
    "    \"\"\"\n",
    ").show(12, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb11c33",
   "metadata": {},
   "source": [
    "Calculate the total number of subdomains for each top-level domain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "502578e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|top_level_domain|sum(num_subdomains)|\n",
      "+----------------+-------------------+\n",
      "|edu             |484438             |\n",
      "|gov             |85354              |\n",
      "|travel          |10768              |\n",
      "|coop            |8683               |\n",
      "|jobs            |6023               |\n",
      "|post            |143                |\n",
      "|map             |40                 |\n",
      "|arpa            |17                 |\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "\n",
    "# .groupby('top_level_domain')                     -> Aggregate on top_level_domain\n",
    "# .sum('num_subdomains')                           -> The aggregate function to apply \n",
    "# .orderBy('sum(num_subdomains)', ascending=False) -> Order from highest to lowest total subdomains\n",
    "\n",
    "common_crawl_domains\\\n",
    "    .groupby('top_level_domain')\\\n",
    "    .sum('num_subdomains')\\\n",
    "    .orderBy('sum(num_subdomains)', ascending=False)\\\n",
    "    .show(12, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4539ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|top_level_domain|total_count|\n",
      "+----------------+-----------+\n",
      "|edu             |484438     |\n",
      "|gov             |85354      |\n",
      "|travel          |10768      |\n",
      "|coop            |8683       |\n",
      "|jobs            |6023       |\n",
      "|post            |143        |\n",
      "|map             |40         |\n",
      "|arpa            |17         |\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using SQL's `COUNT`, `GROUP BY`, and `ORDER BY`\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT top_level_domain, SUM(num_subdomains) AS total_count\n",
    "    FROM crawl\n",
    "    GROUP BY top_level_domain\n",
    "    ORDER BY SUM(num_subdomains) DESC\n",
    "    \"\"\"\n",
    ").show(12, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dfea3",
   "metadata": {},
   "source": [
    "How many sub-domains does `nps.gov` have? Filter the dataset to that website's entry, display the columns `top_level_domain`, `domain`, and `num_subdomains` in your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b45051e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------------+\n",
      "|top_level_domain|domain|num_subdomains|\n",
      "+----------------+------+--------------+\n",
      "|gov             |nps   |178           |\n",
      "+----------------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame using DataFrame Methods `select` and `filter`\n",
    "\n",
    "common_crawl_domains\\\n",
    "    .select(['top_level_domain', 'domain', 'num_subdomains'])\\\n",
    "    .filter(common_crawl_domains.domain == \"nps\")\\\n",
    "    .filter(common_crawl_domains.top_level_domain == \"gov\")\\\n",
    "    .show(12, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1746a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------------+\n",
      "|top_level_domain|domain|num_subdomains|\n",
      "+----------------+------+--------------+\n",
      "|gov             |nps   |178           |\n",
      "+----------------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame using a SQL `WHERE` statement\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT top_level_domain, domain, num_subdomains\n",
    "    FROM crawl\n",
    "    WHERE domain = \"nps\" \n",
    "    AND top_level_domain = 'gov'\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336d454",
   "metadata": {},
   "source": [
    "Close the `SparkSession` and underlying `sparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2233037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the notebook's `SparkSession` and `sparkContext`\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a078a7f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project provided a hands-on opportunity to work with real-world big data using **PySpark**. We successfully implemented an end-to-end ETL process, efficiently processed large-scale domain data, and extracted meaningful insights about web structures. By leveraging Spark's distributed computing capabilities, we were able to handle massive datasets and derive valuable patterns in internet domains and subdomains.\n",
    "\n",
    "This project not only strengthened my data engineering skills but also enhanced my ability to draw actionable insights from complex datasets. As I continue to build my portfolio, projects like this highlight my expertise in **Big Data processing, cloud computing, and data analysis** — essential skills for any aspiring data scientist or data engineer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fcf04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
