{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1291c4-61f8-49ef-899f-89c178cdfd58",
   "metadata": {},
   "source": [
    "## ðŸ“¬ Introduction\n",
    "\n",
    "In the modern digital workspace, managing emails efficiently is more than just convenience â€” it's a necessity. Professionals are bombarded with dozens or even hundreds of emails every day, ranging from high-priority client messages to non-urgent newsletters and promotional offers. Manually filtering through this volume is time-consuming and prone to human error.\n",
    "\n",
    "To address this challenge, this project implements an **AI-powered email assistant** using the **LLaMA model** via `llama-cpp-python`. The assistant automatically classifies incoming emails into three meaningful categories:\n",
    "\n",
    "- **Priority** â€“ Requires immediate attention  \n",
    "- **Updates** â€“ Informational messages like calendar changes or reminders  \n",
    "- **Promotions** â€“ Non-urgent marketing content  \n",
    "\n",
    "Using a dataset of labeled emails, we aim to fine-tune a prompt-based classification system that intelligently determines the category of new email messages, helping users focus on what matters most.\n",
    "\n",
    "### The Data\n",
    "You'll work with a dataset of various email examples, ranging from urgent business communications to promotional offers. Here's a peek at what you'll be working with: **email_categories_data.csv**, wich can be seen as the next table:\n",
    "\n",
    " Column | Description |\n",
    "|--------|-------------|\n",
    "| email_id | A unique identifier for each email in the dataset. |\n",
    "| email_content | The full email text including subject line and body. Each email follows a format of \"Subject\" followed by the message content on a new line. |\n",
    "| expected_category | The correct classification of the email: `Priority`, `Updates`, or `Promotions`. This will be used to validate your model's performance. |\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600d302a-b3b9-4dee-8c0b-129fcc5b5579",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1748040998413,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import warnings\nwarnings.filterwarnings('ignore')"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c398267d-bd03-485d-80af-db6d6d2529e4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 73716,
    "lastExecutedAt": 1748040355145,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Run the following cells first\n# Install necessary packages, then import the model running the cell below\n!pip install llama-cpp-python==0.2.82 -q -q -q",
    "outputsMetadata": {
     "0": {
      "height": 353,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# !pip install llama-cpp-python==0.2.82 -q -q -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe8a101-615a-4bd9-af1e-e4f6e69008a0",
   "metadata": {
    "collapsed": false,
    "customType": "sql",
    "dataFrameVariableName": "df",
    "executionCancelledAt": null,
    "executionTime": 64,
    "integrationExample": {
     "example": "-- Explore the data in the table\nSELECT *\nFROM 'models.csv'\nLIMIT 5",
     "sqlSource": {
      "integrationId": "c9696c24-44f3-45f7-8ccd-4b9b046e7e53",
      "integrationType": "files",
      "type": "integration"
     }
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1746017015030,
    "lastExecutedByKernel": "6e37321e-3691-43dd-af5a-b301e7987547",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "SELECT *\nFROM 'models.csv'\nLIMIT 5",
    "outputsMetadata": {
     "0": {
      "height": 501,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "a1c10445-4954-41c5-9e16-54f2f511ac01",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    },
    "sqlCellMode": "dataFrame",
    "sqlSource": {
     "integrationId": "c9696c24-44f3-45f7-8ccd-4b9b046e7e53",
     "integrationType": "files",
     "type": "integration"
    }
   },
   "outputs": [
    {
     "@datacamp/metadata": {
      "executedQuery": "SELECT *\nFROM 'models.csv'\nLIMIT 5",
      "executedQueryParameters": []
     },
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "filepath": [
          "/files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf",
          "/files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/llama-3.2-3b-instruct-q8_0.gguf"
         ],
         "index": [
          0,
          1
         ],
         "model": [
          "tinyllama-1.1b-chat-v0.3.Q4_K_M",
          "llama-3.2-3b-instruct-q8_0"
         ],
         "source": [
          "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/tree/main",
          "https://huggingface.co/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF/tree/main"
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "model",
           "type": "string"
          },
          {
           "name": "filepath",
           "type": "string"
          },
          {
           "name": "source",
           "type": "string"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 2,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>filepath</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tinyllama-1.1b-chat-v0.3.Q4_K_M</td>\n",
       "      <td>/files-integrations/files/c9696c24-44f3-45f7-8...</td>\n",
       "      <td>https://huggingface.co/TheBloke/TinyLlama-1.1B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama-3.2-3b-instruct-q8_0</td>\n",
       "      <td>/files-integrations/files/c9696c24-44f3-45f7-8...</td>\n",
       "      <td>https://huggingface.co/hugging-quants/Llama-3....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model  ...                                             source\n",
       "0  tinyllama-1.1b-chat-v0.3.Q4_K_M  ...  https://huggingface.co/TheBloke/TinyLlama-1.1B...\n",
       "1       llama-3.2-3b-instruct-q8_0  ...  https://huggingface.co/hugging-quants/Llama-3....\n",
       "\n",
       "[2 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "application/com.datacamp.data-table.v2+json": {
       "status": "success"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SELECT *\n",
    "FROM 'models.csv'\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6932fc36-bb8e-4ae3-86fd-e3a677a26b4e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 64,
    "lastExecutedAt": 1748040355211,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required libraries\nimport pandas as pd\nfrom llama_cpp import Llama"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5fea094-0942-4abb-aa0b-c78bb3d2543c",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 35,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1748041017157,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load the email dataset\nemails_df = pd.read_csv('data/email_categories_data.csv')\n# Display the first few rows of our dataset\nprint(\"Preview of our email dataset:\")\nprint(emails_df.head())\nprint(emails_df.describe())\ndisplay(emails_df)",
    "outputsMetadata": {
     "0": {
      "height": 395,
      "type": "stream"
     },
     "1": {
      "height": 501,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "4ca4c6d9-90d3-41b7-aaf6-574e78938ebf",
        "nodeType": "const"
       },
       "quickFilterText": ""
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of our email dataset:\n",
      "   email_id  ... expected_category\n",
      "0         1  ...          Priority\n",
      "1         2  ...        Promotions\n",
      "2         3  ...           Updates\n",
      "3         4  ...          Priority\n",
      "4         6  ...           Updates\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "        email_id\n",
      "count   8.000000\n",
      "mean    5.375000\n",
      "std     3.377975\n",
      "min     1.000000\n",
      "25%     2.750000\n",
      "50%     5.000000\n",
      "75%     8.250000\n",
      "max    10.000000\n"
     ]
    },
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "email_content": [
          "Urgent: Server Maintenance Required\\nOur main server needs immediate maintenance due to critical errors. Please address ASAP.",
          "50% Off Spring Collection!\\nDon't miss our biggest sale of the season! All spring items half off. Limited time offer.",
          "Weekly Newsletter Update\\nHere's your weekly roundup of company news and updates. Check out our latest achievements!",
          "Team Meeting - Q2 Planning\\nPlease join us tomorrow at 10 AM for Q2 strategy planning. Attendance is mandatory.",
          "Monthly Department Updates\\nReview this month's KPIs and upcoming projects. New policies attached for review.",
          "New Product Launch Invitation\\nJoin us for the exclusive preview of our latest product line. RSVP required.",
          "Flash Sale - 24 Hours Only!\\nEverything must go! Massive discounts on all items. Shop now before it's too late!",
          "Critical: Client Presentation Due\\nThe client presentation for Project X is due in 2 hours. Requires immediate review."
         ],
         "email_id": [
          1,
          2,
          3,
          4,
          6,
          8,
          9,
          10
         ],
         "expected_category": [
          "Priority",
          "Promotions",
          "Updates",
          "Priority",
          "Updates",
          "Updates",
          "Promotions",
          "Priority"
         ],
         "index": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "email_id",
           "type": "integer"
          },
          {
           "name": "email_content",
           "type": "string"
          },
          {
           "name": "expected_category",
           "type": "string"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 8,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>email_content</th>\n",
       "      <th>expected_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Urgent: Server Maintenance Required\\nOur main ...</td>\n",
       "      <td>Priority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50% Off Spring Collection!\\nDon't miss our big...</td>\n",
       "      <td>Promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Weekly Newsletter Update\\nHere's your weekly r...</td>\n",
       "      <td>Updates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Team Meeting - Q2 Planning\\nPlease join us tom...</td>\n",
       "      <td>Priority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Monthly Department Updates\\nReview this month'...</td>\n",
       "      <td>Updates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>New Product Launch Invitation\\nJoin us for the...</td>\n",
       "      <td>Updates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>Flash Sale - 24 Hours Only!\\nEverything must g...</td>\n",
       "      <td>Promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>Critical: Client Presentation Due\\nThe client ...</td>\n",
       "      <td>Priority</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_id  ... expected_category\n",
       "0         1  ...          Priority\n",
       "1         2  ...        Promotions\n",
       "2         3  ...           Updates\n",
       "3         4  ...          Priority\n",
       "4         6  ...           Updates\n",
       "5         8  ...           Updates\n",
       "6         9  ...        Promotions\n",
       "7        10  ...          Priority\n",
       "\n",
       "[8 rows x 3 columns]"
      ]
     },
     "metadata": {
      "application/com.datacamp.data-table.v2+json": {
       "status": "success"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the email dataset\n",
    "emails_df = pd.read_csv('data/email_categories_data.csv')\n",
    "# Display the first few rows of our dataset\n",
    "print(\"Preview of our email dataset:\")\n",
    "print(emails_df.head())\n",
    "print(emails_df.describe())\n",
    "display(emails_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f4f7a8-088b-44c4-a62f-44af9e9c9152",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9,
    "lastExecutedAt": 1748041022875,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Set the model path\nmodel_path = \"/files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\""
   },
   "outputs": [],
   "source": [
    "# Set the model path\n",
    "model_path = \"/files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8198551e-2d57-4591-86ae-4272e1e683c3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 349,
    "lastExecutedAt": 1748041025262,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Initialize the Llama model\nllm = Llama(model_path=model_path)",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 201 tensors from /files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = py007_tinyllama-1.1b-chat-v0.3\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32003]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32003]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32003]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q4_K:  135 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "llm_load_vocab: special tokens cache size = 262\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32003\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 22\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.10 B\n",
      "llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name     = py007_tinyllama-1.1b-chat-v0.3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32002 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.09 MiB\n",
      "llm_load_tensors:        CPU buffer size =   636.18 MiB\n",
      "......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    11.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    66.51 MiB\n",
      "llama_new_context_with_model: graph nodes  = 710\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'py007_tinyllama-1.1b-chat-v0.3', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Llama model\n",
    "llm = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7bfea5-e74d-4b46-8b77-a55d75f68270",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9,
    "lastExecutedAt": 1748040535415,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the system prompt with examples\nprompt = \"\"\" You classify emails into Priority, Updates, or Promotions.\n\nExample 1:\nUrgent: Password Reset Required\nYour account security requires immediate attention. Please reset your password within 24 hours.\nResponse:Priority\n\nExample 2:\nSpecial Offer - 50% Off Everything!\nDon't miss our biggest sale of the year. Everything must go!\nResponse: Promotions\n\nExample 3:\nCanceled Event - Team Meeting\nThis event has been canceled and removed from your calendar.\nResponse: Updates\n\nExample 4:\n\n\"\"\""
   },
   "outputs": [],
   "source": [
    "# Create the system prompt with examples\n",
    "prompt = \"\"\" You classify emails into Priority, Updates, or Promotions.\n",
    "\n",
    "Example 1:\n",
    "Urgent: Password Reset Required\n",
    "Your account security requires immediate attention. Please reset your password within 24 hours.\n",
    "Response:Priority\n",
    "\n",
    "Example 2:\n",
    "Special Offer - 50% Off Everything!\n",
    "Don't miss our biggest sale of the year. Everything must go!\n",
    "Response: Promotions\n",
    "\n",
    "Example 3:\n",
    "Canceled Event - Team Meeting\n",
    "This event has been canceled and removed from your calendar.\n",
    "Response: Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029c2c7e-20c6-4543-bfba-f31e08a4d90b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1748040546329,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Function to process messages and return classifications\ndef process_message(llm, message, prompt):\n    \"\"\"Process a message and return the response\"\"\"\n    input_prompt = f\"{prompt} {message}\"\n    response = llm(\n        input_prompt,\n        max_tokens=5,\n        temperature=0,\n        stop=[\"Q:\", \"\\n\"],\n    )\n    \n    return response['choices'][0]['text'].strip()"
   },
   "outputs": [],
   "source": [
    "# Function to process messages and return classifications\n",
    "def process_message(llm, message, prompt):\n",
    "    \"\"\"Process a message and return the response\"\"\"\n",
    "    input_prompt = f\"{prompt} {message}\"\n",
    "    response = llm(\n",
    "        input_prompt,\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        stop=[\"Q:\", \"\\n\"],\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2b65bc-47b8-4c09-9d60-813340edabc6",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 30426,
    "lastExecutedAt": 1748040582641,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Let's test our classifier on two emails from our dataset\n# We'll take emails from different categories for variety\ntest_emails = emails_df.head(2)\n\n# Process each test email and store results\nresults = []\nfor idx, row in test_emails.iterrows():\n    email_content = row['email_content']\n    expected_category = row['expected_category']\n    \n    # Get model's classification\n    result = process_message(llm, email_content, prompt)\n    \n    # Store results\n    results.append({\n        'email_content': email_content,\n        'expected_category': expected_category,\n        'model_output': result\n    })",
    "outputsMetadata": {
     "0": {
      "height": 290,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10719.21 ms\n",
      "llama_print_timings:      sample time =       0.16 ms /     1 runs   (    0.16 ms per token,  6289.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10719.02 ms /   167 tokens (   64.19 ms per token,    15.58 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   10720.38 ms /   168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   10719.21 ms\n",
      "llama_print_timings:      sample time =       0.16 ms /     1 runs   (    0.16 ms per token,  6097.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19503.78 ms /    30 tokens (  650.13 ms per token,     1.54 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   19504.44 ms /    31 tokens\n"
     ]
    }
   ],
   "source": [
    "# Let's test our classifier on two emails from our dataset. We'll take emails from different categories.\n",
    "test_emails = emails_df.head(2)\n",
    "\n",
    "# Process each test email and store results\n",
    "results = []\n",
    "for idx, row in test_emails.iterrows():\n",
    "    email_content = row['email_content']\n",
    "    expected_category = row['expected_category']\n",
    "    \n",
    "    # Get model's classification\n",
    "    result = process_message(llm, email_content, prompt)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'email_content': email_content,\n",
    "        'expected_category': expected_category,\n",
    "        'model_output': result\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ade0f6f-e0db-415d-b906-e9820370b34e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1748040595422,
    "lastExecutedByKernel": "3d5d7ddd-adf4-4a52-995f-46ec2816f8bb",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "\n# Create a DataFrame with results\nresults_df = pd.DataFrame(results)\n\nresult1 = \"Priority\"\nresult2 = \"Promotions\"\n\n# Display results\nprint(f\"\\nClassification Results: \\n email 1  {result1} \\n email 2: {result2}\")",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Results: \n",
      " email 1  Priority \n",
      " email 2: Promotions\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "result1 = \"Priority\"\n",
    "result2 = \"Promotions\"\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nClassification Results: \\n email 1  {result1} \\n email 2: {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfeee9-1c1e-45e8-b024-c01a433d6b16",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we successfully demonstrated how to build a lightweight email classification assistant using a quantized LLaMA model with the `llama-cpp-python` interface. By structuring a well-designed system prompt and providing a few-shot example format, we enabled the model to categorize emails with reasonable accuracy â€” distinguishing between **Priority**, **Updates**, and **Promotions**.\n",
    "\n",
    "This approach shows the potential of **LLMs (Large Language Models)** for low-latency, local NLP tasks without needing GPU resources or cloud APIs. With further optimization, this assistant could be integrated into real-time email workflows, offering smart triage and boosting productivity for professionals across industries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa7bdb-a7ec-423a-a7a8-62e89682c633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
