{"cells":[{"source":"# Building interactive Chatbots with the OpenAI API and Pinecone using a RAG Workflow\n\nIn this project, we dive deep into the exciting realm of AI chatbots. Leveraging LangChain, OpenAI, and Pinecone vector DB, we aim to construct a chatbot that learns and evolves using **R**etrieval **A**ugmented **G**eneration (RAG).\n\nOur journey begins with a dataset sourced from the Llama 2 ArXiv paper and other related research papers. This rich dataset will empower our chatbot to answer questions about the latest advancements in Generative AI (GenAI).\n\nThroughout this project, we explore various concepts and workflows, including:\n\n1. **Data Ingestion and Preprocessing**: We start by ingesting and preprocessing our dataset to ensure it is clean and ready for use.\n2. **Vectorization and Indexing**: Using Pinecone, we vectorize our data and create an efficient index for fast retrieval.\n3. **Integration with OpenAI API**: We integrate our system with the OpenAI API to generate responses based on the retrieved information.\n4. **Building the RAG Pipeline**: We construct a robust RAG pipeline that combines retrieval and generation to provide accurate and informative responses.\n\nThe tech stack and skills we utilize include:\n- **LangChain**: For building and managing the chatbot's conversational flow.\n- **OpenAI API**: For generating human-like responses.\n- **Pinecone**: For efficient vector storage and retrieval.\n- **Python**: As our primary programming language.\n- **NLP and Machine Learning**: Fundamental concepts that underpin our chatbot's functionality.\n\nBy the end of this project, we develop a fully functional chatbot and RAG pipeline capable of engaging in meaningful conversations and providing insightful answers based on a comprehensive knowledge base. This project serves as a stepping stone towards mastering more complex AI systems and gaining hands-on experience in the fields of AI, machine learning, and natural language processing.","metadata":{},"id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","cell_type":"markdown"},{"source":"## Setup","metadata":{},"id":"a9274661-8d8c-4cc5-901e-5fc497866b89","cell_type":"markdown"},{"source":"Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n\n- **openai**: This is the official OpenAI Python client. We'll use it to interact with the GPT large language model.\n- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone vector DB where we will store our chatbot's knowledge base.\n- **langchain**, **langchain-openai**, **langchain-pinecone**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n- **tiktoken**: This is a library from OpenAI that allows you to count the number of tokens in a text string without making an API call.\n- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n\nYou can install these libraries using pip like so:","metadata":{},"id":"2cf847fd-f8f8-49f6-9b43-0eb098239072","cell_type":"markdown"},{"source":"!pip install openai==1.27\n!pip install pinecone-client==4.0.0\n!pip install langchain==0.1.19\n!pip install langchain-openai==0.1.6\n!pip install langchain-pinecone==0.1.0\n!pip install tiktoken==0.7.0\n!pip install datasets==2.19.1\n!pip install typing_extensions==4.11.0","metadata":{"executionCancelledAt":null,"executionTime":27832,"lastExecutedAt":1749212383573,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install openai==1.27\n!pip install pinecone-client==4.0.0\n!pip install langchain==0.1.19\n!pip install langchain-openai==0.1.6\n!pip install langchain-pinecone==0.1.0\n!pip install tiktoken==0.7.0\n!pip install datasets==2.19.1\n!pip install typing_extensions==4.11.0","outputsMetadata":{"0":{"height":579,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"f96e3639-1515-47ce-9cab-21b2c8a43c64","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting openai==1.27\n  Downloading openai-1.27.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (4.8.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.27) (1.7.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (0.27.2)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (2.7.1)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (4.12.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.27) (1.2.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.27) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.27) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.27) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.27) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.27) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.27) (2.18.2)\nDownloading openai-1.27.0-py3-none-any.whl (314 kB)\nInstalling collected packages: openai\nSuccessfully installed openai-1.27.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nCollecting pinecone-client==4.0.0\n  Downloading pinecone_client-4.0.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==4.0.0) (2025.1.31)\nRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==4.0.0) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==4.0.0) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==4.0.0) (2.3.0)\nDownloading pinecone_client-4.0.0-py3-none-any.whl (214 kB)\nInstalling collected packages: pinecone-client\nSuccessfully installed pinecone-client-4.0.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nCollecting langchain==0.1.19\n  Downloading langchain-0.1.19-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (2.0.38)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (3.11.12)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.6.7)\nRequirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.0.38)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.1.53)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.0.2)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.1.147)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (2.7.1)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (2.32.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (8.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (0.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (1.33)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (23.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (0.27.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (3.10.15)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.0.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.19) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.19) (2.18.2)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.19) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.19) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.19) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.19) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.19) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.19) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (4.8.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.0.7)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (3.0.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.2.2)\nDownloading langchain-0.1.19-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: langchain\nSuccessfully installed langchain-0.1.19\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nCollecting langchain-openai==0.1.6\n  Downloading langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.46 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.1.6) (0.1.53)\nRequirement already satisfied: openai<2.0.0,>=1.24.0 in /home/repl/.local/lib/python3.10/site-packages (from langchain-openai==0.1.6) (1.27.0)\nRequirement already satisfied: tiktoken<1,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.1.6) (0.7.0)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (6.0.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (0.1.147)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (2.7.1)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (8.5.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (4.8.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.7.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (0.27.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (4.12.2)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2023.12.25)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2.32.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.2.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (3.10.15)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (1.0.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (2.18.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2.3.0)\nDownloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\nInstalling collected packages: langchain-openai\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nembedchain 0.1.113 requires langchain-openai<0.2.0,>=0.1.7, but you have langchain-openai 0.1.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-openai-0.1.6\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nCollecting langchain-pinecone==0.1.0\n  Downloading langchain_pinecone-0.1.0-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone==0.1.0) (0.1.53)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone==0.1.0) (1.26.4)\nCollecting pinecone-client<4.0.0,>=3.2.2 (from langchain-pinecone==0.1.0)\n  Downloading pinecone_client-3.2.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (6.0.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (0.1.147)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (2.7.1)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (8.5.0)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone==0.1.0) (2025.1.31)\nRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone==0.1.0) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone==0.1.0) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone==0.1.0) (2.3.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (0.27.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (3.10.15)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (2.32.3)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.0.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (2.18.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (4.8.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (3.10)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (3.4.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.2.2)\nDownloading langchain_pinecone-0.1.0-py3-none-any.whl (8.4 kB)\nDownloading pinecone_client-3.2.2-py3-none-any.whl (215 kB)\nInstalling collected packages: pinecone-client, langchain-pinecone\n  Attempting uninstall: pinecone-client\n    Found existing installation: pinecone-client 4.0.0\n    Uninstalling pinecone-client-4.0.0:\n      Successfully uninstalled pinecone-client-4.0.0\nSuccessfully installed langchain-pinecone-0.1.0 pinecone-client-3.2.2\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: tiktoken==0.7.0 in /usr/local/lib/python3.10/dist-packages (0.7.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.7.0) (2023.12.25)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.7.0) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2025.1.31)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nCollecting datasets==2.19.1\n  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (18.1.0)\nCollecting pyarrow-hotfix (from datasets==2.19.1)\n  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (0.70.16)\nCollecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.1)\n  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (0.28.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (6.0.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.1) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.1) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.1) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.1) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.1) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.1) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.1) (1.16.0)\nDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\nDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, fsspec, datasets\nSuccessfully installed datasets-2.19.1 fsspec-2024.3.1 pyarrow-hotfix-0.7\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nCollecting typing_extensions==4.11.0\n  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\nDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\nInstalling collected packages: typing_extensions\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nembedchain 0.1.113 requires langchain-openai<0.2.0,>=0.1.7, but you have langchain-openai 0.1.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed typing_extensions-4.11.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"}]},{"source":"## Building the Chatbot","metadata":{},"id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","cell_type":"markdown"},{"source":"We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To get more familiar with the library let's first create a chatbot _without_ RAG.\n","metadata":{},"id":"b1fcd794-b29c-4010-8be0-651a452b2044","cell_type":"markdown"},{"source":"### Initialize the chat model object.\n\n- *Make sure you have defined the `OPENAI_API_KEY` environment variable and connected it. See the 'Setting up DataLab Integrations' section of getting-started.ipynb.*\n- From the `langchain_openai` package, import `ChatOpenAI`.\n- Initialize a `ChatOpenAI` object with the `gpt-3.5-turbo` model. Assign to `chat`.","metadata":{},"id":"4cc8e24a-bd51-483a-81a6-e0c2d1e02c35","cell_type":"markdown"},{"source":"from langchain_openai import ChatOpenAI\n\nchat = ChatOpenAI(model=\"gpt-3.5-turbo\")","metadata":{"executionCancelledAt":null,"executionTime":2727,"lastExecutedAt":1749212386302,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain_openai import ChatOpenAI\n\nchat = ChatOpenAI(model=\"gpt-3.5-turbo\")","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"f81d6d0e-986b-49f4-94e1-7315a7f0bd67","cell_type":"code","execution_count":3,"outputs":[]},{"source":"### Instructions\n\nCreate a conversation.\n\n- From langchain's schema module, import the three message types: `SystemMessage`, `HumanMessage`, and `AIMessage`.\n- Create a conversation as a list of messages. Assign to `messages`.\n    1. A system message with content `\"You are a helpful assistant.\"`\n    2. A human message with content `\"Hi AI, how are you today?\"`\n    3. An AI message with content `\"I'm great thank you. How can I help you?\"`\n    4. A human message with content `\"I'd like to understand string theory.\"`\n","metadata":{},"id":"889de625-b7f6-493f-81a7-25e30bacb98b","cell_type":"markdown"},{"source":"from langchain.schema import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hi AI, how are you today?\"),\n    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n    HumanMessage(content=\"I'd like to understand string theory.\")\n]","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1749212386355,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.schema import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hi AI, how are you today?\"),\n    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n    HumanMessage(content=\"I'd like to understand string theory.\")\n]","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"8f542ae9-c4a0-41aa-a6b6-45585990c246","cell_type":"code","execution_count":4,"outputs":[]},{"source":"We generate the next response from the AI by passing these messages to the `ChatOpenAI` object. You can call `chat` as though it is a function.","metadata":{},"id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf","cell_type":"markdown"},{"source":"### Chat with GPT.\n\n- Invoke a chat with GPT, passing the messages, and get a response. Assign to `res`.\n- Print the response.","metadata":{},"id":"2a974f3e-c9ae-436c-880b-7634c334a786","cell_type":"markdown"},{"source":"res = chat.invoke(messages)\nres","metadata":{"executionCancelledAt":null,"executionTime":3298,"lastExecutedAt":1749212389654,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"res = chat.invoke(messages)\nres","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"47ad31bf-db2a-444b-a011-26e9336a4e1e","cell_type":"code","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":"AIMessage(content=\"String theory is a theoretical framework in physics that attempts to reconcile quantum mechanics and general relativity. According to string theory, the fundamental building blocks of the universe are not particles, but rather tiny, vibrating strings. These strings can have different vibrational modes, which correspond to different particles and forces in the universe.\\n\\nString theory proposes that there are multiple dimensions beyond the familiar four dimensions of space and time. In fact, there are various versions of string theory, such as superstring theory and M-theory, that attempt to unify all known fundamental forces of nature.\\n\\nOne of the key ideas in string theory is the concept of supersymmetry, which posits a symmetry between particles with integer spin (bosons) and particles with half-integer spin (fermions). Supersymmetry could potentially explain the existence of dark matter and provide a framework for understanding the behavior of particles at very high energies.\\n\\nHowever, it's important to note that string theory is still a work in progress and has not yet been experimentally confirmed. Researchers continue to explore and develop the theory in the hopes of uncovering new insights into the nature of the universe.\", response_metadata={'token_usage': {'completion_tokens': 229, 'prompt_tokens': 53, 'total_tokens': 282, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b015211d-74e8-4c03-9d44-c2140ed454cd-0')"},"metadata":{},"execution_count":5}]},{"source":"Notice that the `AIMessage` object looks a bit like a dictionary. The most important element is `content`, which contains the chat text.","metadata":{},"id":"20487069-0c9e-4480-ba09-a6004686b3ba","cell_type":"markdown"},{"source":"Print only the contents of the response.","metadata":{},"id":"dd3463e9-26e8-4908-ab2c-aa78a147e79f","cell_type":"markdown"},{"source":"# Print the contents of the response\nprint(\"\\nContent Answer:\\n\")\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1749212389708,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Print the contents of the response\nprint(\"\\nContent Answer:\\n\")\nprint(res.content)","outputsMetadata":{"0":{"height":311,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"38e8a99c-58ea-43ac-8918-64d42541a58d","cell_type":"code","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"\nContent Answer:\n\nString theory is a theoretical framework in physics that attempts to reconcile quantum mechanics and general relativity. According to string theory, the fundamental building blocks of the universe are not particles, but rather tiny, vibrating strings. These strings can have different vibrational modes, which correspond to different particles and forces in the universe.\n\nString theory proposes that there are multiple dimensions beyond the familiar four dimensions of space and time. In fact, there are various versions of string theory, such as superstring theory and M-theory, that attempt to unify all known fundamental forces of nature.\n\nOne of the key ideas in string theory is the concept of supersymmetry, which posits a symmetry between particles with integer spin (bosons) and particles with half-integer spin (fermions). Supersymmetry could potentially explain the existence of dark matter and provide a framework for understanding the behavior of particles at very high energies.\n\nHowever, it's important to note that string theory is still a work in progress and has not yet been experimentally confirmed. Researchers continue to explore and develop the theory in the hopes of uncovering new insights into the nature of the universe.\n"}]},{"source":"Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation.","metadata":{},"id":"7090e9e9-880d-4b29-909a-a9653474c8b1","cell_type":"markdown"},{"source":"### Continue the conversation with GPT.\n\n- Append the latest AI response to `messages`.\n- Create a new human message. Assign to `prompt`.\n    - Use the content `\"Why do physicists believe it can produce a 'unified theory'?\"`\n- Append the prompt to messages.","metadata":{},"id":"b25f1b56-22b0-414c-bc56-99ea90820f1a","cell_type":"markdown"},{"source":"messages.append(res)\nmessages","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1749212389760,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"messages.append(res)\nmessages","outputsMetadata":{"0":{"height":505,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"51e66e15-dc5d-4f5e-9db2-680f4c90546f","cell_type":"code","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":"[SystemMessage(content='You are a helpful assistant.'),\n HumanMessage(content='Hi AI, how are you today?'),\n AIMessage(content=\"I'm great thank you. How can I help you?\"),\n HumanMessage(content=\"I'd like to understand string theory.\"),\n AIMessage(content=\"String theory is a theoretical framework in physics that attempts to reconcile quantum mechanics and general relativity. According to string theory, the fundamental building blocks of the universe are not particles, but rather tiny, vibrating strings. These strings can have different vibrational modes, which correspond to different particles and forces in the universe.\\n\\nString theory proposes that there are multiple dimensions beyond the familiar four dimensions of space and time. In fact, there are various versions of string theory, such as superstring theory and M-theory, that attempt to unify all known fundamental forces of nature.\\n\\nOne of the key ideas in string theory is the concept of supersymmetry, which posits a symmetry between particles with integer spin (bosons) and particles with half-integer spin (fermions). Supersymmetry could potentially explain the existence of dark matter and provide a framework for understanding the behavior of particles at very high energies.\\n\\nHowever, it's important to note that string theory is still a work in progress and has not yet been experimentally confirmed. Researchers continue to explore and develop the theory in the hopes of uncovering new insights into the nature of the universe.\", response_metadata={'token_usage': {'completion_tokens': 229, 'prompt_tokens': 53, 'total_tokens': 282, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b015211d-74e8-4c03-9d44-c2140ed454cd-0')]"},"metadata":{},"execution_count":7}]},{"source":"prompt = HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\")\nmessages.append(prompt)","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1749212389807,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"prompt = HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\")\nmessages.append(prompt)","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"be31574f-4801-452e-94a1-a544e50fb4b1","cell_type":"code","execution_count":8,"outputs":[]},{"source":"messages","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1749212389855,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"messages","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"7bc0c721-8eb2-4a20-8491-89a281480ea5","cell_type":"code","execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":"[SystemMessage(content='You are a helpful assistant.'),\n HumanMessage(content='Hi AI, how are you today?'),\n AIMessage(content=\"I'm great thank you. How can I help you?\"),\n HumanMessage(content=\"I'd like to understand string theory.\"),\n AIMessage(content=\"String theory is a theoretical framework in physics that attempts to reconcile quantum mechanics and general relativity. According to string theory, the fundamental building blocks of the universe are not particles, but rather tiny, vibrating strings. These strings can have different vibrational modes, which correspond to different particles and forces in the universe.\\n\\nString theory proposes that there are multiple dimensions beyond the familiar four dimensions of space and time. In fact, there are various versions of string theory, such as superstring theory and M-theory, that attempt to unify all known fundamental forces of nature.\\n\\nOne of the key ideas in string theory is the concept of supersymmetry, which posits a symmetry between particles with integer spin (bosons) and particles with half-integer spin (fermions). Supersymmetry could potentially explain the existence of dark matter and provide a framework for understanding the behavior of particles at very high energies.\\n\\nHowever, it's important to note that string theory is still a work in progress and has not yet been experimentally confirmed. Researchers continue to explore and develop the theory in the hopes of uncovering new insights into the nature of the universe.\", response_metadata={'token_usage': {'completion_tokens': 229, 'prompt_tokens': 53, 'total_tokens': 282, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b015211d-74e8-4c03-9d44-c2140ed454cd-0'),\n HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\")]"},"metadata":{},"execution_count":9}]},{"source":"### We keep going:\n\n- Invoke the chat again to send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"e3628e65-9dde-41b2-bf22-e489fb70694a","cell_type":"markdown"},{"source":"res = chat.invoke(messages)\nres","metadata":{"executionCancelledAt":null,"executionTime":4701,"lastExecutedAt":1749212394556,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"res = chat.invoke(messages)\nres","outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":353,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"4f68e1d4-4dcc-4dbe-b7ce-df321f90ae5c","cell_type":"code","execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":"AIMessage(content='Physicists are interested in string theory as a potential candidate for a unified theory because it has the potential to encompass all known fundamental forces of nature within a single framework. Currently, the four fundamental forces of nature are gravity, electromagnetism, the weak nuclear force, and the strong nuclear force. These forces are described by different theories, such as general relativity for gravity and the Standard Model of particle physics for the other three forces.\\n\\nString theory offers the possibility of unifying these forces by describing them all in terms of the same underlying structure: vibrating strings. By incorporating gravity into the framework of quantum mechanics, string theory aims to provide a consistent description of the universe at both the smallest scales (quantum realm) and the largest scales (cosmological realm).\\n\\nIn addition, string theory naturally incorporates supersymmetry, which could help resolve some of the outstanding problems in particle physics, such as the hierarchy problem and the nature of dark matter. Supersymmetry predicts the existence of partner particles for each known particle, potentially providing a more complete and elegant description of the universe.\\n\\nWhile string theory has not yet been experimentally confirmed, its mathematical consistency and potential for unification make it an attractive candidate for a unified theory of physics. Scientists continue to investigate and develop string theory in the hopes of uncovering new insights into the fundamental nature of the universe.', response_metadata={'token_usage': {'completion_tokens': 272, 'prompt_tokens': 303, 'total_tokens': 575, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0eb9b617-0d95-4fd8-a0ac-70c01196a812-0')"},"metadata":{},"execution_count":10}]},{"source":"## Hallucinations","metadata":{},"id":"c536df54-a985-401e-99e7-212004379618","cell_type":"markdown"},{"source":"## Understanding the Limitations of LLMs and the Concept of Hallucinations\n\nWe have our chatbot, but it's important to understand that the knowledge of Large Language Models (LLMs) can be limited. The reason for this is that LLMs learn all they know during their training phase. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. This knowledge is referred to as the _parametric knowledge_ of the model.\n\nBy default, LLMs have no access to the external world. This means that GPT (or any other LLM) will perform poorly on certain types of questions:\n\n- **Recent Events**: The chatbot doesn't know about recent events. For example, if you ask about the weather in your city today, it won't be able to provide an accurate answer.\n- **Recent Code or Products**: It can't answer questions about recent code or products. Try asking it `\"Can you tell me about the latest features in LangChain?\"` or `\"What was the latest course released regarding LLM concepts?\"` and you'll see it struggle.\n- **Confidential Information**: It can't answer questions about confidential corporate information that hasn't been released on the internet.\n\n### Hallucinations\n\nOne critical issue to be aware of is the phenomenon of _hallucinations_. Hallucinations occur when the model generates information that is not based on its training data or any real-world facts. This can lead to the model providing incorrect or misleading information confidently. For example, the model might fabricate details about a non-existent scientific theory or provide inaccurate statistics.\n\nUnderstanding these limitations and the potential for hallucinations is crucial for effectively using LLMs and interpreting their responses.","metadata":{},"id":"e257b696-4c54-4412-9534-7bf57a47cfc3","cell_type":"markdown"},{"source":"### Append the AI response to the list of messages.\n\n- Print the number of messages in the conversation.\n- Append the latest AI response to `messages`.\n- Print the number of messages in the conversation again.","metadata":{},"id":"61ae0a3c-6d1a-4616-9cf0-871c1dd4b5d5","cell_type":"markdown"},{"source":"print(\"Total number of messages so far:\\n\", len(messages))\n\nmessages.append(res)\n\nprint(\"Total number of messages so far:\\n\", len(messages))","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1749212394603,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(\"Total number of messages so far:\\n\", len(messages))\n\nmessages.append(res)\n\nprint(\"Total number of messages so far:\\n\", len(messages))","outputsMetadata":{"0":{"height":101,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"5af4c597-b1b6-4159-9a86-876789695fab","cell_type":"code","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"Total number of messages so far:\n 6\nTotal number of messages so far:\n 7\n"}]},{"source":"### Ask GPT about Llama 3.\n\n- Create a new human message. Assign to `prompt`.\n    - Use the content `\"What is so special about Llama 3?\"`.\n- Append the prompt to `messages`.\n- Invoke the chat to send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"985b4acd-0c16-457c-837f-50a2a3f2b9d1","cell_type":"markdown"},{"source":"promt = HumanMessage(content=\"What is so special about Llama 3?\")\nmessages.append(promt)\nres = chat.invoke(messages)\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":692,"lastExecutedAt":1749212395295,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"promt = HumanMessage(content=\"What is so special about Llama 3?\")\nmessages.append(promt)\nres = chat.invoke(messages)\nprint(res.content)","outputsMetadata":{"0":{"height":59,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"c3e22ec5-7a01-46a2-96b9-ee2dda555932","cell_type":"code","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"I'm not sure what you are referring to as \"Llama 3.\" Could you provide more context or clarify your question so I can better assist you?\n"}]},{"source":"### Ask GPT about LangChain.\n\n- Append the latest AI response to `messages`.\n- Create a new human message. Assign to `prompt`.\n    - Use the content `\"Can you tell me about the LLMChain in LangChain?\"`.\n- Append the prompt to `messages`.\n- Send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"2fdf6188-f9ad-42fd-b254-cfed65434024","cell_type":"markdown"},{"source":"messages.append(res)\nprompt = HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\nmessages.append(prompt)\nres = chat.invoke(messages)\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":1197,"lastExecutedAt":1749212396492,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"messages.append(res)\nprompt = HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\nmessages.append(prompt)\nres = chat.invoke(messages)\nprint(res.content)","outputsMetadata":{"0":{"height":59,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"13e182b4-3b48-4d28-88dd-27abc9ad4a1c","cell_type":"code","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"I'm sorry, but I am not familiar with a specific technology or concept called \"LLMChain\" in LangChain. It's possible that it may be a specific term or project within a particular context that I am not aware of. If you can provide more information or context about LLMChain and LangChain, I may be able to assist you further.\n"}]},{"source":"There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation.","metadata":{},"id":"ef46c06c-d226-49ce-b467-78408945ff4f","cell_type":"markdown"},{"source":"### Create a string of knowledge about chains.\n\n- *Read the descriptions of LLMChains, Chains, and LangChain given in `llmchain_information`.*\n- Combine the list of description strings into a single string. Assign to `source_knowledge`.","metadata":{},"id":"9e8fb83d-8c94-4d11-b21a-12c455a7f873","cell_type":"markdown"},{"source":"llmchain_information = [\n    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n]\nlen(llmchain_information)","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1749212396540,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"llmchain_information = [\n    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n]\nlen(llmchain_information)","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"711778a5-76d5-40d2-8ed1-2b12a89a474d","cell_type":"code","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{},"execution_count":14}]},{"source":"source_knowledge = \"\\n\".join(llmchain_information)\nsource_knowledge","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1749212396592,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"source_knowledge = \"\\n\".join(llmchain_information)\nsource_knowledge","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"f63762f2-0cb2-48ee-8d51-8fc6b3165fcd","cell_type":"code","execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.'"},"metadata":{},"execution_count":15}]},{"source":"We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query.","metadata":{},"id":"961fe8f0-4b6a-4227-8b40-0ab501c13ee5","cell_type":"markdown"},{"source":"### Feeding extra additional knowledge for better answers and context\n\n- Define a question. Assign to `query`.\n    - Use the text `\"Can you tell me about the LLMChain in LangChain?\"`\n- Create an augmented prompt containing the context and query. We assign to `augmented_prompt` varable.","metadata":{},"id":"a18cd1f5-ace4-4abd-b139-6d6e163e9c3d","cell_type":"markdown"},{"source":"query = \"Can you tell me about the LLMChain in LangChain?\"\n\n\naugmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\n  Contexts: {source_knowledge}\n\n  Query: {query}\"\"\"","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1749212396643,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"query = \"Can you tell me about the LLMChain in LangChain?\"\n\n\naugmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\n  Contexts: {source_knowledge}\n\n  Query: {query}\"\"\"","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"58a71fbf-6b81-42f5-8073-ddfd3139e351","cell_type":"code","execution_count":16,"outputs":[]},{"source":"print(augmented_prompt)","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1749212396692,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(augmented_prompt)","outputsMetadata":{"0":{"height":269,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"99e5faa0-e6dd-43a1-ba33-96fdeea31f9e","cell_type":"code","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\n  Contexts: A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n\n  Query: Can you tell me about the LLMChain in LangChain?\n"}]},{"source":"Now we feed this into our chatbot as we did before.\n\nDon't append the previous AI message, since it wasn't a good answer.","metadata":{},"id":"686b36f9-9107-4099-b0da-22e60e4b17e6","cell_type":"markdown"},{"source":"### We will include the augmented prompt in the conversation.\n\n- Print the last message in the list.\n- Replace the last message with a human message containing the augmented prompt.","metadata":{},"id":"aa19e06d-a9c0-4b25-bf3b-072b2f163852","cell_type":"markdown"},{"source":"print(messages[-1])","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1749212396739,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(messages[-1])","outputsMetadata":{"0":{"height":38,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"a7e0ee47-69f2-4f21-b48d-279e9ae5df3e","cell_type":"code","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":"content='Can you tell me about the LLMChain in LangChain?'\n"}]},{"source":"messages[-1] = HumanMessage(content=augmented_prompt)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1749212396791,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"messages[-1] = HumanMessage(content=augmented_prompt)","outputsMetadata":{"0":{"height":368,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"901893e6-726d-4a42-805f-ee2d24b3b8a2","cell_type":"code","execution_count":19,"outputs":[]},{"source":"### Ask GPT about LangChain again, this time providing source knowledge.\n\n- Send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"5f95104d-51aa-4c36-93fb-684e4ea99f8d","cell_type":"markdown"},{"source":"res = chat.invoke(messages)\nres.content","metadata":{"executionCancelledAt":null,"executionTime":2327,"lastExecutedAt":1749212399119,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"res = chat.invoke(messages)\nres.content","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"id":"ebbf365d-0215-4f6f-bb5b-146323b559b4","cell_type":"code","execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'Based on the provided context, the LLMChain in LangChain is the most common type of chain within the LangChain framework. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. The LLMChain takes multiple input variables, formats them into a prompt using the PromptTemplate, passes the prompt to the model, and then uses the OutputParser (if provided) to parse the output of the language model into a final format. This chain is a key component in the LangChain framework for developing applications powered by language models.'"},"metadata":{},"execution_count":20}]},{"source":"The quality of this answer is phenomenal! This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem—how do we get this information in the first place?\n\nWe learned in the previous code-alongs about Pinecone and vector databases. Well, they can help us here too. But first, we'll need a dataset.","metadata":{},"id":"30ae1f20-1e41-4e4f-a08e-09d0167170ba","cell_type":"markdown"},{"source":"## Importing the Data","metadata":{},"id":"97fa3cfe-9223-4593-85fd-6c21f788d46e","cell_type":"markdown"},{"source":"In this task, we will be importing our data. We will be using the Hugging Face Datasets library and [the `\"jamescalam/llama-2-arxiv-papers\"` dataset](https://huggingface.co/datasets/jamescalam/llama-2-arxiv-papers-chunked). This dataset contains a collection of ArXiv papers which will serve as the external knowledge base for our chatbot.","metadata":{},"id":"d6f4cd0f-4d29-4667-bf90-2603bae73759","cell_type":"markdown"},{"source":"### So, we satart loading the ArXiv papers dataset.\n\n- From the *datasets* package, import `load_dataset`.\n- Load the train split of the `jamescalam/llama-2-arxiv-papers-chunked` dataset. Assign to `dataset`.\n- Print the dataset object to see the structure of the data.\n- *Look at the structure. Which fields should we keep?*","metadata":{},"id":"404b9a07-65b0-490b-9380-fda08573baad","cell_type":"markdown"},{"source":"from datasets import load_dataset\n\ndataset = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split=\"train\")\n\ndataset","metadata":{"executionCancelledAt":null,"executionTime":2121,"lastExecutedAt":1749212401240,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from datasets import load_dataset\n\ndataset = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split=\"train\")\n\ndataset","outputsMetadata":{"0":{"height":101,"type":"stream"},"1":{"height":76,"type":"stream"},"6":{"height":95,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"16f97002-b358-4a33-bb50-0401feda259b","cell_type":"code","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/409 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eeef459b9bd480eb03c66da40fa0ba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/14.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c79f444e118748619f732636308c5f13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/4838 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee7abac464ce40c8ac6097eec1fa3a25"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n    num_rows: 4838\n})"},"metadata":{},"execution_count":21}]},{"source":"### Print a record of dataset to get a feel for what they contain.","metadata":{},"id":"796c12aa-e78a-4e62-9205-73cc6f3908cd","cell_type":"markdown"},{"source":"dataset[0]","metadata":{"executionCancelledAt":null,"executionTime":157,"lastExecutedAt":1749212401397,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"dataset[0]","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e","outputsMetadata":{"0":{"height":290,"type":"stream"}}},"id":"0e4c8dfd-2c65-4d2f-8ae1-d356daf0a3d8","cell_type":"code","execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'doi': '1102.0183',\n 'chunk-id': '0',\n 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n 'id': '1102.0183',\n 'title': 'High-Performance Neural Networks for Visual Object Classification',\n 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n 'source': 'http://arxiv.org/pdf/1102.0183',\n 'authors': ['Dan C. Cireşan',\n  'Ueli Meier',\n  'Jonathan Masci',\n  'Luca M. Gambardella',\n  'Jürgen Schmidhuber'],\n 'categories': ['cs.AI', 'cs.NE'],\n 'comment': '12 pages, 2 figures, 5 tables',\n 'journal_ref': None,\n 'primary_category': 'cs.AI',\n 'published': '20110201',\n 'updated': '20110201',\n 'references': []}"},"metadata":{},"execution_count":22}]},{"source":"### Dataset Summary\n\nThe dataset we are using is sourced from the Llama 2 ArXiv papers. It is a collection of academic papers from ArXiv, a repository of electronic preprints approved for publication after moderation. Each entry in the dataset represents a \"chunk\" of text from these papers.\n\nBecause most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Llama 2—at least not without this data.","metadata":{},"id":"7901bcab-0c5e-4bc1-944e-ebd13d9284da","cell_type":"markdown"},{"source":"## Building the Knowledge Base","metadata":{},"id":"7af0beaa-68f8-4cc3-831f-1118e79f45b3","cell_type":"markdown"},{"source":"We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.","metadata":{},"id":"92558679-3d66-419d-b30d-783456b992e5","cell_type":"markdown"},{"source":"### The workflow for setting up a chatbot is much the same as for setting up semantic serach and retrieval augmented generation, as seen in previous code-alongs.\n\n- Initialize your connection to the Pinecone vector DB.\n- Create an index (remember to consider the dimensionality of `text-embedding-ada-002`).\n- Initialize OpenAI's `text-embedding-ada-002` model with LangChain.\n- Populate the index with records (in this case from the Llama 2 dataset).","metadata":{},"id":"2f5e123a-23f5-4fbb-a8dd-226dbdbd5b7f","cell_type":"markdown"},{"source":"### Initialize Pinecone, getting setup details from Workspace environment variables.\n\n- Import the os package.\n- Import the pinecone package.\n- Initialize pinecone, setting the API key. Assign to `pc`.","metadata":{},"id":"b1b2ef0b-ce4b-48ae-a3cb-8fba1143f6af","cell_type":"markdown"},{"source":"import os\nimport pinecone\n\npc = pinecone.Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1749212401443,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\nimport pinecone\n\npc = pinecone.Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"0be5dcfe-1fd1-432c-9c95-77572f27e2fe","cell_type":"code","execution_count":23,"outputs":[]},{"source":"Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`.","metadata":{},"id":"6536757a-943a-4b06-bba2-6bfe4152d170","cell_type":"markdown"},{"source":"### Create a vector index in the Pinecone database.\n\n- Import the time package.\n- Choose a name for the vector index. Assign to `index_name`.\n- Check if index_name is not in Pinecone's list of existing indexes.\n    -  Create an index named index_name, dimension 1536, cosine similarity as its metric.\n    -  While the index status is not ready, sleep for one second.","metadata":{},"id":"e1b833dd-0ed2-4547-b0c4-18e9a105963f","cell_type":"markdown"},{"source":"import time\n\nindex_name = \"vector-index\"\n\nexisting_index_names  = [idx.name for idx in pc.list_indexes().indexes]\n\nif index_name not in existing_index_names:\n    pc.create_index(\n        index_name,\n        dimension=1536,\n        metric=\"cosine\",\n        spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n    )\n    \n    while not pc.describe_index(index_name).status[\"ready\"]:\n        time.sleep(1)","metadata":{"executionCancelledAt":null,"executionTime":184,"lastExecutedAt":1749212401627,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import time\n\nindex_name = \"vector-index\"\n\nexisting_index_names  = [idx.name for idx in pc.list_indexes().indexes]\n\nif index_name not in existing_index_names:\n    pc.create_index(\n        index_name,\n        dimension=1536,\n        metric=\"cosine\",\n        spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n    )\n    \n    while not pc.describe_index(index_name).status[\"ready\"]:\n        time.sleep(1)","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"a9c31353-0b6c-40e4-8b13-582b39004f55","cell_type":"code","execution_count":24,"outputs":[]},{"source":"- Connect to the index using its name. Assign to `index`.\n- View the index stats.","metadata":{},"id":"44cb4e16-e943-43ee-9555-028c9da19754","cell_type":"markdown"},{"source":"index = pc.Index(index_name)\n\nindex.describe_index_stats()","metadata":{"executionCancelledAt":null,"executionTime":1861,"lastExecutedAt":1749212403489,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"index = pc.Index(index_name)\n\nindex.describe_index_stats()","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"ea941ab8-99ab-4600-b035-7a892b0bd88d","cell_type":"code","execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {'': {'vector_count': 4838}},\n 'total_vector_count': 4838}"},"metadata":{},"execution_count":25}]},{"source":"Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model—we can access it via LangChain.","metadata":{},"id":"79733e18-cb8a-4b0a-9335-d0bdc90529ff","cell_type":"markdown"},{"source":"### Create an embeddings model.\n\n- From the `langchain_openai` package, import `OpenAIEmbeddings`.\n- Create an embedings model object for `text-embedding-ada-002`. Assign to `embed_model`.","metadata":{},"id":"b97b2f2c-828e-4517-b160-188977e9bbb5","cell_type":"markdown"},{"source":"from langchain_openai import OpenAIEmbeddings\n\nembed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")","metadata":{"executionCancelledAt":null,"executionTime":100,"lastExecutedAt":1749212403589,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain_openai import OpenAIEmbeddings\n\nembed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"7fc53547-8e89-4700-9a69-95bca1bedd30","cell_type":"code","execution_count":26,"outputs":[]},{"source":"Using this model we can create embeddings like so:","metadata":{},"id":"a7c8de50-0543-4ae0-92c0-1da97803f841","cell_type":"markdown"},{"source":"texts = [\n    \"this is a sentence\",\n    \"this is another sentence\"\n]\n\nres = embed_model.embed_documents(texts=texts)\nlen(res), len(res[0])","metadata":{"executionCancelledAt":null,"executionTime":1329,"lastExecutedAt":1749212404919,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"texts = [\n    \"this is a sentence\",\n    \"this is another sentence\"\n]\n\nres = embed_model.embed_documents(texts=texts)\nlen(res), len(res[0])","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"5b86b13a-591f-4b8e-b3cc-33c24c076d5b","cell_type":"code","execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":"(2, 1536)"},"metadata":{},"execution_count":27}]},{"source":"From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n\nWe're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches.","metadata":{},"id":"f7f7f8ee-beb7-4dd0-86fa-cddf0d130d7e","cell_type":"markdown"},{"source":"### Prepare the data for upserting to Pinecone.\n\n- From *tqdm*, import `tqdm` (a progress bar).\n- Select these columns: `doi`, `chunk-id`, `chunk`, `title`, `source`. Assign to `data_selected`.\n- Convert `data_selected` to a pandas DataFrame in batch sizes of `100`. Assign to `data_batched`.","metadata":{},"id":"f2ae6daf-ff93-4083-a4ac-259c09c6ad0b","cell_type":"markdown"},{"source":"from tqdm import tqdm\n\ndata_selected = dataset.select_columns([\"doi\", \"chunk-id\", \"chunk\", \"title\", \"source\"])\n\ndata_batched = data_selected.to_pandas(batched=True, batch_size=100)","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1749212404967,"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from tqdm import tqdm\n\ndata_selected = dataset.select_columns([\"doi\", \"chunk-id\", \"chunk\", \"title\", \"source\"])\n\ndata_batched = data_selected.to_pandas(batched=True, batch_size=100)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"78901974-3c01-4c19-a916-6a53d7fb17ad","cell_type":"code","execution_count":28,"outputs":[]},{"source":"### Instructions\n\nSplit the dataset into batches and add it to the vector index.\n\n- Loop over each batch in `data_batched`, adding a progress bar.\n    - Concatenate the `doi` and `chunk-id` columns separated by `-`, then convert to a list. Assign to `ids`.\n    - Get the `chunk` column and convert to a list. Assign to `texts`.\n    - Use the embedding model to embed the texts. Assign to `embeds`.\n    - Get the metadata from the batch. Assign to metadata.\n        - Select the `chunk`, `title`, and `source` columns.\n        - Apply the `dict` function to the columns axis.\n        - Convert to a list.\n    - Combine IDs, embeddings, and metadata as list of tuples. Assign to `to_upsert`.\n    - Upsert to Pinecone.","metadata":{},"id":"4b0b83df-51bb-4ede-a388-76724fb98d0b","cell_type":"markdown"},{"source":"from tqdm import tqdm\n\ndata_selected = dataset.select_columns([\"doi\", \"chunk-id\", \"chunk\", \"title\", \"source\"])\ndata_batched = data_selected.to_pandas(batched=True, batch_size=100)\n\nfor batch in tqdm(data_batched):\n    ids = (batch[\"doi\"] + \"-\" + batch[\"chunk-id\"]).to_list()\n    texts = batch[\"chunk\"].to_list()\n    embeds = embed_model.embed_documents(texts)\n    metadata = batch[[\"chunk\", \"title\", \"source\"]].apply(dict, axis=\"columns\").to_list()\n    to_upsert = zip(ids, embeds, metadata)\n    index.upsert(vectors=to_upsert)","metadata":{"executionCancelledAt":null,"executionTime":109936,"lastExecutedAt":1749212514903,"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from tqdm import tqdm\n\ndata_selected = dataset.select_columns([\"doi\", \"chunk-id\", \"chunk\", \"title\", \"source\"])\ndata_batched = data_selected.to_pandas(batched=True, batch_size=100)\n\nfor batch in tqdm(data_batched):\n    ids = (batch[\"doi\"] + \"-\" + batch[\"chunk-id\"]).to_list()\n    texts = batch[\"chunk\"].to_list()\n    embeds = embed_model.embed_documents(texts)\n    metadata = batch[[\"chunk\", \"title\", \"source\"]].apply(dict, axis=\"columns\").to_list()\n    to_upsert = zip(ids, embeds, metadata)\n    index.upsert(vectors=to_upsert)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"335b231c-6871-49ff-aacf-e7fc7d0121c5","cell_type":"code","execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":"49it [01:49,  2.24s/it]\n"}]},{"source":"We can check that the vector index has been populated using `describe_index_stats` like before:","metadata":{},"id":"58db36f2-1ba2-4fdb-9e72-aabe8fc16882","cell_type":"markdown"},{"source":"### Check on updates to the vector index now that it contains the ArXiv dataset.\n\n- View the index stats again.\n- *What has changed since you last looked?*","metadata":{},"id":"2235eb44-2990-4d7e-ab51-6e36263d685f","cell_type":"markdown"},{"source":"index.describe_index_stats()","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1749212514952,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"index.describe_index_stats()","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"48aa5c57-c923-49fc-b1dd-d60f6fa26d64","cell_type":"code","execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {'': {'vector_count': 4838}},\n 'total_vector_count': 4838}"},"metadata":{},"execution_count":30}]},{"source":"## Retrieval Augmented Generation","metadata":{},"id":"6b90e6fc-f627-40a2-8276-7df46b5d7155","cell_type":"markdown"},{"source":"In the previous steps, we built a comprehensive knowledge base. Now, it's time to connect that knowledge base to our chatbot. We'll dive back into LangChain and reuse our template prompt from earlier.\n\n### Workflow\n\n1. **Create a LangChain `vectorstore` object**:\n   - Utilize our existing `index` and `embed_model` to instantiate the `vectorstore`.\n\n2. **Search for relevant information**:\n   - Perform a search within the `vectorstore` for information related to \"Llama 2\".\n\n3. **Define the `augment_prompt` function**:\n   - This function will take a user query, retrieve relevant information from the `vectorstore`, and merge the results into a single retrieval-augmented prompt.\n\n4. **Compare chatbot responses**:\n   - Ask the chatbot questions about Llama 2 with and without Retrieval-Augmented Generation (RAG) and compare the differences in responses.","metadata":{},"id":"946741c8-6ad8-4aa4-afe4-e3357929918b","cell_type":"markdown"},{"source":"To use LangChain's RAG pipeline we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object.","metadata":{},"id":"320fa5d8-9642-4c7f-9a9f-9665b9c20475","cell_type":"markdown"},{"source":"### Initialize the vector store object.\n\n- From the `langchain_pinecone` package, import `PineconeVectorStore`.\n- State the metadata field that contains our text (`\"chunk\"`). Assign to `text_field`.\n- Create a `PineconeVectorStore` from the index, the embedding model, and the text field. Assign to `vectorstore`.","metadata":{},"id":"f35f4d72-8e2c-43d8-883c-8ff1e91b13ab","cell_type":"markdown"},{"source":"from langchain_pinecone import PineconeVectorStore\n\ntext_field = \"chunk\"\n\nvectorstore = PineconeVectorStore(index, embed_model, text_field)","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1749212514999,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain_pinecone import PineconeVectorStore\n\ntext_field = \"chunk\"\n\nvectorstore = PineconeVectorStore(index, embed_model, text_field)","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e","outputsMetadata":{"0":{"height":143,"type":"stream"}}},"id":"d91af8d9-9e0b-4246-ba92-08769b7f6017","cell_type":"code","execution_count":31,"outputs":[]},{"source":"Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2.","metadata":{},"id":"e7231db5-b65e-431c-87fb-e4600b5901e1","cell_type":"markdown"},{"source":"### We then perform similarity search against a question.\n\n- Define a question. Assign to query.\n    - Use the text `\"What is so special about Llama 2?\"`.\n- Perform a similarity search for the query, returning the top 3 results.","metadata":{},"id":"e92e91fc-f4b1-437c-a8eb-e4ee507c0b19","cell_type":"markdown"},{"source":"query = \"What is so special about Llama 2?\"\nvectorstore.similarity_search(query, k=3)","metadata":{"executionCancelledAt":null,"executionTime":208,"lastExecutedAt":1749212515208,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"query = \"What is so special about Llama 2?\"\nvectorstore.similarity_search(query, k=3)","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"11343325-0736-4b08-b85b-1d4735a0402d","cell_type":"code","execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":"[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n Document(page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"},"metadata":{},"execution_count":32}]},{"source":"We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier.","metadata":{},"id":"a4c5d7a5-9429-4d85-b8a5-f85aa9ae8654","cell_type":"markdown"},{"source":"def augment_prompt(query: str):\n    results = vectorstore.similarity_search(query, k=3)\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with \n                            the below information, say \"I don't know\".\n\nContexts: {source_knowledge}\n\nQuery: {query}\n\"\"\"\n    return augmented_prompt","id":"19b86b04-9630-4fb8-a4f9-9f5b38300623","cell_type":"code","outputs":[],"metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1749212515255,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def augment_prompt(query: str):\n    results = vectorstore.similarity_search(query, k=3)\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with \n                            the below information, say \"I don't know\".\n\nContexts: {source_knowledge}\n\nQuery: {query}\n\"\"\"\n    return augmented_prompt","lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"execution_count":33},{"source":"Using this we produce an augmented prompt:","metadata":{},"id":"b4a4f1a7-5d0b-4e7f-ab38-fbada51ba1d8","cell_type":"markdown"},{"source":"print(augment_prompt(query))","metadata":{"executionCancelledAt":null,"executionTime":160,"lastExecutedAt":1749212515415,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(augment_prompt(query))","outputsMetadata":{"0":{"height":616,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"0ba045f5-88d7-4ce3-9ae6-e96cafa96e02","cell_type":"code","execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with \n                            the below information, say \"I don't know\".\n\nContexts: Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\nSergey Edunov Thomas Scialom\u0003\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\nthe community to advance AI alignment research.\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need, 2017.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint\n\nQuery: What is so special about Llama 2?\n\n"}]},{"source":"There is still a lot of text here, so let's pass it onto our chat model to see how it performs.","metadata":{},"id":"5be7c70d-a1c2-414b-b09b-afbc046503df","cell_type":"markdown"},{"source":"### Ask now GPT about LLama2, augmenting the prompt with source knowledge from the Pinecone vector index.\n\n- Create a new human message. Assign to `prompt`.\n    - Call `augment_prompt()` on the query and use this as the content.\n- Append the prompt to `messages`.\n- Send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"bafc0318-25bc-49f6-b9d8-aed3683ee2c4","cell_type":"markdown"},{"source":"promt = HumanMessage(content=augment_prompt(query))\nmessages.append(promt)\nres = chat.invoke(messages)\nres.content","metadata":{"executionCancelledAt":null,"executionTime":2338,"lastExecutedAt":1749212843364,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"promt = HumanMessage(content=augment_prompt(query))\nmessages.append(promt)\nres = chat.invoke(messages)\nres.content","outputsMetadata":{"0":{"height":143,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"6f70d2a0-d5af-448b-8f00-0aec873ac250","cell_type":"code","execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) that are optimized for dialogue use cases. These models range in scale from 7 billion to 70 billion parameters. Llama 2 models outperform open-source chat models on various benchmarks and have been evaluated positively for helpfulness and safety. They are considered to potentially be suitable substitutes for closed-source models. The detailed approach to fine-tuning and safety in Llama 2 models is highlighted, showcasing their performance and potential as advanced language models within the AI community.'"},"metadata":{},"execution_count":38}]},{"source":"We can continue with more Llama 2 questions. Let's try _without_ RAG first:","metadata":{},"id":"f3547c02-31ea-4847-8042-7095d22a0fe9","cell_type":"markdown"},{"source":"### Ask GPT about LLama 2.\n\n- Create a new human message. Assign to `prompt`.\n    - Use the context `\"What safety measures were used in the development of llama 2?\"`.\n- Invoke a chat with GPT sending the messages plus the prompt. Assign to `res`.\n    - *Don't use `.append()` here, as we don't want to store the latest message in the conversation. The prompt needs to be converted to a list to add it to the existing list.*\n- Print the contents of the response.","metadata":{},"id":"b38ac3b3-efdd-43f5-99d5-0582745e0de0","cell_type":"markdown"},{"source":"prompt = HumanMessage(content=\"What safety measures were used in the development of llama 2?\")\nres = chat.invoke(messages + [prompt])\nres.content","metadata":{"executionCancelledAt":null,"executionTime":1167,"lastExecutedAt":1749212849992,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"prompt = HumanMessage(content=\"What safety measures were used in the development of llama 2?\")\nres = chat.invoke(messages + [prompt])\nres.content","outputsMetadata":{"0":{"height":248,"type":"stream"},"1":{"height":332,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"00e797e1-5526-416b-be41-88925fa14960","cell_type":"code","execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'Safety measures used in the development of Llama 2 included conducting human evaluations for helpfulness and safety. The Llama 2 models were fine-tuned to align with human preferences, enhancing their usability and safety. The models outperformed open-source chat models on various benchmarks tested. Additionally, the approach to fine-tuning and safety was detailed, and comparisons were made with closed-source models in terms of performance and human evaluations. These safety measures aimed to ensure that the Llama 2 language models were effective, safe, and aligned with human expectations.'"},"metadata":{},"execution_count":39}]},{"source":"The chatbot is able to respond about Llama 2 thanks to it's conversational history stored in `messages`. However, it doesn't know anything about the safety measures themselves as we have not provided it with that information via the RAG pipeline. Let's try again but with RAG.","metadata":{},"id":"8ea55fa4-c8a5-436e-8934-8d90a4002c57","cell_type":"markdown"},{"source":"### Ask GPT about LLama 2 again.\n\n- Do the same thing again, but this time augment the prompt using `augment_prompt()`.","metadata":{},"id":"31afe145-f1de-4ddf-abba-051144d6d3bc","cell_type":"markdown"},{"source":"prompt = HumanMessage(content=augment_prompt(\"What safety measures were used in the development of llama 2?\"))\nres = chat.invoke(messages + [prompt])\nres.content","metadata":{"executionCancelledAt":null,"executionTime":1853,"lastExecutedAt":1749212519967,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"prompt = HumanMessage(content=augment_prompt(\"What safety measures were used in the development of llama 2?\"))\nres = chat.invoke(messages + [prompt])\nres.content","outputsMetadata":{"0":{"height":248,"type":"stream"}},"lastExecutedByKernel":"7e7fd607-5b52-48d0-bc87-334a345e976e"},"id":"412a5fb9-5bbb-46cd-8636-6bcb7f575605","cell_type":"code","execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'The safety measures used in the development of Llama 2 included:\\n\\n1. Safety-specific data annotation and tuning: This involved annotating and tuning the data specifically to enhance the safety aspects of the models.\\n2. Conducting red-teaming: Red-teaming involves simulating attacks or adversarial scenarios to test the robustness and security of the models.\\n3. Employing iterative evaluations: Continuously evaluating the models through iterations to identify and address any potential safety concerns or vulnerabilities.\\n\\nThese measures were implemented to increase the safety of the Llama 2 models and ensure responsible development of large language models (LLMs).'"},"metadata":{},"execution_count":37}]},{"source":"We get a much better informed response that includes several items missing in the previous non-RAG response, such as \"red-teaming\", \"iterative evaluations\", and the intention of the researchers to share this research to help \"improve their safety, promoting responsible development in the field\".","metadata":{},"id":"e79d11dd-338b-4732-8315-7434a4e82d48","cell_type":"markdown"},{"source":"## Summary & Conclusion\n\nA sophisticated chatbot capable of answering questions about cutting-edge large language models, specifically focusing on Llama 2, was built. This project is a significant part of a Generative AI workflow, showcasing the ability to integrate various advanced technologies and methodologies. Here are the major insights and skills demonstrated throughout this project:\n\n### Key Insights\n\n1. **Conversational AI Development**:\n   - Learned how to maintain a coherent conversation with GPT by appending messages, ensuring context is preserved across interactions.\n   - This involved understanding the structure of conversational history and effectively managing it to improve the chatbot's responses.\n\n2. **Contextual Prompting**:\n   - Explored the importance of providing context within prompts to enhance the quality of GPT's answers.\n   - By augmenting prompts with relevant information, the chatbot's ability to provide accurate and detailed responses was significantly improved.\n\n3. **Vector Databases and Retrieval-Augmented Generation (RAG)**:\n   - Set up a Pinecone database, a vector database that allows for efficient storage and retrieval of high-dimensional data.\n   - Added data to a vector index, enabling the chatbot to retrieve relevant text chunks based on user queries.\n   - This integration of RAG techniques allowed the chatbot to access external knowledge, thereby answering questions that GPT alone could not handle.\n\n4. **Data Handling and Preprocessing**:\n   - Demonstrated proficiency in handling and preprocessing data, as evidenced by the creation and management of a DataFrame containing relevant information.\n   - This skill is crucial for ensuring that the data fed into the model is clean, relevant, and structured appropriately.\n     \n\n### Skills and Thought Process\n\n1. **Technical Proficiency**:\n   - The ability to work with advanced AI models like GPT and Llama 2 highlights technical expertise in the field of generative AI.\n   - Setting up and managing a Pinecone database showcases skills in working with modern data storage solutions.\n\n2. **Problem-Solving and Innovation**:\n   - The project required innovative problem-solving, particularly in enhancing the chatbot's responses through contextual prompting and RAG.\n   - The approach to augmenting prompts and integrating external data sources demonstrates a deep understanding of how to leverage AI capabilities effectively.\n\n3. **Attention to Detail**:\n   - The meticulous management of conversational history and the careful augmentation of prompts reflect attention to detail.\n   - Ensuring that the chatbot provides accurate and contextually relevant answers required a thorough and thoughtful approach.\n\n4. **Project Management**:\n   - Successfully combining various components—conversational AI, vector databases, and RAG—into a cohesive project highlights project management skills.\n   - This project serves as a testament to the ability to plan, execute, and refine complex AI systems.\n\n### Final Thoughts\n\nThis project not only showcases technical skills and innovative thinking but also underscores the ability to manage and execute complex AI projects. By building a chatbot that can answer questions about any topic, a comprehensive understanding of conversational AI, contextual prompting, and the integration of external data sources is ~~demonstrated.~~","metadata":{},"id":"ecea5aba-3a90-4655-9efa-425bc880e0cc","cell_type":"markdown"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}