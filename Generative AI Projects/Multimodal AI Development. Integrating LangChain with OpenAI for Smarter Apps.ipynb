{"cells":[{"source":"# Crafting Advanced Multimodal AI Solutions with LangChain & OpenAI API","metadata":{},"id":"bbf7941b-6c0a-4b14-b1fc-c703e57e352b","cell_type":"markdown"},{"source":"## Introduction\n\nIn today's digital age, videos are a treasure trove of information, but extracting that information efficiently can be a challenge. Watching an entire video or skimming through it can be time-consuming and often inefficient. This project aims to streamline this process by leveraging the power of multimodal AI to create an interactive Q&A bot that can answer questions about the content of a YouTube video.\n\n### Project Overview\n\nIn this project, we will:\n\n1. **Download a Tutorial Video from YouTube**: We'll start by selecting a tutorial video from YouTube that contains valuable information.\n2. **Transcribe the Audio**: Using the Whisper API, we'll convert the audio from the video into text.\n3. **Create a Q&A Bot**: We'll utilize LangChain to build a bot that can answer questions based on the transcribed text.\n\n### Goals\n\n- **Understand the Building Blocks of Multimodal AI Projects**: Gain insights into how different AI models can be combined to create powerful applications.\n- **Apply Fundamental Concepts of LangChain**: Explore the core functionalities of LangChain and how it can be used to build interactive applications.\n- **Use the Whisper API for Transcription**: Learn how to transcribe audio to text using the Whisper API.\n- **Combine LangChain and Whisper API**: Integrate both tools to create a seamless Q&A experience for any YouTube video.\n\n### Workflow\n\n1. **Video Selection and Download**:\n   - Choose a tutorial video from YouTube.\n   - Use a YouTube downloader tool to save the video locally.\n\n2. **Audio Transcription**:\n   - Extract the audio from the downloaded video.\n   - Use the Whisper API to transcribe the audio into text.\n\n3. **Text Processing**:\n   - Clean and preprocess the transcribed text to ensure it is suitable for querying.\n   - Segment the text into manageable chunks for better processing.\n\n4. **Q&A Bot Development**:\n   - Utilize LangChain to build a Q&A bot.\n   - Feed the processed text into the bot.\n   - Implement a user interface to interact with the bot and ask questions.\n\n5. **Interactive Insights Extraction**:\n   - Allow users to input questions and receive answers in real-time.\n   - Highlight key insights and information from the video based on user queries.\n\n### Thought Process\n\n- **Efficiency**: The primary goal is to make information extraction from videos faster and more efficient.\n- **Interactivity**: By creating a Q&A bot, we provide an interactive way for users to engage with the content.\n- **Scalability**: The approach can be scaled to handle multiple videos and different types of content.\n\n### Tech Stack\n\n- **YouTube Downloader**: For downloading videos.\n- **Whisper API**: For transcribing audio to text.\n- **LangChain**: For building the Q&A bot.\n- **Python**: The primary programming language for scripting and development.\n- **Jupyter Notebook**: For interactive development and demonstration.\n\n### Skills Utilized\n\n- **Python Programming**: Writing scripts for downloading videos, transcribing audio, and building the bot.\n- **API Integration**: Using the Whisper API for transcription and LangChain for the Q&A bot.\n- **Natural Language Processing (NLP)**: Processing and querying the transcribed text.\n- **Interactive Development**: Using Jupyter Notebook to develop and demonstrate the project.\n\n### Step-by-Step Description\n\n1. **Download the Video**:\n   - Use a Python script to download the video from YouTube.\n   - Example: `youtube-dl <video_url>`\n\n2. **Extract and Transcribe Audio**:\n   - Extract audio from the video file.\n   - Use the Whisper API to transcribe the audio.\n   - Example: \n     ```python\n     import whisper\n     model = whisper.load_model(\"base\")\n     result = model.transcribe(\"audio_file.mp3\")\n     text = result[\"text\"]\n     ```\n\n3. **Process the Transcribed Text**:\n   - Clean the text to remove any noise or irrelevant information.\n   - Segment the text into smaller chunks for better querying.\n\n4. **Build the Q&A Bot**:\n   - Initialize LangChain and load the processed text.\n   - Implement the Q&A functionality.\n   - Example:\n     ```python\n     from langchain import LangChain\n     chain = LangChain()\n     chain.load_text(text)\n     response = chain.query(\"What is the main topic of the video?\")\n     print(response)\n     ```\n\n5. **Interactive Insights Extraction**:\n   - Create a user interface to input questions and display answers.\n   - Highlight key insights based on user queries.\n\nBy following these steps, we can efficiently extract and interact with valuable information from YouTube videos, making the process faster and more user-friendly.","metadata":{},"id":"701b76fe-04db-405c-98f7-f0f5babd84b4","cell_type":"markdown"},{"source":"## Setup","metadata":{},"id":"a9274661-8d8c-4cc5-901e-5fc497866b89","cell_type":"markdown"},{"source":"The project requires several packages that need to be installed into Workspace.\n\n- `langchain` is a framework for developing generative AI applications.\n- `yt_dlp` lets you download YouTube videos.\n- `tiktoken` converts text into tokens.\n- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text).","metadata":{},"id":"823598ac-fa77-4532-997d-2923d0017e90","cell_type":"markdown"},{"source":"### Run the following code to install the packages.","metadata":{},"id":"c17ab340-c582-4ba7-ab33-5d582210f5c2","cell_type":"markdown"},{"source":"# Install the openai package, locked to version 1.27\n!pip install openai==1.27\n\n# Install the langchain package, locked to version 0.1.19\n!pip install langchain==0.1.19\n\n# Install the langchain-openai package, locked to version 0.1.6\n!pip install langchain-openai==0.1.6\n\n# Install the yt_dlp package, locked to version 2024.4.9\n!pip install yt_dlp==2024.4.9\n\n# Install the tiktoken package, locked to version 0.6.0\n!pip install tiktoken==0.6.0\n\n# Install the docarray package, locked to version 0.40.0\n!pip install docarray==0.40.0","metadata":{"executionCancelledAt":null,"executionTime":37661,"lastExecutedAt":1715608961060,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Install the openai package, locked to version 1.27\n!pip install openai==1.27\n\n# Install the langchain package, locked to version 0.1.19\n!pip install langchain==0.1.19\n\n# Install the yt_dlp package, locked to version 2024.4.9\n!pip install yt_dlp==2024.4.9\n\n# Install the tiktoken package, locked to version 0.6.0\n!pip install tiktoken==0.6.0\n\n# Install the docarray package, locked to version 0.40.0\n!pip install docarray==0.40.0","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"e1ca41e3-2dfd-4b9a-b595-e5af720ca36a","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting openai==1.27\n  Downloading openai-1.27.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (3.6.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (1.10.12)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (1.2.0)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (4.64.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai==1.27) (2.8)\nRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai==1.27) (2019.11.28)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.8/dist-packages (from httpx<1,>=0.23.0->openai==1.27) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.8/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.27) (0.14.0)\nDownloading openai-1.27.0-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: openai\n\u001b[33m  WARNING: The script openai is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0mSuccessfully installed openai-1.27.0\nDefaulting to user installation because normal site-packages is not writeable\nCollecting langchain==0.1.19\n  Downloading langchain-0.1.19-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (1.4.40)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.1.19)\n  Downloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (4.0.2)\nCollecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.19)\n  Downloading dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\nCollecting langchain-community<0.1,>=0.0.38 (from langchain==0.1.19)\n  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\nCollecting langchain-core<0.2.0,>=0.1.52 (from langchain==0.1.19)\n  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.19)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.19)\n  Downloading langsmith-0.1.57-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (1.23.2)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (8.2.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (21.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.8.1)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19)\n  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (0.8.0)\nCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.19)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.19)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19)\n  Downloading orjson-3.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<3,>=1->langchain==0.1.19) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain==0.1.19) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.1.19) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.1.19) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.1.19) (2019.11.28)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.19) (1.1.3)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.8/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (2.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (0.4.3)\nDownloading langchain-0.1.19-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\nDownloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.57-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.2/142.2 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, jsonpatch, marshmallow, langsmith, aiohttp, langchain-core, dataclasses-json, langchain-text-splitters, langchain-community, langchain\n\u001b[33m  WARNING: The script langsmith is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngremlinpython 3.6.1 requires aiohttp<=3.8.1,>=3.8.0, but you have aiohttp 3.9.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiohttp-3.9.5 dataclasses-json-0.6.6 jsonpatch-1.33 langchain-0.1.19 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.1 langsmith-0.1.57 marshmallow-3.21.2 orjson-3.10.3 packaging-23.2\nDefaulting to user installation because normal site-packages is not writeable\nCollecting yt_dlp==2024.4.9\n  Downloading yt_dlp-2024.4.9-py3-none-any.whl.metadata (165 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting brotli (from yt_dlp==2024.4.9)\n  Downloading Brotli-1.1.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (5.5 kB)\nRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from yt_dlp==2024.4.9) (2019.11.28)\nCollecting mutagen (from yt_dlp==2024.4.9)\n  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\nCollecting pycryptodomex (from yt_dlp==2024.4.9)\n  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.8/dist-packages (from yt_dlp==2024.4.9) (2.31.0)\nCollecting urllib3<3,>=1.26.17 (from yt_dlp==2024.4.9)\n  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\nCollecting websockets>=12.0 (from yt_dlp==2024.4.9)\n  Downloading websockets-12.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.31.0->yt_dlp==2024.4.9) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.31.0->yt_dlp==2024.4.9) (2.8)\nDownloading yt_dlp-2024.4.9-py3-none-any.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading websockets-12.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Brotli-1.1.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: brotli, websockets, urllib3, pycryptodomex, mutagen, yt_dlp\n\u001b[33m  WARNING: The scripts mid3cp, mid3iconv, mid3v2, moggsplit, mutagen-inspect and mutagen-pony are installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script yt-dlp is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbotocore 1.31.40 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.1 which is incompatible.\nopensearch-py 1.1.0 requires urllib3<2,>=1.21.1, but you have urllib3 2.2.1 which is incompatible.\nweb3 5.31.0 requires websockets<10,>=9.1, but you have websockets 12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.20.0 urllib3-2.2.1 websockets-12.0 yt_dlp-2024.4.9\nDefaulting to user installation because normal site-packages is not writeable\nCollecting tiktoken==0.6.0\n  Downloading tiktoken-0.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken==0.6.0) (2022.8.17)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from tiktoken==0.6.0) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken==0.6.0) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (2019.11.28)\nDownloading tiktoken-0.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.6.0\nDefaulting to user installation because normal site-packages is not writeable\nCollecting docarray==0.40.0\n  Downloading docarray-0.40.0-py3-none-any.whl.metadata (36 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from docarray==0.40.0) (1.23.2)\nRequirement already satisfied: orjson>=3.8.2 in /home/repl/.local/lib/python3.8/site-packages (from docarray==0.40.0) (3.10.3)\nRequirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.8/dist-packages (from docarray==0.40.0) (1.10.12)\nCollecting rich>=13.1.0 (from docarray==0.40.0)\n  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\nCollecting types-requests>=2.28.11.6 (from docarray==0.40.0)\n  Downloading types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from docarray==0.40.0) (0.8.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic>=1.10.8->docarray==0.40.0) (4.9.0)\nCollecting markdown-it-py>=2.2.0 (from rich>=13.1.0->docarray==0.40.0)\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from rich>=13.1.0->docarray==0.40.0) (2.13.0)\nRequirement already satisfied: urllib3>=2 in /home/repl/.local/lib/python3.8/site-packages (from types-requests>=2.28.11.6->docarray==0.40.0) (2.2.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect>=0.8.0->docarray==0.40.0) (0.4.3)\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray==0.40.0)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nDownloading docarray-0.40.0-py3-none-any.whl (270 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\nDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: types-requests, mdurl, markdown-it-py, rich, docarray\n\u001b[33m  WARNING: The script markdown-it is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmdit-py-plugins 0.3.0 requires markdown-it-py<3.0.0,>=1.0.0, but you have markdown-it-py 3.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed docarray-0.40.0 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.1 types-requests-2.31.0.20240406\n"}]},{"source":"## Required Libraries ","metadata":{},"id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf","cell_type":"markdown"},{"source":"For this project, we need the following packages:\n\n- `os`: This package provides a way of using operating system-dependent functionality, such as reading or writing to the file system.\n- `yt_dlp`: This package allows us to download YouTube videos. We will use it to download a video of your choice, convert it to an `.mp3` format, and save the file locally.\n- `openai`: This package simplifies making API calls to OpenAI models, which we will use for various generative AI tasks.\n\nMake sure all these packages are installed and properly configured before proceeding with the project.","metadata":{},"id":"2cf847fd-f8f8-49f6-9b43-0eb098239072","cell_type":"markdown"},{"source":"### Import the following packages.\n\n- Import `os`.\n- Import `glob`.\n- Import `openai`.\n- Import `yt_dlp` with the alias `youtube_dl`.\n- From the `yt_dlp` package, import `DowloadError`.\n- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`.","metadata":{},"id":"b1fcd794-b29c-4010-8be0-651a452b2044","cell_type":"markdown"},{"source":"# Import the os package\nimport os \n\n# Import the glob package\nimport glob\n\n# Import the openai package \nimport openai \n\n# Import the yt_dlp package as youtube_dl\nimport yt_dlp as youtube_dl\n\n# Import DownloadError from yt_dlp\nfrom yt_dlp import DownloadError \n\n# Import DocArray \nimport docarray \n","metadata":{"executionCancelledAt":null,"executionTime":8363,"lastExecutedAt":1715608969425,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the os package\nimport os \n\n# Import the glob package\nimport glob\n\n# Import the openai package \nimport openai \n\n# Import the yt_dlp package as youtube_dl\nimport yt_dlp as youtube_dl\n\n# Import DownloadError from yt_dlp\nfrom yt_dlp import DownloadError \n\n# Import DocArray \nimport docarray \n","outputsMetadata":{"0":{"height":77,"type":"stream"}},"lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"541cd9f5-0aaa-4374-8411-25bedecd8c84","cell_type":"code","execution_count":2,"outputs":[]},{"source":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1715608969475,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")","lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"7156b205-f844-4d9e-8867-449ff5840839","cell_type":"code","execution_count":3,"outputs":[]},{"source":"## Download the YouTube Video","metadata":{},"id":"751b9539-cbf7-4d6e-9634-045345cf8a4a","cell_type":"markdown"},{"source":"After setting up the environment, the first step is to download a YouTube video and convert it to an audio file (.mp3).\n\nFor this example, we'll download a coding tutorial about machine learning in Python.\n\nHere are the steps we'll follow:\n\n1. **Set Variables**: Define the `youtube_url` and the `output_dir` where the file will be stored.\n2. **Download and Convert**: Use `yt_dlp` to download the video and convert it to an `.mp3` file.\n3. **List Audio Files**: Create a loop to search the `output_dir` for any `.mp3` files and store them in a list called `audio_files`. This list will be used later to send each file to the Whisper model for transcription.","metadata":{},"id":"48abc459-48e5-4c7c-a795-daaf347ceef6","cell_type":"markdown"},{"source":"- Run the code to set the URL of the video, `youtube_url`, the directory to store the downloaded video, `youtube_url`, and the download settings, `ydl_config`.","metadata":{},"id":"8f2f2698-f768-4437-8e7f-c11327d3d4a7","cell_type":"markdown"},{"source":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1715608969527,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}","outputsMetadata":{"0":{"height":357,"type":"stream"},"1":{"height":137,"type":"stream"},"2":{"height":97,"type":"stream"},"3":{"height":37,"type":"stream"},"4":{"height":257,"type":"stream"},"5":{"height":77,"type":"stream"},"6":{"height":57,"type":"stream"},"7":{"height":57,"type":"stream"},"8":{"height":97,"type":"stream"},"9":{"height":77,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"ffb3836d-7b1b-47db-9ccc-6910972dd045","cell_type":"code","execution_count":4,"outputs":[]},{"source":"- Check if `output_dir` exists, if not, then make that directory.\n- Try to download the video using the specified configuration.\n  -  If a DownloadError occurs, attempt to download the video again.","metadata":{},"id":"3fa75be2-cc48-41a1-971c-60206787b487","cell_type":"markdown"},{"source":"# Check if the output directory exists, if not create it\nif not os.path.exists(output_dir): \n    os.makedirs(output_dir)\n\n# Print a message indicating which video is being downloaded\nprint(f\"Downloading video from {youtube_url}\")\n\n# Try to download the video using the specified configuration\n# If a DownloadError occurs, attempt to download the video again\ntry: \n    with youtube_dl.YoutubeDL(ydl_config) as ydl: \n        ydl.download([youtube_url])\nexcept DownloadError: \n    with youtube_dl.YoutubeDL(ydl_config) as ydl: \n        ydl.download([youtube_url])","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":311,"type":"stream"},"2":{"height":101,"type":"stream"},"3":{"height":38,"type":"stream"},"4":{"height":38,"type":"stream"},"5":{"height":122,"type":"stream"},"6":{"height":38,"type":"stream"},"7":{"height":80,"type":"stream"},"8":{"height":38,"type":"stream"},"9":{"height":206,"type":"stream"},"10":{"height":59,"type":"stream"},"11":{"height":59,"type":"stream"},"12":{"height":38,"type":"stream"},"13":{"height":80,"type":"stream"},"14":{"height":38,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"be4ba359-22e2-4f20-98a6-4a0b114c5d7d","cell_type":"code","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"Downloading video from https://www.youtube.com/watch?v=aqzxYofJ_ck\n"},{"output_type":"stream","name":"stderr","text":"[debug] Encodings: locale UTF-8, fs utf-8, pref UTF-8, out UTF-8 (No ANSI), error UTF-8 (No ANSI), screen UTF-8 (No ANSI)\n[debug] yt-dlp version stable@2024.04.09 from yt-dlp/yt-dlp [ff0779267] (pip) API\n[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': 'files/audio/%(title)s.%(ext)s', 'verbose': True, 'compat_opts': set(), 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.115 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-us,en;q=0.5', 'Sec-Fetch-Mode': 'navigate'}}\n[debug] Python 3.8.10 (CPython x86_64 64bit) - Linux-5.10.209-198.858.amzn2.x86_64-x86_64-with-glibc2.29 (OpenSSL 1.1.1f  31 Mar 2020, glibc 2.31)\n[debug] exe versions: ffmpeg 4.2.7, ffprobe 4.2.7\n[debug] Optional libraries: Cryptodome-3.20.0, brotli-1.1.0, certifi-2019.11.28, mutagen-1.47.0, requests-2.31.0, secretstorage-3.3.3, sqlite3-3.31.1, urllib3-1.25.8, websockets-12.0\n[debug] Proxy map: {}\n[debug] Request Handlers: urllib, websockets\n[debug] Loaded 1810 extractors\n"},{"output_type":"stream","name":"stdout","text":"[youtube] Extracting URL: https://www.youtube.com/watch?v=aqzxYofJ_ck\n[youtube] aqzxYofJ_ck: Downloading webpage\n[youtube] aqzxYofJ_ck: Downloading ios player API JSON\n[youtube] aqzxYofJ_ck: Downloading android player API JSON\n"},{"output_type":"stream","name":"stderr","text":"WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"aqzxYofJ_ck\")\n"},{"output_type":"stream","name":"stdout","text":"[youtube] aqzxYofJ_ck: Downloading player a960a0cb\n"},{"output_type":"stream","name":"stderr","text":"[debug] [youtube] Extracting nsig function with jsinterp\n[debug] Saving youtube-nsig.a960a0cb to cache\n[debug] [youtube] Decrypted nsig mpAKXPmdsQ2KLfnJ6 => n6t9edO8ZlO1rQ\n[debug] Loading youtube-nsig.a960a0cb from cache\n[debug] [youtube] Decrypted nsig 6vesnfccWruEJ7TsY => 9L5aMNsiYTAS1A\n"},{"output_type":"stream","name":"stdout","text":"[youtube] aqzxYofJ_ck: Downloading m3u8 information\n"},{"output_type":"stream","name":"stderr","text":"[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec:vp9.2, channels, acodec, lang, proto\n[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec:vp9.2(10), channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n"},{"output_type":"stream","name":"stdout","text":"[info] aqzxYofJ_ck: Downloading 1 format(s): 251\n"},{"output_type":"stream","name":"stderr","text":"[debug] Invoking http downloader on \"https://rr3---sn-ab5sznzl.googlevideo.com/videoplayback?expire=1715630570&ei=ih1CZuOBFumGkucPrJeq-Ac&ip=3.88.83.220&id=o-AHVPdBMFqeO-p8gUUPS3FAQvBxGpl0qMTMfcmbkwR7_G&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zw&mm=31%2C29&mn=sn-ab5sznzl%2Csn-ab5l6nrd&ms=au%2Crdu&mv=u&mvi=3&pl=24&bui=AWRWj2QtjOQl4cf0mEA8gqEqhT8RfXSBmgAs1IMzKB9ULqJSkNNTkP0YMtauKozZKci1-VePuFsZqKUR&spc=UWF9fyVN2w86wGgK3jV5cD7Rpeks0welMDcfn7W000Hsm-MOut6t75c&vprv=1&svpuc=1&mime=audio%2Fwebm&ns=5SzWiS3Fugfd-wqkdoAalnEQ&rqh=1&gir=yes&clen=10932652&dur=752.701&lmt=1654008313150389&mt=1715608446&fvip=4&keepalive=yes&c=WEB&sefc=1&txp=5318224&n=9L5aMNsiYTAS1A&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIhANvav2j_Tdc8rl0cr3diroFt_xfrfHJCevUe4_XuD2AZAiBLDLcv27c3q2YeWuvpwUUgS5AbGi7m35jPKgteYnzkSw%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl&lsig=AHWaYeowRQIhAPDqIbbhr6VvlMZ1_lMJbFQ2zVifA0bfHj5RxVbxv625AiBR2tjs5EBShJyTwAdzMKaiNNimOM-1em_e_GtRMzLBtw%3D%3D\"\n"},{"output_type":"stream","name":"stdout","text":"[download] Destination: files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm\n[download] 100% of   10.43MiB in 00:00:00 at 18.53MiB/s  \n"},{"output_type":"stream","name":"stderr","text":"[debug] ffmpeg command line: ffprobe -show_streams 'file:files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm'\n"},{"output_type":"stream","name":"stdout","text":"[ExtractAudio] Destination: files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n"},{"output_type":"stream","name":"stderr","text":"[debug] ffmpeg command line: ffmpeg -y -loglevel repeat+info -i 'file:files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm' -vn -acodec libmp3lame -b:a 192.0k -movflags +faststart 'file:files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3'\n"},{"output_type":"stream","name":"stdout","text":"Deleting original file files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm (pass -k to keep)\n"}]},{"source":"To find the audio files that we will use the `glob`module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. ","metadata":{},"id":"df9c586d-309a-411b-90a5-6e81fe85eda4","cell_type":"markdown"},{"source":"### Find the audio file in the output directory.\n\n- Find all the MP3 audio files in the output directory by joining the output directory to the pattern `*.mp3` and using glob to list them.\n- Select the first file in the list and assign it to `audio_filename`.\n- _Check your work._ Print `audio_filename`.","metadata":{},"id":"0fa69a42-7065-4c3f-8699-fe48908f11b1","cell_type":"markdown"},{"source":"# Find the audio file in the output directory\naudio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n\n# Select the first audio file in the list\naudio_filename = audio_files[0]\n\n# Print the name of the selected audio file\nprint(audio_filename)","metadata":{"executionCancelledAt":null,"executionTime":16,"lastExecutedAt":1715609034851,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Find the audio file in the output directory\n\n# Find all the audio files in the output directory\naudio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n\n# Select the first audio file in the list\naudio_filename = audio_files[0]\n\n# Print the name of the selected audio file\nprint(audio_filename)","outputsMetadata":{"0":{"height":38,"type":"stream"}},"lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"c3d0a34d-ade9-4314-bc7d-480f165b3992","cell_type":"code","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n"}]},{"source":"## Transcribe the Video using Whisper","metadata":{},"id":"a9fe2a4e-b6ac-43d3-9b22-7df437015913","cell_type":"markdown"},{"source":"In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n\nUsing these variables we will:\n- create a list to store the transcripts\n- Read the Audio File \n- Send the file to the Whisper Model using the OpenAI package ","metadata":{},"id":"1a00b32c-06e2-4fb1-8830-3634b13d133a","cell_type":"markdown"},{"source":"### Transcribe the audio file.\n\n- _The audio file, output file, and model are definied for you._\n- Define an OpenAI client model. Assign to client.\n- Open the audio file as read-binary (`\"rb\"`).\n  - Use the Whisper model to create a transcription of the opened audio file. Assign to `response`.\n- Extract the transcript from the response.","metadata":{},"id":"e4b60c5a-ea58-469e-b699-d46ef1cc7485","cell_type":"markdown"},{"source":"# Use these settings\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Transcribe the audio file to text using OpenAI API\nprint(\"converting audio to text...\")\n\n# Define an OpenAI client model. Assign to client.\nclient = openai.OpenAI()\n\n# Open the audio file as read-binary\nwith open(audio_file, \"rb\") as audio:\n    # Use the model to create a transcription\n    response = client.audio.transcriptions.create(file=audio, model=model)\n\n# Extract the transcript from the response\ntranscript = response.text\n\n# Print the transcript\nprint(transcript)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":null},"id":"54306dcc-40f7-4a12-97ef-388b95c70ad4","cell_type":"code","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"converting audio to text...\nHi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating because it's going to make your model appear to perform better than it actually does. So it's giving you a sort of false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using DataCamp Workspace here. And this is one of the data sets that is available as standard with DataCamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as pd. That's the sort of standard alias for it. And then we actually we just one function from scikit-learn. So this is in the scikit-learn model selection submodule. And the function for splitting into training and testing sets is called train test split. So let's run that. All right, so this data is about loan applications. So I'm just going to call it loan applications. And we can use pd.read CSV because it is in a CSV file. And the file is called loan underscore data dot CSV. Let me just check and see if I got that correct. Loan underscore data dot CSV. Yes, it did. Okay, so let me just copy and paste this variable name so we can print out the results. Okay, so here you can see the table here. Actually, to make this easier, we've got nine and a half thousand rows here. What I'm going to do is I'm just going to import the first thousand rows. And this is going to make some of the results a bit easier to understand. All right, so now we've only got a thousand rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, because it's a categorical column, that's going to become important how we deal with that in a moment. All right, so first of all, we'll just concentrate on the response. So the response variable is called credit dot policy. And so each row is an application. So when that application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one, and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And we're going to take the credit policy column, and then I'm going to copy and paste this variable name again, so you can see the results. So in this case, it is a Pandas series, and you can see it's got ones and zeros. All right, so we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications with a T somewhere in there, and we use every column except credit policy. So this drop method is a little shortcut for just like saying, I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And then I'm going to print this out. So, so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right, so now the crux of this. So we're going to split the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think this one is a response test and then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features. Features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same feature as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here, 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit more in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size. I'm going to set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both the training testing sets. What I mean by that is that by default, the training and testing sets are randomly generated. So each of the rows from the dataset is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument to make any number you like. I'm just going to pick 999. And so because we set the random state, this code's going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice. And so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So we're going to look at features train 2, and you see even though it's random, we have exactly the same results. It's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training testing sets. I hope it's been helpful.\n"}]},{"source":"### Save the transcript to a text file.\n\n- If the directory for the output file doesn't exist, make it.\n- Write the transcript to the output file","metadata":{},"id":"7255d301-69e2-4e04-813d-5a90b5ebcbdc","cell_type":"markdown"},{"source":"# Create the directory for the output file if it doesn't exist\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# Write the transcript to the output file\nwith open(output_file, \"w\") as file:\n    file.write(transcript)","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1715609015415,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the directory for the output file if it doesn't exist\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# Write the transcript to the output file\nwith open(output_file, \"w\") as file:\n    file.write(transcript)","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"6798bff4-ac8d-46e3-8c62-e540655e859d","cell_type":"code","execution_count":8,"outputs":[]},{"source":"## Create a TextLoader using LangChain ","metadata":{},"id":"c1001454-eb29-4981-825f-fa08e2fc48e8","cell_type":"markdown"},{"source":"## Loading Text Data with LangChain\n\nTo utilize text or other types of data with LangChain, we need to convert that data into `Document` objects. This conversion is facilitated by using loaders. In this tutorial, we will employ the `TextLoader` to take the text from our transcript and load it into a `Document`. This process is essential for preparing the data for further processing and analysis within the LangChain framework.\n\nBy following the steps below, you will learn how to:\n\n1. Initialize a `TextLoader`.\n2. Load the transcript text into a `Document`.\n\nLet's get started!\n```","metadata":{},"id":"8191715b-72ad-4a34-ac95-02e1fbf8d391","cell_type":"markdown"},{"source":"### Load the documents from the text file using a TextLoader.\n\n- From the `langchain.document_loaders` module, import `TextLoader`.\n- Create a `TextLoader`, passing it the directory of the transcripts, `\"./files/text\"`. Assign to `loader`.\n- Use the TextLoader to load the documents. Assign to `docs`.","metadata":{},"id":"4f75f541-5bd7-4214-a75e-79681303c6f6","cell_type":"markdown"},{"source":"# From the langchain.document_loaders module, import TextLoader\nfrom langchain.document_loaders import TextLoader\n\n# Create a `TextLoader`, passing the directory of the transcripts. Assign to `loader`.\nloader = TextLoader(\"./files/text\")\n\n# Use the TextLoader to load the documents. Assign to docs.\ndocs = loader.load()","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1715609097066,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From the langchain.document_loaders module, import TextLoader\nfrom langchain.document_loaders import TextLoader\n\n# Create a `TextLoader`, passing the directory of the transcripts. Assign to `loader`.\nloader = TextLoader(\"./files/text\")\n\n# Use the TextLoader to load the documents. Assign to docs.\ndocs = loader.load()","lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"bb8654f7-965e-4e62-98ab-d08b7026e3d9","cell_type":"code","execution_count":15,"outputs":[]},{"source":"# Show the first element of docs to verify it has been loaded \ndocs[0]","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1715609015672,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Show the first element of docs to verify it has been loaded \ndocs[0]","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"269aaed5-7d07-43d7-a2d0-a89730ec4bc9","cell_type":"code","execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":"Document(page_content=\"Hi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating because it's going to make your model appear to perform better than it actually does. So it's giving you a sort of false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using Datacamp Workspace here, and this is one of the data sets that is available as standard with Datacamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as pd. That's the sort of standard alias for it. And then we actually just one function from scikit-learn. So this is in the scikit-learn model selection sub-module. And the function for splitting into training and testing sets is called train-test-split. So let's run that. All right, so this data is about loan applications. So I'm just going to call it loan-applications. And we can use pd.read.csv because it is in a CSV file. And the file is called loan-data.csv. Let me just check and see if I got that correct. loan-data.csv. Yes, it did. Okay, so let me just copy and paste this variable name so we can print out the results. Okay, so here you can see the table here. Actually, to make this easier, we've got 9,500 rows here. What I'm going to do is I'm just going to import the first 1,000 rows. And this is going to make some of the results a bit easier to understand. All right, so now we've only got 1,000 rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, because it's a categorical column, that's going to become important how we deal with that in a moment. All right, so first of all, we'll just concentrate on the response. So the response variable is called credit.policy. And so each row is an application. So when that application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one, and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame, and we're going to take the credit policy column, and then I'm going to copy and paste this variable name again so you can see the results. So in this case, it is a Pandas series, and you can see it's got ones and zeros. All right, so we're going to use all the other columns of features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications, with a t somewhere in there, and we use every column except credit policy. So this drop method is a little shortcut for just like saying I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose, so we need one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And then I'm going to print this out. So so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right, so now the crux of this. So we're going to split the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think this one is a response test and then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly... paste it twice. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here because 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit less in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size, and we set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both the training testing sets. What I mean by that is that by default, the training testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible, despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument. You can make any number you like. I'm just going to pick 999. And so because we set the random state, this code's going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice. And so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So we're going to look at features train two. And you see, even though it's random, we have exactly the same results. So it's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful. you\\n\", metadata={'source': './files/text'})"},"metadata":{},"execution_count":10}]},{"source":"## Create an In-Memory Vector Store ","metadata":{},"id":"577069b3-02f6-4b73-aaaa-d8b8e8006d98","cell_type":"markdown"},{"source":"Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n\nFor large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this project, we can create an in-memory vector store using the `docarray` package. \n\nWe will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. ","metadata":{},"id":"79af9e43-c32f-478d-b057-dc3b7890925e","cell_type":"markdown"},{"source":"### Import the `tiktoken` package. ","metadata":{},"id":"d3a5eb22-3a34-40a5-9f00-bd4895a1c4ca","cell_type":"markdown"},{"source":"#tiktoken package\nimport tiktoken","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1715609015719,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the tiktoken package\nimport tiktoken","lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"15298bd3-5465-450d-b917-5e5d87d78bf2","cell_type":"code","execution_count":11,"outputs":[]},{"source":"## Create the Document Search ","metadata":{},"id":"6e01af9c-f5ea-4382-b7ff-01fe0bb30edc","cell_type":"markdown"},{"source":"We will now use the LangChain library to perform essential operations for creating a Question and Answer experience. This will be achieved using the integrated `RetrievalQA` class, which facilitates the retrieval of relevant information from our vector store to answer user queries.","metadata":{},"id":"22438b44-b8f8-4c78-a573-87ee4bdb2234","cell_type":"markdown"},{"source":"### Instructions\n\n- From the `langchain.chains` module, import `RetrievalQA`.\n- From the `langchain_openai` package, import `ChatOpenAI`, `OpenAIEmbeddings`.\n- From the `langchain.vectorstores` module, import `DocArrayInMemorySearch`.","metadata":{},"id":"e6ffa48b-803d-46b0-b9c2-72f44b5cb5c7","cell_type":"markdown"},{"source":"# From the langchain.chains module, import RetrievalQA\nfrom langchain.chains import RetrievalQA\n\n# From the langchain_openai package, import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# From the langchain.vectorstores module, import DocArrayInMemorySearch\nfrom langchain.vectorstores import DocArrayInMemorySearch","metadata":{"executionCancelledAt":null,"executionTime":93,"lastExecutedAt":1715609221278,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From the langchain.chains module, import RetrievalQA\nfrom langchain.chains import RetrievalQA\n\n# From the langchain_openai package, import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# From the langchain.vectorstores module, import DocArrayInMemorySearch\nfrom langchain.vectorstores import DocArrayInMemorySearch","lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"3a7fb40d-de20-4ec8-b05a-036b6dc6ad66","cell_type":"code","execution_count":17,"outputs":[]},{"source":"### Create an In-Memory Search Object\n\nTo create an in-memory search object, we will use the `DocArrayInMemorySearch` class. This class will allow us to search through the embeddings generated by the `OpenAIEmbeddings` function. \n\nIn this code:\n- We first initialize the `OpenAIEmbeddings` object.\n- Then, we create the `DocArrayInMemorySearch` object by calling the `from_documents` method with our documents (`docs`) and the initialized embeddings.","metadata":{},"id":"665d55d7-25fb-4aeb-9434-6b76de0ee405","cell_type":"markdown"},{"source":"# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\ndb = DocArrayInMemorySearch.from_documents(\n    docs, \n    OpenAIEmbeddings()\n)","metadata":{"executionCancelledAt":null,"executionTime":1299,"lastExecutedAt":1715609226024,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\ndb = DocArrayInMemorySearch.from_documents(\n    docs, \n    OpenAIEmbeddings()\n)","lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"66ef212c-eefd-4cf2-a02c-3c01b1b29118","cell_type":"code","execution_count":18,"outputs":[]},{"source":"We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM.\n\nRecall that the temperature of an LLM refers to how random the results are. Setting the temperature to zero makes the results more repeatable.","metadata":{},"id":"033f3ebc-c098-49e8-a96f-f428940996d9","cell_type":"markdown"},{"source":"- Convert the `DocArrayInMemorySearch` instance to a retriever. Assign to `retriever`.\n- Create a new `ChatOpenAI` instance with a temperature of `0.0`. Assign to `llm`. ","metadata":{},"id":"0aabff95-8fa2-47ea-b0b3-23c53c6d3c38","cell_type":"markdown"},{"source":"# Convert the DocArrayInMemorySearch instance to a retriever\nretriever = db.as_retriever()\n\n# Create a new ChatOpenAI instance \nllm = ChatOpenAI(temperature = 0.0)","metadata":{"executionCancelledAt":null,"executionTime":23,"lastExecutedAt":1715609229317,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Convert the DocArrayInMemorySearch instance to a retriever. Assign to retriever.\nretriever = db.as_retriever()\n\n# Create a new ChatOpenAI instance with a temperature of 0.0\nllm = ChatOpenAI(temperature = 0.0)","lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"7c7f6113-c145-47ff-ab9c-ada04ca047ce","cell_type":"code","execution_count":19,"outputs":[]},{"source":"### Create a new RetrievalQA instance from the chain type. \n\n- Set `llm` to the ChatOpenAI instance you just created.\n- Set `chain_type` to `\"stuff\"`.\n- Set `retriever` to the retriever you just created.\n- Set `verbose` to `True`.","metadata":{},"id":"2a5ce4f9-e025-40e6-b737-be77213d5110","cell_type":"markdown"},{"source":"# RetrievalQA instance \nqa_stuff = RetrievalQA.from_chain_type(\n    llm=llm,            \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True       \n)","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1715609232957,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a new RetrievalQA instance with the specified parameters\nqa_stuff = RetrievalQA.from_chain_type(\n    llm=llm,            # The ChatOpenAI instance to use for generating responses\n    chain_type=\"stuff\", # The type of chain to use for the QA system\n    retriever=retriever, # The retriever to use for retrieving relevant documents\n    verbose=True        # Whether to print verbose output during retrieval and generation\n)","lastExecutedByKernel":"5dc70d31-fcd7-4c86-9495-e9736162d04d"},"id":"09fc202b-198f-4510-8d81-258f914d5c08","cell_type":"code","execution_count":20,"outputs":[]},{"source":"## Queries ","metadata":{},"id":"4ce8cff5-ab49-44f0-96ee-88826d88ea6a","cell_type":"markdown"},{"source":"Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. ","metadata":{},"id":"d51218d4-4e81-4d87-9f3f-77eacde057c1","cell_type":"markdown"},{"source":"### Ask GPT some questions about the transcript.\n\n- Create question, `\"What is this tutorial about?\"`. Assign to `query`.\n- Invoke the query through the RetrievalQA instance. Assign to `response`. \n- Print the response.","metadata":{},"id":"5e2b036b-cef6-4b52-9421-5ccf2b865482","cell_type":"markdown"},{"source":"# Set the query to be used for the QA system\nquery = \"What is this tutorial about?\"\n\n# Invoke the query through the RetrievalQA instance\nresponse = qa_stuff.invoke(query)\n\n# output\nresponse","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"}},"lastExecutedByKernel":null},"id":"d576672c-5078-487a-9dc5-3703f17d82f1","cell_type":"code","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"{'query': 'What is this tutorial about?',\n 'result': \"This tutorial is about a data pre-processing technique for machine learning called splitting your data. It explains the importance of splitting your data set into a training set and a testing set, the problems that can arise if you don't do this, and when in your machine learning workflow you should split your data. The tutorial also provides a practical example using loan application data and demonstrates how to split the data into training and testing sets using Python.\"}"},"metadata":{},"execution_count":21}]},{"source":"We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. ","metadata":{},"id":"de9f2df3-87d1-40d3-862a-95769c11d015","cell_type":"markdown"},{"source":"# Set the query to be used for the QA system\nquery = \"What is the difference between a training set and test set?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.invoke(query)\n\n# Print the response to the console\nresponse","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"}},"lastExecutedByKernel":null},"id":"dbb75225-76c1-4eb8-9055-e13a3bb68682","cell_type":"code","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"{'query': 'What is the difference between a training set and test set?',\n 'result': 'The training set is used to train the machine learning model, meaning the model learns patterns and relationships from this data. The test set, on the other hand, is used to evaluate the performance of the trained model on unseen data. The training set helps the model learn, while the test set helps assess how well the model generalizes to new data.'}"},"metadata":{},"execution_count":22}]},{"source":"# Set the query to be used for the QA system\nquery = \"Who should watch this lesson?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.invoke(query)\n\n# Print the response to the console\nresponse ","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"}},"lastExecutedByKernel":null},"id":"13864a14-0eda-4afd-bfe5-90fdefbc5d49","cell_type":"code","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"{'query': 'Who should watch this lesson?',\n 'result': 'This lesson on splitting data into training and testing sets is beneficial for individuals who are learning or working with machine learning models. It provides essential information on why data splitting is necessary, when to perform it in the machine learning workflow, and how to implement it using Python libraries like pandas and scikit-learn.'}"},"metadata":{},"execution_count":23}]},{"source":"# Set the query to be used for the QA system\nquery = \"Who is the greatest football team on earth?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.invoke(query)\n\n# Print the response to the console\nresponse ","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"}},"lastExecutedByKernel":null},"id":"c62b73e4-e746-49f9-8106-921cbb4e6df8","cell_type":"code","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"{'query': 'Who is the greatest football team on earth?',\n 'result': \"I don't know the answer to that question as it is subjective and varies based on personal preferences and opinions.\"}"},"metadata":{},"execution_count":24}]},{"source":"# Set the query to be used for the QA system\nquery = \"How long is the circumference of the earth?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.invoke(query)\n\n# Print the response to the console\nresponse ","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"},"1":{"height":77,"type":"stream"}},"lastExecutedByKernel":null},"id":"f9f7a7f3-f0f1-44ad-be76-d15aa009ac34","cell_type":"code","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"{'query': 'How long is the circumference of the earth?',\n 'result': \"I don't know the exact length of the circumference of the Earth.\"}"},"metadata":{},"execution_count":25}]},{"source":"## Conclusion of the Project\n\nIn this project, we implemented a question-answering (QA) system using a retrieval-based approach. The primary goal was to enable the system to respond to various queries by retrieving relevant information from a predefined dataset or knowledge base.\n\n### Logic Applied\n\n1. **Query Formulation**: We formulated specific queries that we wanted the QA system to answer. These queries ranged from factual questions like \"What is the difference between a training set and test set?\" to more subjective ones like \"Who is the greatest football team on earth?\"\n\n2. **RetrievalQA Instance**: We utilized a `RetrievalQA` instance, which is designed to process the queries and retrieve the most relevant information from the dataset. This instance likely uses techniques such as vector embeddings and similarity search to find the best matches for the queries.\n\n3. **Response Handling**: For each query, the system invoked the `invoke` method of the `RetrievalQA` instance, which processed the query and returned a response. This response was then printed to the console for review.\n\n### Major Advantages of This Workflow\n\n- **Efficiency**: The retrieval-based approach allows for quick responses to queries by leveraging pre-existing data, making it suitable for real-time applications.\n\n- **Scalability**: As the dataset grows, the system can scale to handle more queries without significant changes to the underlying architecture.\n\n- **Flexibility**: The system can handle a wide range of queries, from factual to subjective, demonstrating its versatility.\n\n- **Ease of Use**: By abstracting the complexity of the retrieval process, the system provides a simple interface for users to interact with, requiring only the input of a query to obtain an answer.\n\nOverall, this project showcases the potential of retrieval-based QA systems in providing accurate and efficient answers to diverse queries, highlighting their applicability in various domains.","metadata":{},"cell_type":"markdown","id":"7f660507-b41d-41cf-b97f-c7503c237c85"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}