{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8fde4c-9222-4265-b72b-8d7693520250",
   "metadata": {},
   "source": [
    "# Mastering Large Language Models and GenerativeAI: An In-Depth Workflow with GPT & LangChain\n",
    "\n",
    "## Skills Involved:\n",
    "- Natural Language Processing (NLP)\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Machine Learning (ML)\n",
    "- Prompt Engineering\n",
    "- Data Preprocessing\n",
    "- API Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e302e1c-4c18-4c44-87fd-ba935c3a0853",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "Welcome to this exciting project where we delve into the world of Large Language Models (LLMs) using GPT and LangChain. This project is designed to showcase the capabilities of generative AI in the context of data analysis and automation. By the end of this project, you will have a comprehensive understanding of how to leverage GPT via the OpenAI API and integrate it with LangChain for various tasks.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset we will be exploring is the [Electric Vehicle Population Data](https://catalog.data.gov/dataset/electric-vehicle-population-data) from Washington state, USA. This dataset provides detailed information about electric vehicles (EVs) registered in Washington, including make, model, year, and other relevant attributes. It serves as an excellent example to demonstrate how GPT can be used to analyze and derive insights from real-world data.\n",
    "\n",
    "## Workflow Process\n",
    "\n",
    "### 1. Setting Up OpenAI Developer Account and Integration with Workspace\n",
    "\n",
    "The first step involves setting up an OpenAI developer account. This will allow you to access the OpenAI API, which is essential for interacting with GPT. You will also integrate this API with your workspace to streamline the development process.\n",
    "\n",
    "### 2. Calling the Chat Functionality in the OpenAI API\n",
    "\n",
    "Once the setup is complete, you will learn how to call the chat functionality in the OpenAI API. This will be done in two ways:\n",
    "- **Without LangChain:** Directly interacting with the OpenAI API to send prompts and receive responses.\n",
    "- **With LangChain:** Utilizing LangChain, a powerful framework that simplifies working with generative AI models, to enhance the interaction with GPT.\n",
    "\n",
    "### 3. Simple Prompt Engineering\n",
    "\n",
    "Prompt engineering is a crucial skill when working with LLMs. You will explore various techniques to craft effective prompts that yield the best possible responses from GPT. This includes understanding the nuances of prompt design and experimenting with different prompt structures.\n",
    "\n",
    "### 4. Holding a Conversation with GPT\n",
    "\n",
    "In this step, you will simulate a conversation with GPT. This will demonstrate the model's ability to maintain context and provide coherent responses over multiple interactions. It is a key feature that showcases the potential of GPT in applications like customer support, virtual assistants, and more.\n",
    "\n",
    "### 5. Incorporating GPT into Data Analysis or Data Science Workflow\n",
    "\n",
    "Finally, you will explore ideas for incorporating GPT into a data analysis or data science workflow. This includes:\n",
    "- **Data Cleaning:** Using GPT to identify and correct inconsistencies in the dataset.\n",
    "- **Data Exploration:** Generating insights and summaries from the data.\n",
    "- **Automated Reporting:** Creating automated reports based on the analysis performed by GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9274661-8d8c-4cc5-901e-5fc497866b89",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf847fd-f8f8-49f6-9b43-0eb098239072",
   "metadata": {},
   "source": [
    "We need to update the `openai` package, and install the `langchain` and `langchain-openai` packages. These are currently being developed quickly, sometimes with breaking changes, so we fix the versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67f9639-2c6e-43e2-8681-46c5a42c1b12",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 15869,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1748962000024,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Install the openai package, locked to version 1.54.4\n!pip install openai==1.54.4\n\n# Install the langchain package, locked to version 0.3.0\n!pip install langchain==0.3.0\n\n# Install the langchain-openai package, locked to version 0.2.8\n!pip install langchain-openai==0.2.8\n\n# Update the typing_extensions package, locked to version 4.12.2\n!pip install typing_extensions==4.12.2",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai==1.54.4\n",
      "  Downloading openai-1.54.4-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.54.4) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.54.4) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.54.4) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.54.4) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.54.4) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.54.4) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.54.4) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai==1.54.4) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.54.4) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.54.4) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.54.4) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.54.4) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.54.4) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.54.4) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.54.4) (2.18.2)\n",
      "Downloading openai-1.54.4-py3-none-any.whl (389 kB)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-1.54.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain==0.3.0\n",
      "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (3.11.12)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (4.0.3)\n",
      "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain==0.3.0)\n",
      "  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.0)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (1.26.4)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain==0.3.0)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0) (0.7.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.0) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.2.2)\n",
      "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, pydantic, langchain-core, langchain-text-splitters, langchain\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "crewai 0.30.11 requires langchain<0.2.0,>=0.1.10, but you have langchain 0.3.0 which is incompatible.\n",
      "embedchain 0.1.113 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.3.0 which is incompatible.\n",
      "langchain-cohere 0.1.5 requires langchain-core<0.3,>=0.1.42, but you have langchain-core 0.3.63 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.63 which is incompatible.\n",
      "langchain-openai 0.1.7 requires langchain-core<0.3,>=0.1.46, but you have langchain-core 0.3.63 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.3.0 langchain-core-0.3.63 langchain-text-splitters-0.3.8 pydantic-2.11.5 pydantic-core-2.33.2 typing-inspection-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain-openai==0.2.8\n",
      "  Downloading langchain_openai-0.2.8-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in /home/repl/.local/lib/python3.10/site-packages (from langchain-openai==0.2.8) (0.3.63)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /home/repl/.local/lib/python3.10/site-packages (from langchain-openai==0.2.8) (1.54.4)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.8) (0.7.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (4.12.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /home/repl/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (2.11.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.8) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.8) (2.32.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/repl/.local/lib/python3.10/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/repl/.local/lib/python3.10/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.8) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.8) (2.3.0)\n",
      "Downloading langchain_openai-0.2.8-py3-none-any.whl (50 kB)\n",
      "Installing collected packages: langchain-openai\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "embedchain 0.1.113 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.3.0 which is incompatible.\n",
      "embedchain 0.1.113 requires langchain-openai<0.2.0,>=0.1.7, but you have langchain-openai 0.2.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-openai-0.2.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the openai package, locked to version 1.54.4\n",
    "!pip install openai==1.54.4\n",
    "\n",
    "# Install the langchain package, locked to version 0.3.0\n",
    "!pip install langchain==0.3.0\n",
    "\n",
    "# Install the langchain-openai package, locked to version 0.2.8\n",
    "!pip install langchain-openai==0.2.8\n",
    "\n",
    "# Update the typing_extensions package, locked to version 4.12.2\n",
    "!pip install typing_extensions==4.12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787756e-02d2-483b-8da2-482366103c8f",
   "metadata": {},
   "source": [
    "In order to chat with GPT, we need first need to load the `openai` and `os` packages to set the API key from the environment variables we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce3946a-d0a7-4a89-96ae-6ad8ac8ca2c8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2125,
    "lastExecutedAt": 1748962002151,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import the os package\nimport os\n\n# Import the openai package\nimport openai\n\n# Set openai.api_key to the OPENAI_API_KEY environment variable\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   },
   "outputs": [],
   "source": [
    "# Import the os package\n",
    "import os\n",
    "\n",
    "# Import the openai package\n",
    "import openai\n",
    "\n",
    "# Set openai.api_key to the OPENAI_API_KEY environment variable\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdeb78-f229-4d3c-8cac-9b2d6c06351e",
   "metadata": {},
   "source": [
    "We need to import the `langchain` package. It has many submodules, so to save typing later, we'll also import some specific functions from those submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3971bc6-1bbd-4e00-9b75-7ad51cfa14e3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 370,
    "lastExecutedAt": 1748962002523,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import the langchain package as lc\nimport langchain as lc\n\n# From the langchain_openai package, import OpenAI, ChatOpenAI\nfrom langchain_openai import OpenAI, ChatOpenAI\n\n# From the langchain.schema module, import AIMessage, HumanMessage, SystemMessage\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage"
   },
   "outputs": [],
   "source": [
    "# Import the langchain package as lc\n",
    "import langchain as lc\n",
    "\n",
    "# From the langchain_openai package, import OpenAI, ChatOpenAI\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "# From the langchain.schema module, import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98530c9-aca9-45bc-8da3-072b78b0c08d",
   "metadata": {},
   "source": [
    "You'll also need to do some light data manipulation with the `pandas` package and data visualization with `plotly.express`.  Finally, the `IPython.display` pacakges contains functions to prettily display Markdown content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81d6d0e-986b-49f4-94e1-7315a7f0bd67",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 271,
    "lastExecutedAt": 1748962002796,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import pandas using the alias pd\nimport pandas as pd\n\n# Import plotly.express using the alias px\nimport plotly.express as px\n\n# From the IPython.display package, import display and Markdown\nfrom IPython.display import display, Markdown"
   },
   "outputs": [],
   "source": [
    "# Import pandas using the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Import plotly.express using the alias px\n",
    "import plotly.express as px\n",
    "\n",
    "# From the IPython.display package, import display and Markdown\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c51c7-bbbb-4f0f-b218-850221f3dcdf",
   "metadata": {},
   "source": [
    "## Import the Electric Cars Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce00845-9741-4979-91aa-e43ff0db5299",
   "metadata": {},
   "source": [
    "The electric cars data is contained in a CSV file named `electric_cars.csv`.\n",
    "\n",
    "Each row in the dataset represents the count of the number of cars registered within a city, for a particular model.\n",
    "\n",
    "The dataset contains the following columns.\n",
    "\n",
    "- `city` (character): The city in which the registered owner resides.\n",
    "- `county` (character): The county in which the registered owner resides.\n",
    "- `model_year` (integer): The [model year](https://en.wikipedia.org/wiki/Model_year#United_States_and_Canada) of the car.\n",
    "- `make` (character): The manufacturer of the car.\n",
    "- `model` (character): The model of the car.\n",
    "- `electric_vehicle_type` (character): Either \"Plug-in Hybrid Electric Vehicle (PHEV)\" or \"Battery Electric Vehicle (BEV)\".\n",
    "- `n_cars` (integer): The count of the number of vehicles registered.\n",
    "\n",
    "Our first step is to import and print the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19bfda1-fe9a-4344-9dd9-613d33598d21",
   "metadata": {},
   "source": [
    "Import the electric cars data to a pandas dataframe.\n",
    "\n",
    "- Read the data from `electric_cars.csv`. Assign to `electric_cars`.\n",
    "- Display a description of the numeric columns of `electric_cars`.\n",
    "- Display a description of the object columns of `electric_cars`.\n",
    "- Print the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "474d98a8-33ce-4898-a641-4853f17e5738",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     },
     "1": {
      "height": 550,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "13fa9deb-55fd-45ed-87ac-aecb3685e1fa",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     },
     "2": {
      "height": 59,
      "type": "stream"
     },
     "3": {
      "height": 550,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "13fa9deb-55fd-45ed-87ac-aecb3685e1fa",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     },
     "4": {
      "height": 332,
      "type": "stream"
     },
     "5": {
      "height": 343,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of numeric columns\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "index": [
          "count",
          "mean",
          "std",
          "min",
          "25%",
          "50%",
          "75%",
          "max"
         ],
         "model_year": [
          26813,
          2019.3755267967,
          3.2862571531,
          1997,
          2017,
          2020,
          2022,
          2024
         ],
         "n_cars": [
          26813,
          5.6121657405,
          26.9973254926,
          1,
          1,
          2,
          4,
          1514
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "string"
          },
          {
           "name": "model_year",
           "type": "number"
          },
          {
           "name": "n_cars",
           "type": "number"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 8,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_year</th>\n",
       "      <th>n_cars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26813.000000</td>\n",
       "      <td>26813.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019.375527</td>\n",
       "      <td>5.612166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.286257</td>\n",
       "      <td>26.997325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1997.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2022.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024.000000</td>\n",
       "      <td>1514.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model_year        n_cars\n",
       "count  26813.000000  26813.000000\n",
       "mean    2019.375527      5.612166\n",
       "std        3.286257     26.997325\n",
       "min     1997.000000      1.000000\n",
       "25%     2017.000000      1.000000\n",
       "50%     2020.000000      2.000000\n",
       "75%     2022.000000      4.000000\n",
       "max     2024.000000   1514.000000"
      ]
     },
     "metadata": {
      "application/com.datacamp.data-table.v2+json": {
       "status": "success"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of text columns\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "city": [
          26813,
          683,
          "Bothell",
          479
         ],
         "county": [
          26813,
          183,
          "King",
          7066
         ],
         "electric_vehicle_type": [
          26813,
          2,
          "Battery Electric Vehicle (BEV)",
          15885
         ],
         "index": [
          "count",
          "unique",
          "top",
          "freq"
         ],
         "make": [
          26813,
          37,
          "TESLA",
          5071
         ],
         "model": [
          26813,
          127,
          "LEAF",
          1889
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "string"
          },
          {
           "name": "city",
           "type": "string"
          },
          {
           "name": "county",
           "type": "string"
          },
          {
           "name": "make",
           "type": "string"
          },
          {
           "name": "model",
           "type": "string"
          },
          {
           "name": "electric_vehicle_type",
           "type": "string"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 4,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>electric_vehicle_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26813</td>\n",
       "      <td>26813</td>\n",
       "      <td>26813</td>\n",
       "      <td>26813</td>\n",
       "      <td>26813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>683</td>\n",
       "      <td>183</td>\n",
       "      <td>37</td>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Bothell</td>\n",
       "      <td>King</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>LEAF</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>479</td>\n",
       "      <td>7066</td>\n",
       "      <td>5071</td>\n",
       "      <td>1889</td>\n",
       "      <td>15885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city county   make  model           electric_vehicle_type\n",
       "count     26813  26813  26813  26813                           26813\n",
       "unique      683    183     37    127                               2\n",
       "top     Bothell   King  TESLA   LEAF  Battery Electric Vehicle (BEV)\n",
       "freq        479   7066   5071   1889                           15885"
      ]
     },
     "metadata": {
      "application/com.datacamp.data-table.v2+json": {
       "status": "success"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The electric cars dataset\n",
      "\n",
      "       city     county  ...           electric_vehicle_type n_cars\n",
      "0   Seattle       King  ...  Battery Electric Vehicle (BEV)   1514\n",
      "1   Seattle       King  ...  Battery Electric Vehicle (BEV)   1153\n",
      "2   Seattle       King  ...  Battery Electric Vehicle (BEV)   1147\n",
      "3   Seattle       King  ...  Battery Electric Vehicle (BEV)   1122\n",
      "4  Bellevue       King  ...  Battery Electric Vehicle (BEV)    931\n",
      "5   Seattle       King  ...  Battery Electric Vehicle (BEV)    805\n",
      "6   Seattle       King  ...  Battery Electric Vehicle (BEV)    770\n",
      "7   Seattle       King  ...  Battery Electric Vehicle (BEV)    705\n",
      "8   Bothell  Snohomish  ...  Battery Electric Vehicle (BEV)    700\n",
      "9   Seattle       King  ...  Battery Electric Vehicle (BEV)    699\n",
      "\n",
      "[10 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the data from electric_cars.csv. Assign to electric_cars.\n",
    "electric_cars = pd.read_csv(\"electric_cars.csv\")\n",
    "\n",
    "# Display a description of the numeric columns\n",
    "print(\"Description of numeric columns\\n\")\n",
    "display(electric_cars.describe())\n",
    "\n",
    "# Display a description of the text (object) columns\n",
    "print(\"Description of text columns\\n\")\n",
    "display(electric_cars.describe(include=\"O\"))\n",
    "\n",
    "# Print the whole dataset\n",
    "print(\"The electric cars dataset\\n\")\n",
    "print(electric_cars.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65454beb-970f-4af6-a04c-798b9f665b6f",
   "metadata": {},
   "source": [
    "## Asking GPT a Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb43b1-9455-456e-a674-34b58e2faae2",
   "metadata": {},
   "source": [
    "Let's start by sending a message to GPT and getting a response. For now, we won't worry about including any details about the dataset&mdash;it's the equivalent of asking \"is this microphone turned on?\".\n",
    "\n",
    "We'll also skip using langchain for now so you can see more clearly how the `openai` packages works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a3334-8917-41cb-9300-3977744e702b",
   "metadata": {},
   "source": [
    "Send a question to GPT and get a response.\n",
    "\n",
    "- We will define the system message as follows and assign to `system_msg_test`.\n",
    "\n",
    "```\n",
    "\"\"\"You are a helpful assistant who understands data science.\n",
    " You write in a clear language that a ten year old can understand.\n",
    " You keep your answers brief.\"\"\". \n",
    "```\n",
    "    \n",
    "- We also define the user message as follows and assign to `user_msg_test`.\n",
    "\n",
    "```\n",
    "\"Tell me some uses of GPT for data analysis.\"\n",
    "```\n",
    "\n",
    "- Create a message list from the system and user messages. Assign to `msgs_test`.\n",
    "- Use the `openai` package to define an `OpenAI` client. Assign to `client`.\n",
    "- Send the messages to GPT. Assign to `rsps_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1175,
    "id": "bA5ajAmk7XH6",
    "lastExecutedAt": 1748962004085,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Define the system message. Assign to system_msg_test.\nsystem_msg_test = \"\"\"You are a helpful assistant who understands data science.\n You write in a clear language that a ten year old can understand.\n You keep your answers brief.\"\"\" \n\n# Define the user message. Assign to user_msg_test.\nuser_msg_test = \"Tell me some uses of GPT for data analysis.\"\n\n# Create a message list from the system and user messages. Assign to msgs_test.\nmsgs_test = [\n    {\"role\": \"system\", \"content\": system_msg_test},\n    {\"role\": \"user\", \"content\": user_msg_test}\n]\n\n# Use the openai pacakge to define an OpenAI client. Assign to client.\nclient = openai.OpenAI()\n\n# Send the messages to GPT. Assign to rsps_test.\nrsps_test = client.chat.completions.create(\n    model = \"gpt-3.5-turbo\",\n    messages = msgs_test)"
   },
   "outputs": [],
   "source": [
    "# Define the system message. Assign to system_msg_test.\n",
    "system_msg_test = \"\"\"You are a helpful assistant who understands data science.\n",
    " You write in a clear language that a ten year old can understand.\n",
    " You keep your answers brief.\"\"\" \n",
    "\n",
    "# Define the user message. Assign to user_msg_test.\n",
    "user_msg_test = \"Tell me some uses of GPT for data analysis.\"\n",
    "\n",
    "# Create a message list from the system and user messages. Assign to msgs_test.\n",
    "msgs_test = [\n",
    "    {\"role\": \"system\", \"content\": system_msg_test},\n",
    "    {\"role\": \"user\", \"content\": user_msg_test}\n",
    "]\n",
    "\n",
    "# Use the openai pacakge to define an OpenAI client. Assign to client.\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Send the messages to GPT. Assign to rsps_test.\n",
    "rsps_test = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = msgs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc4dab-01d7-4027-8e90-da027cd9bfd9",
   "metadata": {},
   "source": [
    "Print the whole response and just the text content.\n",
    "\n",
    "- Print the whole response.\n",
    "- Print just the response's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20279022-1034-4fc8-8c34-deb411dfeebd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1748962004134,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Print the whole response\nprint(\"The whole response\\n\")\nprint(rsps_test)\n\n\nprint(\"\\n\\n----\\n\\n\")\n\n# Print just the response's content\nprint(\"Just the response's content\\n\")\nprint(rsps_test.choices[0].message.content)\n",
    "outputsMetadata": {
     "0": {
      "height": 332,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole response\n",
      "\n",
      "ChatCompletion(id='chatcmpl-BeNCV4IwOQcBPFT8OVdjEAsTV3Rk3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='GPT can generate text for reports, summarize data, and answer questions about data sets.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1748962003, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=52, total_tokens=70, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Just the response's content\n",
      "\n",
      "GPT can generate text for reports, summarize data, and answer questions about data sets.\n"
     ]
    }
   ],
   "source": [
    "# Print the whole response\n",
    "print(\"The whole response\\n\")\n",
    "print(rsps_test)\n",
    "\n",
    "\n",
    "print(\"\\n\\n----\\n\\n\")\n",
    "\n",
    "# Print just the response's content\n",
    "print(\"Just the response's content\\n\")\n",
    "print(rsps_test.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f8b8b-5ef6-412a-a432-74dd103c141c",
   "metadata": {},
   "source": [
    "## Asking a Question About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58127a5-39d4-491e-8f31-7e90f1da6e8e",
   "metadata": {},
   "source": [
    "Now we know that GPT is working, we can start asking questions about data analysis. Because we have details of our dataset, we can pass these in to our prompt to improve the quality of the messages we get back.\n",
    "\n",
    "Another change that we're going to make is to use the `langchain` package, which provides a convenience layer on top of the `openai` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852a0d9-5a94-4611-89af-30c401b3d398",
   "metadata": {},
   "source": [
    "Here we will create a prompt that includes dataset details.\n",
    "\n",
    "- _Read the description of the dataset that is provided._\n",
    "- Create a task for the AI. Assign to `suggest_questions`.\n",
    "    - Use the text `\"Suggest some data analysis questions that could be answered with this dataset.\"`.\n",
    "- Concatenate the dataset description and the request. Assign to `msgs_suggest_questions`.\n",
    "    - The first message is a system message with the content `\"You are a data analysis expert.\"`.\n",
    "    - The second message is a human message with `dataset_description` and `suggest_questions` concatenated with two line breaks in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "858797d3-ca37-4952-9a10-1f17b6703df0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1748962004181,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# A description of the dataset\ndataset_description = \"\"\"\nYou have a dataset about electric cars registered in Washington state, USA in 2020. It is available as a pandas DataFrame named `electric_cars`.\n\nEach row in the dataset represents the count of the number of cars registered within a city, for a particular model.\n\nThe dataset contains the following columns.\n\n- `city` (character): The city in which the registered owner resides.\n- `county` (character): The county in which the registered owner resides.\n- `model_year` (integer): The [model year](https://en.wikipedia.org/wiki/Model_year#United_States_and_Canada) of the car.\n- `make` (character): The manufacturer of the car.\n- `model` (character): The model of the car.\n- `electric_vehicle_type` (character): Either \"Plug-in Hybrid Electric Vehicle (PHEV)\" or \"Battery Electric Vehicle (BEV)\".\n- `n_cars` (integer): The count of the number of vehicles registered.\n\"\"\"\n\n# Create a task for the AI. Assign to suggest_questions.\nsuggest_questions = \"Suggest some data analysis questions that could be answered with this dataset\"\n\n# Concatenate the dataset description and the request. Assign to msgs_suggest_questions.\nmsgs_suggest_questions = [\n    SystemMessage(content = \"You are a data analysis expert.\"),\n    HumanMessage(content = f\"{dataset_description} + \\n\\n + {suggest_questions}\")\n]"
   },
   "outputs": [],
   "source": [
    "# A description of the dataset\n",
    "dataset_description = \"\"\"\n",
    "You have a dataset about electric cars registered in Washington state, USA in 2020. It is available as a pandas DataFrame named `electric_cars`.\n",
    "\n",
    "Each row in the dataset represents the count of the number of cars registered within a city, for a particular model.\n",
    "\n",
    "The dataset contains the following columns.\n",
    "\n",
    "- `city` (character): The city in which the registered owner resides.\n",
    "- `county` (character): The county in which the registered owner resides.\n",
    "- `model_year` (integer): The [model year](https://en.wikipedia.org/wiki/Model_year#United_States_and_Canada) of the car.\n",
    "- `make` (character): The manufacturer of the car.\n",
    "- `model` (character): The model of the car.\n",
    "- `electric_vehicle_type` (character): Either \"Plug-in Hybrid Electric Vehicle (PHEV)\" or \"Battery Electric Vehicle (BEV)\".\n",
    "- `n_cars` (integer): The count of the number of vehicles registered.\n",
    "\"\"\"\n",
    "\n",
    "# Create a task for the AI. Assign to suggest_questions.\n",
    "suggest_questions = \"Suggest some data analysis questions that could be answered with this dataset\"\n",
    "\n",
    "# Concatenate the dataset description and the request. Assign to msgs_suggest_questions.\n",
    "msgs_suggest_questions = [\n",
    "    SystemMessage(content = \"You are a data analysis expert.\"),\n",
    "    HumanMessage(content = f\"{dataset_description} + \\n\\n + {suggest_questions}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f085a-abc2-48c1-b77a-f655a1441d8e",
   "metadata": {},
   "source": [
    "- Create a `ChatOpenAI` object. Assign to `chat`.\n",
    "- Pass your message to GPT. Assign to `rsps_suggest_questions`.\n",
    "- Print the response object and the contents of the response.\n",
    "- Print the type of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93bb457c-134f-4349-81ae-c8c6c6bc1d30",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3276,
    "lastExecutedAt": 1748962007458,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create a ChatOpenAI object. Assign to chat.\nchat = ChatOpenAI()\n\n# Pass your message to GPT. Assign to rsps_suggest_questions.\nrsps_suggest_questions = chat.invoke(msgs_suggest_questions)\n\n# Print the response\nprint(\"The whole response\\n\")\nprint(rsps_suggest_questions)\n\nprint(\"\\n----\\n\")\n\n# Print just the response's content\nprint(\"Just the response's content\\n\")\nprint(rsps_suggest_questions.content)\n\nprint(\"\\n----\\n\")\n\n# Print the type of the response\nprint(\"The type of the response\\n\")\nprint(type(rsps_suggest_questions))",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole response\n",
      "\n",
      "content='Here are some data analysis questions that could be answered with the provided dataset `electric_cars`:\\n\\n1. What is the distribution of electric car registrations by make and model in Washington state in 2020?\\n2. Which county in Washington state had the highest number of electric cars registered in 2020?\\n3. How does the number of registered electric vehicles vary between Plug-in Hybrid Electric Vehicles (PHEVs) and Battery Electric Vehicles (BEVs) in different cities?\\n4. What are the most popular electric car models registered in Washington state in 2020?\\n5. Is there a correlation between the model year of an electric car and the number of cars registered in a city?\\n6. Can we identify any trends or patterns in the registration of electric vehicles based on the manufacturer (make) in different counties?\\n7. Which city in Washington state had the highest proportion of Battery Electric Vehicles (BEVs) registered in 2020?\\n8. Are there any outliers in terms of the number of electric cars registered for a specific model in a particular city?\\n9. How does the total number of electric cars registered in each city vary based on the electric vehicle type (PHEV vs. BEV)?\\n10. Is there a relationship between the number of electric cars registered and the population size of a city in Washington state?\\n\\nThese questions can provide valuable insights into the trends, preferences, and distribution of electric cars in Washington state in 2020.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 293, 'prompt_tokens': 238, 'total_tokens': 531, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--0cae5f20-28ad-4d52-a494-c500a14bd721-0' usage_metadata={'input_tokens': 238, 'output_tokens': 293, 'total_tokens': 531, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "----\n",
      "\n",
      "Just the response's content\n",
      "\n",
      "Here are some data analysis questions that could be answered with the provided dataset `electric_cars`:\n",
      "\n",
      "1. What is the distribution of electric car registrations by make and model in Washington state in 2020?\n",
      "2. Which county in Washington state had the highest number of electric cars registered in 2020?\n",
      "3. How does the number of registered electric vehicles vary between Plug-in Hybrid Electric Vehicles (PHEVs) and Battery Electric Vehicles (BEVs) in different cities?\n",
      "4. What are the most popular electric car models registered in Washington state in 2020?\n",
      "5. Is there a correlation between the model year of an electric car and the number of cars registered in a city?\n",
      "6. Can we identify any trends or patterns in the registration of electric vehicles based on the manufacturer (make) in different counties?\n",
      "7. Which city in Washington state had the highest proportion of Battery Electric Vehicles (BEVs) registered in 2020?\n",
      "8. Are there any outliers in terms of the number of electric cars registered for a specific model in a particular city?\n",
      "9. How does the total number of electric cars registered in each city vary based on the electric vehicle type (PHEV vs. BEV)?\n",
      "10. Is there a relationship between the number of electric cars registered and the population size of a city in Washington state?\n",
      "\n",
      "These questions can provide valuable insights into the trends, preferences, and distribution of electric cars in Washington state in 2020.\n",
      "\n",
      "----\n",
      "\n",
      "The type of the response\n",
      "\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "# Create a ChatOpenAI object. Assign to chat.\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# Pass your message to GPT. Assign to rsps_suggest_questions.\n",
    "rsps_suggest_questions = chat.invoke(msgs_suggest_questions)\n",
    "\n",
    "# Print the response\n",
    "print(\"The whole response\\n\")\n",
    "print(rsps_suggest_questions)\n",
    "\n",
    "print(\"\\n----\\n\")\n",
    "\n",
    "# Print just the response's content\n",
    "print(\"Just the response's content\\n\")\n",
    "print(rsps_suggest_questions.content)\n",
    "\n",
    "print(\"\\n----\\n\")\n",
    "\n",
    "# Print the type of the response\n",
    "print(\"The type of the response\\n\")\n",
    "print(type(rsps_suggest_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606312e-389f-4762-a324-f64c019247cf",
   "metadata": {},
   "source": [
    "## Hold a conversation with GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8a719-267a-41bb-9c4a-da6fd3d670b2",
   "metadata": {},
   "source": [
    "Notice that the response from GPT was a dictionary-like object. The most useful part of this is the `.content` element, which contains the text response to your prompt.\n",
    "\n",
    "While a single prompt and response can be useful, often you want to have a longer conversation with GPT. In this case, you can pass previous messages so that GPT can \"remember\" what was said before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f45997-69d0-4e6b-9766-af82d482c28e",
   "metadata": {},
   "source": [
    "### Displaying Markdown content\n",
    "\n",
    "When GPT generates code as an output, it if often formatted as a Markdown code block inside triple backticks. You can display Markdown output more beautifully in Workspace by swapping `print()` for the `display()` and `Markdown()` functions.\n",
    "\n",
    "```py\n",
    "display(Markdown(your_markdown_text))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db3752-ff50-420e-89c9-0a2b88dbe848",
   "metadata": {},
   "source": [
    "Append another prompt to the conversation and chat with GPT again.\n",
    "\n",
    "- Append the response and a new message to the previous messages. Assign to `msgs_python_top_models`.\n",
    "- Pass your message to GPT. Assign to `rsps_python_top_models`.\n",
    "- Display the response's Markdown content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f9d287f-9c19-4c7b-b534-2de5a4ac3b18",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2039,
    "lastExecutedAt": 1748962009497,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Append the response and a new message to the previous messages. \n# Assign to msgs_python_top_models.\nmsgs_python_top_models =  msgs_suggest_questions + [\n    rsps_suggest_questions,\n    HumanMessage(content = \"Write some Python code to find the top 5 most popular make/model combinations of electric car in Washington.\")\n]\n\n# Pass your message to GPT. Assign to rsps_python_top_models.\nrsps_python_top_models = chat(msgs_python_top_models)\n\n# Display the response's Markdown content\ndisplay(Markdown(rsps_python_top_models.content))",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_456/3878508451.py:9: LangChainDeprecationWarning:\n",
      "\n",
      "The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You can use the following Python code snippet to find the top 5 most popular make/model combinations of electric cars in Washington state based on the provided dataset `electric_cars`:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "# Assuming electric_cars is the pandas DataFrame containing the dataset\n",
       "\n",
       "# Group the data by make and model, and sum the number of cars registered for each combination\n",
       "popular_make_model = electric_cars.groupby(['make', 'model'])['n_cars'].sum().reset_index()\n",
       "\n",
       "# Sort the data in descending order based on the total number of cars registered\n",
       "popular_make_model = popular_make_model.sort_values(by='n_cars', ascending=False)\n",
       "\n",
       "# Get the top 5 most popular make/model combinations\n",
       "top_5_popular_make_model = popular_make_model.head(5)\n",
       "\n",
       "print(top_5_popular_make_model)\n",
       "```\n",
       "\n",
       "This code snippet will group the data by make and model, summing the number of cars registered for each combination. It will then sort the combinations in descending order based on the total number of cars registered and extract the top 5 most popular make/model combinations of electric cars in Washington state."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Append the response and a new message to the previous messages. \n",
    "# Assign to msgs_python_top_models.\n",
    "msgs_python_top_models =  msgs_suggest_questions + [\n",
    "    rsps_suggest_questions,\n",
    "    HumanMessage(content = \"Write some Python code to find the top 5 most popular make/model combinations of electric car in Washington.\")\n",
    "]\n",
    "\n",
    "# Pass your message to GPT. Assign to rsps_python_top_models.\n",
    "rsps_python_top_models = chat.invoke(msgs_python_top_models)\n",
    "\n",
    "# Display the response's Markdown content\n",
    "display(Markdown(rsps_python_top_models.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275758d7-5f0d-4b75-a61f-24c588a84bcb",
   "metadata": {},
   "source": [
    "## Execute the Code Provided by GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27c93f-255c-4b90-b2e8-f38b7a5dcad9",
   "metadata": {},
   "source": [
    "We just asked GPT to write some code for us. Next we will need to see if it worked, and fix it if it didn't. \n",
    "\n",
    "This is a standard workflow for interacting with generative AI: the AI acts as a junior data analyst who writes the code, then you act as the boss who reviews the work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bf0d5-6c65-4aef-8a40-72c84dba1f1e",
   "metadata": {},
   "source": [
    "Review the work of your AI assistant.\n",
    "\n",
    "- Copy and paste the code generated by GPT into the next code cell and run it.\n",
    "- _Look at the result. Do you think it is correct?_*\n",
    "- If the code threw an error or gave a wrong answer, use the Workspace AI Assistant (also powered by GPT!) to fix and explain the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37ec0b93-8a67-4f02-9de0-0be59127f70e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1748962009549,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# We paste the code generated by GPT and run it\n\n# Grouping the data by 'make' and 'model', summing the 'n_cars' column to get the total registrations for each combination\npopular_make_model = electric_cars.groupby(['make', 'model'])['n_cars'].sum().reset_index()\n\n# Sorting the data to find the top 5 most popular make/model combinations\ntop_5_popular_make_model = popular_make_model.sort_values(by='n_cars', ascending=False).head(5)\n\n# Displaying the top 5 most popular make/model combinations\nprint(top_5_popular_make_model)",
    "outputsMetadata": {
     "0": {
      "height": 143,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          make    model  n_cars\n",
      "108      TESLA  MODEL Y   28502\n",
      "105      TESLA  MODEL 3   27708\n",
      "91      NISSAN     LEAF   13186\n",
      "106      TESLA  MODEL S    7611\n",
      "30   CHEVROLET  BOLT EV    5733\n"
     ]
    }
   ],
   "source": [
    "# We paste the code generated by GPT and run it\n",
    "\n",
    "# Grouping the data by 'make' and 'model', summing the 'n_cars' column to get the total registrations for each combination\n",
    "popular_make_model = electric_cars.groupby(['make', 'model'])['n_cars'].sum().reset_index()\n",
    "\n",
    "# Sorting the data to find the top 5 most popular make/model combinations\n",
    "top_5_popular_make_model = popular_make_model.sort_values(by='n_cars', ascending=False).head(5)\n",
    "\n",
    "# Displaying the top 5 most popular make/model combinations\n",
    "print(top_5_popular_make_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b52c8-324f-40bd-826d-2b10993267b2",
   "metadata": {},
   "source": [
    "## Continue the Conversation to Create a Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f33e51-9614-4e11-8a27-ff735c184cec",
   "metadata": {},
   "source": [
    "Doing more analysis with GPT assistance is simply a case of continuing the conversation by appending new `HumanMessage` prompts to the message list, and calling the `chat.invoke()` method.\n",
    "\n",
    "The output from GPT is random, but when doing data analysis this isn't always desirable since you'd like your results to be reproducible. With large language models, the amount of randomness can be controled with a parameter known as \"temperature\".\n",
    "\n",
    "- `temperature` controls the randomness of the response. It ranges from `0` to `2` with zero meaning minimal randomness (to make it easier to reproduce results) and two meaning maximum randomness (often gives weird responses). If you use the OpenAI API directly, the default is `1`, but using LangChain reduces the default value to `0.7`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043595b5-1d29-4aac-a428-81703e1300d1",
   "metadata": {},
   "source": [
    "- Create a new OpenAI chat object with temperature set to zero. Assign to `chat0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "649a18e2-cc07-47fb-ab80-735df0366743",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 101,
    "lastExecutedAt": 1748962009650,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create a new OpenAI chat object with temperature set to zero. Assign to chat0.\nchat0 = ChatOpenAI(temperature=0) # no randomness at all\n"
   },
   "outputs": [],
   "source": [
    "# Create a new OpenAI chat object with temperature set to zero. Assign to chat0.\n",
    "chat0 = ChatOpenAI(temperature=0) # no randomness at all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17e508-a08e-4b5e-bc6f-288065c395f9",
   "metadata": {},
   "source": [
    "- Work through the previous conversation flow again, appending the previous response and a new request for Python code to draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type.\n",
    "    - The solution asks for Plotly Express code, but you can pick any Python data viz package you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed9bf9e6-9fac-4703-bcdb-4a327ba62724",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3047,
    "lastExecutedAt": 1748962012700,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Ask GPT for code for a bar plot, as detailed in the instructions\n\nmsg_python_plot = msgs_python_top_models + [\n    rsps_python_top_models,\n    HumanMessage(content = \"Write some Python code to draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type. Use the Plotly Express package.\")\n]\n\nrsps_python_plot = chat0.invoke(msg_python_plot)\n\ndisplay(Markdown(rsps_python_plot.content))"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type using the Plotly Express package in Python, you can use the following code snippet:\n",
       "\n",
       "```python\n",
       "import plotly.express as px\n",
       "\n",
       "# Assuming electric_cars is the pandas DataFrame containing the dataset\n",
       "\n",
       "# Create a bar plot using Plotly Express\n",
       "fig = px.bar(electric_cars, x='model_year', y='n_cars', color='electric_vehicle_type',\n",
       "             labels={'model_year': 'Model Year', 'n_cars': 'Total Count of Electric Cars'},\n",
       "             title='Total Count of Electric Cars by Model Year',\n",
       "             barmode='group')\n",
       "\n",
       "# Show the plot\n",
       "fig.show()\n",
       "```\n",
       "\n",
       "In this code snippet:\n",
       "- We use Plotly Express to create a bar plot where the x-axis represents the model year, the y-axis represents the total count of electric cars, and the bars are colored based on the electric vehicle type (PHEV or BEV).\n",
       "- The `labels` parameter is used to set custom labels for the x-axis and y-axis.\n",
       "- The `title` parameter sets the title of the plot.\n",
       "- The `barmode='group'` parameter groups the bars by model year.\n",
       "\n",
       "You can run this code snippet in a Python environment with the Plotly Express package installed to visualize the total count of electric cars by model year, colored by electric vehicle type."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ask GPT for code for a bar plot, as detailed in the instructions\n",
    "\n",
    "msg_python_plot = msgs_python_top_models + [\n",
    "    rsps_python_top_models,\n",
    "    HumanMessage(content = \"Write some Python code to draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type. Use the Plotly Express package.\")\n",
    "]\n",
    "\n",
    "rsps_python_plot = chat0.invoke(msg_python_plot)\n",
    "\n",
    "display(Markdown(rsps_python_plot.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cde113-6e53-4681-bec6-cfdc8d7d2de0",
   "metadata": {},
   "source": [
    "To see how much variation there is with temperature set to zero, ask GPT for the same thing again.\n",
    "\n",
    "- Call GPT again with the same message list and display the response.\n",
    "- _Look at the response content. How close is it to the previous response content?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a039f5eb-75a6-4cec-9d5f-a8bfa2afbff8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2802,
    "lastExecutedAt": 1748962015502,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call GPT again with the same message list and display the response\n\nrsps_python_plot_1 = chat.invoke(msg_python_plot)\n\ndisplay(Markdown(rsps_python_plot_1.content))"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type using Plotly Express, you can use the following Python code:\n",
       "\n",
       "```python\n",
       "import plotly.express as px\n",
       "\n",
       "# Assuming electric_cars is the pandas DataFrame containing the dataset\n",
       "\n",
       "# Group the data by model year and electric vehicle type, and sum the number of cars registered for each combination\n",
       "total_count_by_model_year = electric_cars.groupby(['model_year', 'electric_vehicle_type'])['n_cars'].sum().reset_index()\n",
       "\n",
       "# Plot the bar plot using Plotly Express\n",
       "fig = px.bar(total_count_by_model_year, x='model_year', y='n_cars', color='electric_vehicle_type',\n",
       "             labels={'model_year': 'Model Year', 'n_cars': 'Total Count of Electric Cars'},\n",
       "             title='Total Count of Electric Cars by Model Year (Colored by Electric Vehicle Type)')\n",
       "\n",
       "fig.show()\n",
       "```\n",
       "\n",
       "This code snippet will group the data by model year and electric vehicle type, summing the number of cars registered for each combination. It will then create a bar plot using Plotly Express, with the x-axis representing the model year, the y-axis representing the total count of electric cars, and the bars colored by electric vehicle type (PHEV or BEV)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call GPT again with the same message list and display the response\n",
    "\n",
    "rsps_python_plot_1 = chat.invoke(msg_python_plot)\n",
    "\n",
    "display(Markdown(rsps_python_plot_1.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa57fd-5901-421c-bfd0-04cdb3bbfea4",
   "metadata": {},
   "source": [
    "## Execute the Code Provided by GPT to See Your Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc700e-2633-4c1b-8fa6-635499c480dd",
   "metadata": {},
   "source": [
    "Setting temperature to zero removed all randomness so you got the same output twice. That makes your workflow more reproducible.\n",
    "\n",
    "The final task is to see the plot. As before, remember that GPT is only your assistant and you are the boss. Check the code and its output to make sure that you really have what you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9521e-b190-4997-ab87-e55a4cd1d402",
   "metadata": {},
   "source": [
    "Run the code and check that the plot is correct.\n",
    "\n",
    "- Run the bar plot code generated by GPT.\n",
    "- _Check that the output is suitable. If not, try changing your prompt in the previous task to improve the output (this is prompt engineering, which you'll see more of in the next code-along project in the series)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d09c630-472c-49d5-8986-0ce986439b96",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 142,
    "lastExecutedAt": 1748962015644,
    "lastExecutedByKernel": "1483c79b-aee9-4d7b-bbf9-6584450b5100",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Paste the code generated by GPT and run it\nimport plotly.express as px\n\n# Grouping the data by 'model_year' and 'electric_vehicle_type', summing the 'n_cars' column to get the total registrations for each combination\ntotal_count_by_year_type = electric_cars.groupby(['model_year', 'electric_vehicle_type'])['n_cars'].sum().reset_index()\n\n# Creating the bar plot using Plotly Express\nfig = px.bar(total_count_by_year_type, x='model_year', y='n_cars', color='electric_vehicle_type',\n             labels={'model_year': 'Model Year', 'n_cars': 'Total Count of Electric Cars'},\n             title='Total Count of Electric Cars by Model Year (Colored by Electric Vehicle Type)')\n\n# Displaying the plot\nfig.show()",
    "outputsMetadata": {
     "0": {
      "height": 467,
      "type": "plotly"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "plotlyServerURL": "https://plot.ly",
        "toImageButtonOptions": {
         "filename": "DataLab plot",
         "format": "png",
         "height": 500,
         "scale": 2,
         "width": 700
        }
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "electric_vehicle_type=Battery Electric Vehicle (BEV)<br>Model Year=%{x}<br>Total Count of Electric Cars=%{y}<extra></extra>",
         "legendgroup": "Battery Electric Vehicle (BEV)",
         "marker": {
          "color": "#6568a0",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Battery Electric Vehicle (BEV)",
         "offsetgroup": "Battery Electric Vehicle (BEV)",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          1997,
          1998,
          1999,
          2000,
          2002,
          2003,
          2008,
          2010,
          2011,
          2012,
          2013,
          2014,
          2015,
          2016,
          2017,
          2018,
          2019,
          2020,
          2021,
          2022,
          2023,
          2024
         ],
         "xaxis": "x",
         "y": [
          1,
          1,
          4,
          8,
          2,
          1,
          18,
          21,
          718,
          776,
          2926,
          1796,
          3625,
          3867,
          4466,
          10046,
          8771,
          9617,
          14998,
          23511,
          31360,
          271
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "electric_vehicle_type=Plug-in Hybrid Electric Vehicle (PHEV)<br>Model Year=%{x}<br>Total Count of Electric Cars=%{y}<extra></extra>",
         "legendgroup": "Plug-in Hybrid Electric Vehicle (PHEV)",
         "marker": {
          "color": "#43d7a4",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Plug-in Hybrid Electric Vehicle (PHEV)",
         "offsetgroup": "Plug-in Hybrid Electric Vehicle (PHEV)",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          2010,
          2011,
          2012,
          2013,
          2014,
          2015,
          2016,
          2017,
          2018,
          2019,
          2020,
          2021,
          2022,
          2023,
          2024
         ],
         "xaxis": "x",
         "y": [
          3,
          78,
          857,
          1639,
          1817,
          1309,
          1783,
          4108,
          4395,
          1945,
          1677,
          3686,
          4288,
          5719,
          371
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "title": {
          "text": "electric_vehicle_type"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#6568a0",
           "#43d7a4",
           "#4095db",
           "#facc5f",
           "#cae279",
           "#7db64f",
           "#f08083",
           "#5bcdf2",
           "#f099dc",
           "#965858"
          ],
          "font": {
           "color": "#05192D",
           "family": "Studio-Feixen-Sans, Arial, sans-serif"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left",
           "font": {
            "family": "Studio-Feixen-Sans, Arial, sans-serif"
           }
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Count of Electric Cars by Model Year (Colored by Electric Vehicle Type)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Model Year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Total Count of Electric Cars"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paste the code generated by GPT and run it\n",
    "import plotly.express as px\n",
    "\n",
    "# Grouping the data by 'model_year' and 'electric_vehicle_type', summing the 'n_cars' column to get the total registrations for each combination\n",
    "total_count_by_year_type = electric_cars.groupby(['model_year', 'electric_vehicle_type'])['n_cars'].sum().reset_index()\n",
    "\n",
    "# Creating the bar plot using Plotly Express\n",
    "fig = px.bar(total_count_by_year_type, x='model_year', y='n_cars', color='electric_vehicle_type',\n",
    "             labels={'model_year': 'Model Year', 'n_cars': 'Total Count of Electric Cars'},\n",
    "             title='Total Count of Electric Cars by Model Year (Colored by Electric Vehicle Type)')\n",
    "\n",
    "# Displaying the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61448d-2571-4ffa-ac93-47d0eba4c94e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Insights from the Data Analysis\n",
    "\n",
    "1. **Electric Vehicle Trends**: The analysis revealed a clear upward trend in the registration of electric vehicles over the years. This indicates a growing adoption of electric vehicles, likely driven by increasing environmental awareness and advancements in electric vehicle technology.\n",
    "\n",
    "2. **Vehicle Type Distribution**: By categorizing the data by electric vehicle type, we observed that certain types of electric vehicles are more popular than others. This insight can help manufacturers and policymakers understand consumer preferences and tailor their strategies accordingly.\n",
    "\n",
    "3. **Model Year Impact**: The model year analysis showed that newer models tend to have higher registration numbers. This could be due to improvements in technology, better marketing, or incentives for newer models.\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "In this project, we leveraged the capabilities of GPT through the OpenAI API and LangChain to assist in data analysis and visualization. Here are the key steps and skills demonstrated:\n",
    "\n",
    "1. **Data Preparation**: We started by preparing the dataset, ensuring it was clean and structured for analysis. This step is crucial for accurate and meaningful insights.\n",
    "\n",
    "2. **GPT Assistance**: We used GPT to generate ideas for analyses and to write code snippets. This demonstrated how AI can be a valuable assistant in the data analysis process, helping to streamline workflows and enhance productivity.\n",
    "\n",
    "3. **Visualization**: Using Plotly Express, we created a bar plot to visualize the total count of electric cars by model year, categorized by electric vehicle type. Visualization is a powerful tool to communicate insights effectively.\n",
    "\n",
    "4. **Reproducibility**: We discussed the importance of reproducibility in data analysis. By setting the temperature to zero, we ensured that the outputs were consistent, making our workflow more reliable.\n",
    "\n",
    "### Skills Demonstrated\n",
    "\n",
    "- **Data Cleaning and Preparation**: Ensuring data quality for accurate analysis.\n",
    "- **AI Integration**: Using GPT for generating code and ideas, showcasing the potential of AI in data science.\n",
    "- **Data Visualization**: Creating clear and informative visualizations to present findings.\n",
    "- **Reproducibility**: Implementing practices to ensure consistent and reliable results.\n",
    "\n",
    "This project not only provided valuable insights into electric vehicle trends but also demonstrated the effective use of AI tools in enhancing data analysis workflows. By combining human expertise with AI assistance, we can achieve more efficient and insightful analyses."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
