{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with DistilGPT-2: A Hyperparameter Exploration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project demonstrates a systematic workflow for text generation using decoder-only transformer models, specifically focusing on DistilGPT-2. Rather than presenting an optimized production solution, this work serves as:\n",
    "\n",
    "- A practical exploration of text generation techniques\n",
    "- A hyperparameter tuning case study\n",
    "- An example of methodical experimentation with language models\n",
    "\n",
    "The implementation showcases how different generation strategies (greedy search, beam search, sampling with temperature) affect output quality while maintaining the same core architecture. This is particularly valuable for understanding the trade-offs between determinism and creativity in autoregressive generation.\n",
    "\n",
    "## Workflow Process\n",
    "\n",
    "1. **Model Initialization**\n",
    "   - Loaded pre-trained DistilGPT-2 model and tokenizer\n",
    "   - Configured padding token for generation completion\n",
    "\n",
    "2. **Core Generation Pipeline**\n",
    "   - Implemented text encoding/decoding utilities\n",
    "   - Established baseline with greedy search generation\n",
    "\n",
    "3. **Strategy Exploration**\n",
    "   - Beam search with varying beam widths (2, 6, 14)\n",
    "   - N-gram repetition penalties (2, 4, 8)\n",
    "   - Temperature sampling with different top-k values\n",
    "\n",
    "4. **Evaluation & Analysis**\n",
    "   - Qualitative output comparison\n",
    "   - Carbon emission tracking via CodeCarbon\n",
    "   - Performance metrics logging\n",
    "\n",
    "5. **Documentation**\n",
    "   - Generation parameter tracking\n",
    "   - Output samples preservation\n",
    "   - Emission metrics recording\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "**Generation Strategy Trade-offs:**\n",
    "- Beam search produced more coherent but conservative outputs\n",
    "- Higher temperatures (0.8-0.9) increased creativity at risk of incoherence\n",
    "- N-gram penalties effectively reduced repetition but could limit fluency\n",
    "\n",
    "**Performance Observations:**\n",
    "- Beam width showed non-linear quality improvements (diminishing returns past 6 beams)\n",
    "- Temperature values between 0.6-0.8 provided best balance of creativity/coherence\n",
    "- N-gram penalties >4 sometimes caused unnatural sentence fragmentation\n",
    "\n",
    "**Practical Considerations:**\n",
    "- Generation quality heavily dependent on prompt engineering\n",
    "- Smaller models like DistilGPT-2 require more constrained parameters\n",
    "- Carbon emissions varied significantly by strategy (beam search being most costly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, set_seed\n",
    "import pandas as pd\n",
    "from codecarbon import track_emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate DistilGPT-2's `tokenizer` and `model` using the `.from_pretrained` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set `pad token` to eos token if not already set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Generation\n",
    "\n",
    "#### Assign `pt_tensors` the input text's tokens in PyTorch tensor form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31373,    11,   995,     0]])\n"
     ]
    }
   ],
   "source": [
    "def encode_text_as_pt_tensor(text):\n",
    "    pt_tensors = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    return pt_tensors\n",
    "\n",
    "print(encode_text_as_pt_tensor(\"hello, world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Decode_tokens` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(tokens):\n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True)  # Added skip_special_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set `random_seed` for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets create a promt to work with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time I was living in Berlin and studying music there!\"\n",
    "tokens = encode_text_as_pt_tensor(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will Instruct the model to generate a completion on your choice of prompt using the `greedy search` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time I was living in Berlin and studying music there!\n",
      "\n",
      "I was a little bit surprised when I heard that the German music scene was growing rapidly.\n"
     ]
    }
   ],
   "source": [
    "output_tokens = model.generate(tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "print(decode_tokens(output_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Generation Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will adapt the function below to use `beam search` in its generations. Then we call it three times with 2 beam, 6 beams, and 14 beams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time I was living in Berlin and studying music there!\n",
      "\n",
      "I had been living in Berlin for a few years and I had been living in Berlin for\n",
      "Once upon a time I was living in Berlin and studying music there! It was the first time in my life that I had a chance to experience the world of music.\n",
      "Once upon a time I was living in Berlin and studying music there! It was the first time in my life that I had the opportunity to go to a concert in Berlin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time I was living in Berlin and studying music there! It was the first time in my life that I had the opportunity to go to a concert in Berlin'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_with_beam_search(prompt, num_beams):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, \n",
    "                          num_beams=num_beams, \n",
    "                          pad_token_id=tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output)\n",
    "    print(completion)\n",
    "    return completion\n",
    "\n",
    "generate_with_beam_search(prompt, 2)\n",
    "generate_with_beam_search(prompt, 6)\n",
    "generate_with_beam_search(prompt, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lest apply the same process we did with `beam search` on step before but with `n-gram penalties` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time I was living in Berlin and studying music there! It was a great place to study music, and it was the perfect place for me to learn.\n",
      "Once upon a time I was living in Berlin and studying music there! It was the first time in my life that I had a chance to experience the world of music.\n",
      "Once upon a time I was living in Berlin and studying music there! It was the first time in my life that I had a chance to experience the world of music.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time I was living in Berlin and studying music there! It was the first time in my life that I had a chance to experience the world of music.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_with_ngram_penalty(prompt, n_gram_penalty, num_beams=6):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, \n",
    "                          num_beams=num_beams, \n",
    "                          no_repeat_ngram_size=n_gram_penalty, \n",
    "                          pad_token_id=tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output)\n",
    "    print(completion)\n",
    "    return completion\n",
    "\n",
    "generate_with_ngram_penalty(prompt, 2)\n",
    "generate_with_ngram_penalty(prompt, 4)\n",
    "generate_with_ngram_penalty(prompt, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lest apply the same process we did with `n-gram penalties` before and experiment with different settings of `temperature` and `top_k` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.6\n",
      "Top K: 50\n",
      "Once upon a time I was living in Berlin and studying music there! I always had a big time in the city and I saw people there singing and dancing and singing.\n",
      "Temperature: 0.8\n",
      "Top K: 30\n",
      "Once upon a time I was living in Berlin and studying music there! I thought that I am going to be doing a documentary on the German music scene. I will say\n",
      "Temperature: 0.9\n",
      "Top K: 20\n",
      "Once upon a time I was living in Berlin and studying music there! I had a chance to play at a German concert but the venue wasn't close. I would have\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Once upon a time I was living in Berlin and studying music there! I had a chance to play at a German concert but the venue wasn't close. I would have\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_with_sampling(prompt, temperature, top_k, n_gram_penalty=2):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, \n",
    "                          no_repeat_ngram_size=n_gram_penalty, \n",
    "                          pad_token_id=tokenizer.eos_token_id, \n",
    "                          do_sample=True, \n",
    "                          temperature=temperature, \n",
    "                          top_k=top_k)\n",
    "    completion = decode_tokens(output)\n",
    "    print(f\"Temperature: {temperature}\\nTop K: {top_k}\\n{completion}\")\n",
    "    return completion\n",
    "\n",
    "generate_with_sampling(prompt, 0.6, 50)\n",
    "generate_with_sampling(prompt, 0.8, 30)\n",
    "generate_with_sampling(prompt, 0.9, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CodeCarbon Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('codecarbon').setLevel(logging.ERROR)\n",
    "\n",
    "# from codecarbon import EmissionsTracker\n",
    "# tracker = EmissionsTracker(allow_multiple_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 23:46:38] Error: Another instance of codecarbon is probably running as we find `C:\\Users\\Santiago\\AppData\\Local\\Temp\\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.6\n",
      "Top K: 50\n",
      "Carbon dioxide is a fuel that can be added to the atmosphere. This is an important component of the carbon dioxide emissions from\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Carbon dioxide is a fuel that can be added to the atmosphere. This is an important component of the carbon dioxide emissions from'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install CodeCarbon safely\n",
    "try:\n",
    "    from codecarbon import track_emissions\n",
    "except ImportError:\n",
    "    print(\"CodeCarbon is not installed. Run `pip install codecarbon`.\")\n",
    "\n",
    "@track_emissions\n",
    "def generate_with_sampling_tracked(prompt, temperature, top_k, n_gram_penalty=2):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, \n",
    "                          no_repeat_ngram_size=n_gram_penalty, \n",
    "                          pad_token_id=tokenizer.eos_token_id, \n",
    "                          do_sample=True, \n",
    "                          temperature=temperature, \n",
    "                          top_k=top_k)\n",
    "    completion = decode_tokens(output)\n",
    "    print(f\"Temperature: {temperature}\\nTop K: {top_k}\\n{completion}\")\n",
    "    return completion\n",
    "\n",
    "generate_with_sampling_tracked(\"Carbon dioxide is a\", 0.6, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At last, we can use pandas `read_csv` method to load in the `emissions.csv` we generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp project_name                                run_id  \\\n",
      "0  2025-03-27T15:00:24   codecarbon  a42ccd37-7527-4452-ac86-b513e6f6cf06   \n",
      "1  2025-03-27T15:25:46   codecarbon  a3f733a3-3501-4c3d-b6b6-235279aaa6a1   \n",
      "2  2025-03-27T15:27:53   codecarbon  e2310fc0-93a0-4bc4-a75a-470df1e9ced8   \n",
      "3  2025-03-27T20:40:57   codecarbon  2d382569-a7ce-4825-9fa0-2f6cc4fc4653   \n",
      "4  2025-03-27T20:48:44   codecarbon  15abf4b9-499b-4a5d-b61a-dc0c1dd65c9e   \n",
      "\n",
      "                          experiment_id  duration  emissions  emissions_rate  \\\n",
      "0  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.553717   0.000002        0.000004   \n",
      "1  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.537023   0.000002        0.000004   \n",
      "2  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.516346   0.000002        0.000004   \n",
      "3  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  1.171983   0.000004        0.000004   \n",
      "4  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.536423   0.000002        0.000004   \n",
      "\n",
      "   cpu_power  gpu_power  ram_power  ...  cpu_count  \\\n",
      "0       32.5        0.0   5.869738  ...         12   \n",
      "1       32.5        0.0   5.869738  ...         12   \n",
      "2       32.5        0.0   5.869738  ...         12   \n",
      "3       32.5        0.0   5.869738  ...         12   \n",
      "4       32.5        0.0   5.869738  ...         12   \n",
      "\n",
      "                                  cpu_model  gpu_count  gpu_model longitude  \\\n",
      "0  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
      "1  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5835   \n",
      "2  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
      "3  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
      "4  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
      "\n",
      "  latitude ram_total_size  tracking_mode  on_cloud  pue  \n",
      "0 -34.3558      15.652634        machine         N  1.0  \n",
      "1 -34.4459      15.652634        machine         N  1.0  \n",
      "2 -34.3558      15.652634        machine         N  1.0  \n",
      "3 -34.3558      15.652634        machine         N  1.0  \n",
      "4 -34.3558      15.652634        machine         N  1.0  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>project_name</th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>emissions</th>\n",
       "      <th>emissions_rate</th>\n",
       "      <th>cpu_power</th>\n",
       "      <th>gpu_power</th>\n",
       "      <th>ram_power</th>\n",
       "      <th>...</th>\n",
       "      <th>cpu_count</th>\n",
       "      <th>cpu_model</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>ram_total_size</th>\n",
       "      <th>tracking_mode</th>\n",
       "      <th>on_cloud</th>\n",
       "      <th>pue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-27T15:00:24</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>a42ccd37-7527-4452-ac86-b513e6f6cf06</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>0.553717</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.5440</td>\n",
       "      <td>-34.3558</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-27T15:25:46</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>a3f733a3-3501-4c3d-b6b6-235279aaa6a1</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>0.537023</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.5835</td>\n",
       "      <td>-34.4459</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-27T15:27:53</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>e2310fc0-93a0-4bc4-a75a-470df1e9ced8</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>0.516346</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.5440</td>\n",
       "      <td>-34.3558</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-27T20:40:57</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>2d382569-a7ce-4825-9fa0-2f6cc4fc4653</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>1.171983</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.5440</td>\n",
       "      <td>-34.3558</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-27T20:48:44</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>15abf4b9-499b-4a5d-b61a-dc0c1dd65c9e</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>0.536423</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.5440</td>\n",
       "      <td>-34.3558</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-03-27T20:56:01</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>0a78cab0-f67c-432c-9a38-72dda9ff3138</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>0.546012</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.5440</td>\n",
       "      <td>-34.3558</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-03-27T21:25:42</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>4f44d1a3-c709-4378-ba7a-5a3da68176c4</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>0.549885</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.5440</td>\n",
       "      <td>-34.3558</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-03-27T21:28:11</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>2f188b5e-1f63-4d82-9513-5563697a7ce8</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>0.546654</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.6342</td>\n",
       "      <td>-34.4549</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-03-27T21:28:42</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>509cab28-fd51-48b9-b3a2-286bc4f980b3</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>0.594984</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.6342</td>\n",
       "      <td>-34.4549</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-03-27T21:29:37</td>\n",
       "      <td>codecarbon</td>\n",
       "      <td>1a01e47f-1dc1-4d2b-9450-3142d9d969dc</td>\n",
       "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
       "      <td>0.573706</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.869738</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.5440</td>\n",
       "      <td>-34.3558</td>\n",
       "      <td>15.652634</td>\n",
       "      <td>machine</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp project_name                                run_id  \\\n",
       "0  2025-03-27T15:00:24   codecarbon  a42ccd37-7527-4452-ac86-b513e6f6cf06   \n",
       "1  2025-03-27T15:25:46   codecarbon  a3f733a3-3501-4c3d-b6b6-235279aaa6a1   \n",
       "2  2025-03-27T15:27:53   codecarbon  e2310fc0-93a0-4bc4-a75a-470df1e9ced8   \n",
       "3  2025-03-27T20:40:57   codecarbon  2d382569-a7ce-4825-9fa0-2f6cc4fc4653   \n",
       "4  2025-03-27T20:48:44   codecarbon  15abf4b9-499b-4a5d-b61a-dc0c1dd65c9e   \n",
       "5  2025-03-27T20:56:01   codecarbon  0a78cab0-f67c-432c-9a38-72dda9ff3138   \n",
       "6  2025-03-27T21:25:42   codecarbon  4f44d1a3-c709-4378-ba7a-5a3da68176c4   \n",
       "7  2025-03-27T21:28:11   codecarbon  2f188b5e-1f63-4d82-9513-5563697a7ce8   \n",
       "8  2025-03-27T21:28:42   codecarbon  509cab28-fd51-48b9-b3a2-286bc4f980b3   \n",
       "9  2025-03-27T21:29:37   codecarbon  1a01e47f-1dc1-4d2b-9450-3142d9d969dc   \n",
       "\n",
       "                          experiment_id  duration  emissions  emissions_rate  \\\n",
       "0  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.553717   0.000002        0.000004   \n",
       "1  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.537023   0.000002        0.000004   \n",
       "2  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.516346   0.000002        0.000004   \n",
       "3  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  1.171983   0.000004        0.000004   \n",
       "4  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.536423   0.000002        0.000004   \n",
       "5  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.546012   0.000002        0.000004   \n",
       "6  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.549885   0.000002        0.000004   \n",
       "7  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.546654   0.000002        0.000004   \n",
       "8  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.594984   0.000002        0.000004   \n",
       "9  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.573706   0.000002        0.000004   \n",
       "\n",
       "   cpu_power  gpu_power  ram_power  ...  cpu_count  \\\n",
       "0       32.5        0.0   5.869738  ...         12   \n",
       "1       32.5        0.0   5.869738  ...         12   \n",
       "2       32.5        0.0   5.869738  ...         12   \n",
       "3       32.5        0.0   5.869738  ...         12   \n",
       "4       32.5        0.0   5.869738  ...         12   \n",
       "5       32.5        0.0   5.869738  ...         12   \n",
       "6       32.5        0.0   5.869738  ...         12   \n",
       "7       32.5        0.0   5.869738  ...         12   \n",
       "8       32.5        0.0   5.869738  ...         12   \n",
       "9       32.5        0.0   5.869738  ...         12   \n",
       "\n",
       "                                  cpu_model  gpu_count  gpu_model longitude  \\\n",
       "0  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
       "1  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5835   \n",
       "2  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
       "3  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
       "4  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
       "5  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
       "6  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
       "7  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.6342   \n",
       "8  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.6342   \n",
       "9  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        NaN        NaN  -58.5440   \n",
       "\n",
       "  latitude ram_total_size  tracking_mode  on_cloud  pue  \n",
       "0 -34.3558      15.652634        machine         N  1.0  \n",
       "1 -34.4459      15.652634        machine         N  1.0  \n",
       "2 -34.3558      15.652634        machine         N  1.0  \n",
       "3 -34.3558      15.652634        machine         N  1.0  \n",
       "4 -34.3558      15.652634        machine         N  1.0  \n",
       "5 -34.3558      15.652634        machine         N  1.0  \n",
       "6 -34.3558      15.652634        machine         N  1.0  \n",
       "7 -34.4549      15.652634        machine         N  1.0  \n",
       "8 -34.4549      15.652634        machine         N  1.0  \n",
       "9 -34.3558      15.652634        machine         N  1.0  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        duration     emissions  emissions_rate  cpu_power  gpu_power  \\\n",
      "count  10.000000  1.000000e+01    1.000000e+01       10.0       10.0   \n",
      "mean    0.612673  2.307892e-06    3.766453e-06       32.5        0.0   \n",
      "std     0.197680  7.463074e-07    1.927414e-09        0.0        0.0   \n",
      "min     0.516346  1.944465e-06    3.763865e-06       32.5        0.0   \n",
      "25%     0.539270  2.030914e-06    3.765835e-06       32.5        0.0   \n",
      "50%     0.548270  2.065092e-06    3.766486e-06       32.5        0.0   \n",
      "75%     0.568709  2.142109e-06    3.766780e-06       32.5        0.0   \n",
      "max     1.171983  4.419448e-06    3.770914e-06       32.5        0.0   \n",
      "\n",
      "       ram_power  cpu_energy  gpu_energy    ram_energy  energy_consumed  \\\n",
      "count  10.000000   10.000000        10.0  1.000000e+01        10.000000   \n",
      "mean    5.869738    0.000006         0.0  9.962514e-07         0.000007   \n",
      "std     0.000000    0.000002         0.0  3.224332e-07         0.000002   \n",
      "min     5.869738    0.000005         0.0  8.391445e-07         0.000005   \n",
      "25%     5.869738    0.000005         0.0  8.765693e-07         0.000006   \n",
      "50%     5.869738    0.000005         0.0  8.913701e-07         0.000006   \n",
      "75%     5.869738    0.000005         0.0  9.246418e-07         0.000006   \n",
      "max     5.869738    0.000011         0.0  1.908517e-06         0.000012   \n",
      "\n",
      "       cloud_provider  cloud_region  cpu_count  gpu_count  gpu_model  \\\n",
      "count             0.0           0.0       10.0        0.0        0.0   \n",
      "mean              NaN           NaN       12.0        NaN        NaN   \n",
      "std               NaN           NaN        0.0        NaN        NaN   \n",
      "min               NaN           NaN       12.0        NaN        NaN   \n",
      "25%               NaN           NaN       12.0        NaN        NaN   \n",
      "50%               NaN           NaN       12.0        NaN        NaN   \n",
      "75%               NaN           NaN       12.0        NaN        NaN   \n",
      "max               NaN           NaN       12.0        NaN        NaN   \n",
      "\n",
      "       longitude   latitude  ram_total_size   pue  \n",
      "count  10.000000  10.000000       10.000000  10.0  \n",
      "mean  -58.565990 -34.384630       15.652634   1.0  \n",
      "std     0.038001   0.046485        0.000000   0.0  \n",
      "min   -58.634200 -34.454900       15.652634   1.0  \n",
      "25%   -58.573625 -34.423375       15.652634   1.0  \n",
      "50%   -58.544000 -34.355800       15.652634   1.0  \n",
      "75%   -58.544000 -34.355800       15.652634   1.0  \n",
      "max   -58.544000 -34.355800       15.652634   1.0  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    emissions = pd.read_csv(\"emissions.csv\")\n",
    "    print(emissions.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Emissions.csv not found. Make sure you have generated the file.\")\n",
    "\n",
    "display(emissions)\n",
    "print(emissions.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project successfully demonstrates a methodical approach to exploring text generation hyperparameters with several key takeaways:\n",
    "\n",
    "1. **There's no universal optimal configuration** - Different tasks demand different generation strategies\n",
    "2. **Parameter tuning is non-linear** - Small changes can have disproportionate effects\n",
    "3. **Efficiency matters** - More complex strategies don't always yield better results\n",
    "4. **Reproducibility is crucial** - Seed setting and proper logging enable valid comparisons\n",
    "\n",
    "The true value of this implementation lies not in the specific outputs generated, but in establishing a reproducible framework for:\n",
    "- **Systematic generation strategy evaluation**\n",
    "- **Environmentally-conscious model experimentation**\n",
    "- **Hypothesis-driven NLP development**\n",
    "\n",
    "This foundation can be extended to larger models, different domains, or more sophisticated evaluation metrics while maintaining the same rigorous experimental approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
