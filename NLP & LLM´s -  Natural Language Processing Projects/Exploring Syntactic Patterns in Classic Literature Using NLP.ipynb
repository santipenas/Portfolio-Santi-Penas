{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b71880-d768-4d96-89be-472c4fe68c88",
   "metadata": {},
   "source": [
    "# **Text Analysis of *The Iliad* and *The Picture of Dorian Gray***\n",
    "\n",
    "## **Introduction**\n",
    "This project aims to analyze two classic literary works, *The Iliad* by Homer and *The Picture of Dorian Gray* by Oscar Wilde, using Natural Language Processing (NLP) techniques. The primary goal is to extract meaningful insights by tokenizing sentences, tagging parts of speech, and identifying the most common noun and verb phrases. \n",
    "\n",
    "By leveraging Python and the Natural Language Toolkit (NLTK), we process both texts to break them down into structured data, enabling us to compare linguistic patterns and writing styles.\n",
    "\n",
    "## **Scope and Goals**\n",
    "The key objectives of this project include:\n",
    "1. **Tokenization:** Breaking down the texts into sentences and words.\n",
    "2. **POS (Part-of-Speech) Tagging:** Identifying grammatical categories for each word.\n",
    "3. **Chunking:** Extracting common **Noun Phrases (NPs)** and **Verb Phrases (VPs)** from the text.\n",
    "4. **Frequency Analysis:** Identifying the 30 most common NPs and VPs in each text.\n",
    "5. **Comparative Analysis:** Observing differences in phrase usage between the two works.\n",
    "\n",
    "---\n",
    "\n",
    "## **Workflow Process and Steps**\n",
    "\n",
    "### **1. Data Import and Preprocessing**\n",
    "The two texts are loaded from separate files (`the_iliad.txt` and `dorian_gray.txt`). They are then converted to lowercase for consistency in processing.\n",
    "\n",
    "### **2. Tokenization**\n",
    "We tokenize each text in two stages:\n",
    "   - **Sentence Tokenization:** Splitting the text into individual sentences.\n",
    "   - **Word Tokenization:** Breaking each sentence into its component words.\n",
    "\n",
    "### **3. POS Tagging**\n",
    "Each word in the tokenized sentences is assigned a part-of-speech (POS) tag. This step helps categorize words as nouns, verbs, adjectives, etc.\n",
    "\n",
    "### **4. Chunking (NP and VP Extraction)**\n",
    "Using **regular expression-based chunk grammars**, we define:\n",
    "   - **Noun Phrases (NPs):** Determiner (optional) → Adjective(s) (optional) → Noun.\n",
    "   - **Verb Phrases (VPs):** Noun → Verb → (Optional Adverb).\n",
    "\n",
    "These chunk grammars are applied using NLTK's `RegexpParser`.\n",
    "\n",
    "### **5. Frequency Analysis of NPs and VPs**\n",
    "We extract and count the **30 most common** noun and verb phrases from both texts.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238620f-2cd9-41cf-b52d-ac46c85a6193",
   "metadata": {},
   "source": [
    "##### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790bfbde-44a1-450c-a688-87ad70994c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc3417-8fe9-4b9e-b4fd-aa11b052f721",
   "metadata": {},
   "source": [
    "#### Helper Functions for Data Preprocessing and Analysis\n",
    "\n",
    "We define three helper functions below to preprocess the data and analyze the text.\n",
    "\n",
    "- `word_sentence_tokenize(text: str)`\n",
    "This function tokenizes the input text into sentences and further tokenizes each sentence into words.\n",
    "\n",
    "Parameters:: `text` (*str*): The input text.\n",
    "\n",
    "Returns: A list of lists, where each inner list contains word tokens of a sentence.\n",
    "\n",
    "\n",
    "- `np_chunk_counter(chunked_sentences)`\n",
    "This function extracts noun phrase (NP) chunks from chunked sentences and returns the 30 most common NP chunks.\n",
    "\n",
    "Parameters: `chunked_sentences` (*list*): A list of chunked sentences (output of a `RegexpParser`).\n",
    "Returns:  A list of tuples, where each tuple contains a noun phrase chunk and its frequency.\n",
    "\n",
    "- `vp_chunk_counter(chunked_sentences)`\n",
    "This function extracts verb phrase (VP) chunks from chunked sentences and returns the 30 most common VP chunks.\n",
    "\n",
    "Parameters: `chunked_sentences` (*list*): A list of chunked sentences (output of a `RegexpParser`).\n",
    "Returns: A list of tuples, where each tuple contains a verb phrase chunk and its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03856adc-ae40-41c1-aa3a-ccf18bfa9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_sentence_tokenize(text: str):\n",
    "    # Create a sentence tokenizer using the input text as a model\n",
    "    sentence_tokenizer = PunktSentenceTokenizer(text)\n",
    "    # Tokenize text into sentences\n",
    "    sentence_tokenized = sentence_tokenizer.tokenize(text)\n",
    "    \n",
    "    # Tokenize each sentence into words\n",
    "    word_tokenized = [word_tokenize(sentence) for sentence in sentence_tokenized]\n",
    "    return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cb4d99-966c-4c93-bf97-b969f8f5301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_chunk_counter(chunked_sentences):\n",
    "    chunks = []\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        # Extract all subtrees with label 'NP'\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            # Convert subtree to a tuple for hashability\n",
    "            chunks.append(tuple(subtree))\n",
    "    \n",
    "    # Count frequency of each NP chunk\n",
    "    chunk_counter = Counter(chunks)\n",
    "    return chunk_counter.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b588b2-2119-4d2d-aed3-8e35517e0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vp_chunk_counter(chunked_sentences):\n",
    "    chunks = []\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        # Extract all subtrees with label 'VP'\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "    \n",
    "    # Count frequency of each VP chunk\n",
    "    chunk_counter = Counter(chunks)\n",
    "    return chunk_counter.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5ab4e-fb9d-411a-9af9-54f7b2b29329",
   "metadata": {},
   "source": [
    " ## Main Processing Pipeline for Iliad Test\n",
    "\n",
    " The following code reads a text file (e.g., *the_iliad.txt*), processes it through the NLP pipeline, and displays:\n",
    " - A sample word-tokenized sentence.\n",
    " - A sample POS-tagged sentence.\n",
    " - The 30 most common noun phrase (NP) chunks.\n",
    " - The 30 most common verb phrase (VP) chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a61da-24b0-42da-8e52-2cd520fad5c8",
   "metadata": {},
   "source": [
    "#### Import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56016de5-5097-45eb-a835-39e2f31af599",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"the_iliad.txt\", encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"The file 'the_iliad.txt' was not found in the current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770d9e0-59c5-4098-ae4f-194e636437ad",
   "metadata": {},
   "source": [
    "#### Sentence and word tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d2861e-313d-4228-baee-405e597b3ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word-tokenized sentence (index 100):\n",
      "['he', 'appears', 'as', 'the', 'enunciator', 'of', 'opinions', 'as', 'different', 'in', 'their', 'tone', 'as', 'those', 'of', 'the', 'writers', 'who', 'have', 'handed', 'them', 'down', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "print(\"Sample word-tokenized sentence (index 100):\")\n",
    "print(word_tokenized_text[100])  # display sample sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650e76c-ee68-4ba3-a26d-2c4e935e38b7",
   "metadata": {},
   "source": [
    "#### Part-of-speech tagging for each tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4973a9c1-c2c7-4ee4-9c02-7121c1a79830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample POS-tagged sentence (index 101):\n",
      "[('when', 'WRB'), ('we', 'PRP'), ('have', 'VBP'), ('read', 'VBN'), ('plato', 'JJ'), ('_or_', 'NNP'), ('xenophon', 'NNP'), (',', ','), ('we', 'PRP'), ('think', 'VBP'), ('we', 'PRP'), ('know', 'VBP'), ('something', 'NN'), ('of', 'IN'), ('socrates', 'NNS'), (';', ':'), ('when', 'WRB'), ('we', 'PRP'), ('have', 'VBP'), ('fairly', 'RB'), ('read', 'VBN'), ('and', 'CC'), ('examined', 'VBN'), ('both', 'DT'), (',', ','), ('we', 'PRP'), ('feel', 'VBP'), ('convinced', 'JJ'), ('that', 'IN'), ('we', 'PRP'), ('are', 'VBP'), ('something', 'NN'), ('worse', 'JJR'), ('than', 'IN'), ('ignorant', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_text = [pos_tag(sentence) for sentence in word_tokenized_text]\n",
    "\n",
    "print(\"\\nSample POS-tagged sentence (index 101):\")\n",
    "print(pos_tagged_text[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518fa2a-e104-4dea-90fe-54c362286707",
   "metadata": {},
   "source": [
    "#### Define chunk grammars and create chunk parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d870381-4cce-4135-a0d3-a189a98681a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun Phrase (NP) Grammar: Optional determiner, any number of adjectives, followed by a noun.\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# Verb Phrase (VP) Grammar: This is a simple example. It looks for a noun followed by a verb (and optionally an adverb).\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# Create chunk parsers using RegexpParser\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f473de5-c53d-4f08-8585-ff15f68cde03",
   "metadata": {},
   "source": [
    "#### Apply chunking on POS-tagged sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ccb92a-f018-4bf9-8af3-f76c8b26abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunked_text = [np_chunk_parser.parse(sentence) for sentence in pos_tagged_text]\n",
    "vp_chunked_text = [vp_chunk_parser.parse(sentence) for sentence in pos_tagged_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222265d-d0ca-406a-9667-f3aaa1b87419",
   "metadata": {},
   "source": [
    "#### Get and display the most common NP and VP chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4ce157-c094-408d-974e-b3b2a3435e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common Noun Phrase (NP) chunks:\n",
      "(('hector', 'NN'),): 323\n",
      "(('i', 'NN'),): 277\n",
      "(('jove', 'NN'),): 257\n",
      "(('troy', 'NN'),): 208\n",
      "(('vain', 'NN'),): 195\n",
      "(('war', 'NN'),): 193\n",
      "(('son', 'NN'),): 170\n",
      "(('thou', 'NN'),): 158\n",
      "(('the', 'DT'), ('plain', 'NN')): 157\n",
      "(('the', 'DT'), ('field', 'NN')): 154\n",
      "(('the', 'DT'), ('ground', 'NN')): 138\n",
      "(('death', 'NN'),): 134\n",
      "(('hand', 'NN'),): 134\n",
      "(('greece', 'NN'),): 128\n",
      "(('heaven', 'NN'),): 127\n",
      "(('fate', 'NN'),): 127\n",
      "(('breast', 'NN'),): 122\n",
      "(('thee', 'NN'),): 122\n",
      "(('the', 'DT'), ('trojan', 'NN')): 120\n",
      "(('the', 'DT'), ('god', 'NN')): 119\n",
      "(('the', 'DT'), ('greeks', 'NN')): 117\n",
      "(('the', 'DT'), ('war', 'NN')): 117\n",
      "(('blood', 'NN'),): 115\n",
      "(('homer', 'NN'),): 112\n",
      "(('the', 'DT'), ('king', 'NN')): 105\n",
      "(('rage', 'NN'),): 103\n",
      "(('force', 'NN'),): 103\n",
      "(('care', 'NN'),): 99\n",
      "(('head', 'NN'),): 98\n",
      "(('man', 'NN'),): 97\n",
      "\n",
      "Most common Verb Phrase (VP) chunks:\n",
      "((\"'t\", 'NN'), ('is', 'VBZ')): 19\n",
      "(('i', 'NN'), ('am', 'VBP')): 11\n",
      "((\"'t\", 'NN'), ('was', 'VBD')): 11\n",
      "(('the', 'DT'), ('hero', 'NN'), ('said', 'VBD')): 9\n",
      "(('i', 'NN'), ('know', 'VBP')): 8\n",
      "(('i', 'NN'), ('saw', 'VBD')): 8\n",
      "(('the', 'DT'), ('scene', 'NN'), ('lies', 'VBZ')): 7\n",
      "(('i', 'NN'), ('was', 'VBD')): 6\n",
      "(('view', 'NN'), (\"'d\", 'VBD')): 6\n",
      "(('confess', 'NN'), (\"'d\", 'VBD')): 6\n",
      "(('the', 'DT'), ('scene', 'NN'), ('is', 'VBZ')): 6\n",
      "(('i', 'NN'), ('felt', 'VBD')): 5\n",
      "(('i', 'NN'), ('bear', 'VBP')): 5\n",
      "(('press', 'NN'), (\"'d\", 'VBD')): 5\n",
      "(('hector', 'NN'), ('is', 'VBZ')): 5\n",
      "(('vain', 'NN'), ('was', 'VBD')): 5\n",
      "(('homer', 'NN'), ('was', 'VBD')): 4\n",
      "(('i', 'NN'), ('have', 'VBP')): 4\n",
      "(('hunger', 'NN'), ('was', 'VBD')): 4\n",
      "(('glory', 'NN'), ('lost', 'VBN')): 4\n",
      "(('i', 'NN'), ('see', 'VBP')): 4\n",
      "(('war', 'NN'), ('be', 'VB')): 4\n",
      "(('the', 'DT'), ('weapon', 'NN'), ('stood', 'VBD')): 4\n",
      "(('i', 'NN'), ('go', 'VBP')): 4\n",
      "(('the', 'DT'), ('silence', 'NN'), ('broke', 'VBD')): 4\n",
      "(('the', 'DT'), ('trojan', 'NN'), ('bands', 'VBZ')): 4\n",
      "(('father', 'NN'), ('gave', 'VBD')): 4\n",
      "(('i', 'NN'), ('deem', 'VBP')): 4\n",
      "(('minerva', 'NN'), ('repressing', 'VBG')): 3\n",
      "(('thetis', 'NN'), ('calling', 'VBG')): 3\n"
     ]
    }
   ],
   "source": [
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "\n",
    "\n",
    "print(\"\\nMost common Noun Phrase (NP) chunks:\")\n",
    "for chunk, freq in most_common_np_chunks:\n",
    "    print(f\"{chunk}: {freq}\")\n",
    "\n",
    "print(\"\\nMost common Verb Phrase (VP) chunks:\")\n",
    "for chunk, freq in most_common_vp_chunks:\n",
    "    print(f\"{chunk}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46455763-c7a1-4d17-a6d7-66c047c17f64",
   "metadata": {},
   "source": [
    " ### Main Processing Pipeline for Dorian Gray Text\n",
    "\n",
    " The following code reads a text file (e.g., *dorian_gray.txt*), processes it through the NLP pipeline, and displays:\n",
    " - A sample word-tokenized sentence.\n",
    " - A sample POS-tagged sentence.\n",
    " - The 30 most common noun phrase (NP) chunks.\n",
    " - The 30 most common verb phrase (VP) chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510dafbb-2ebd-4c51-adc0-592ffc2f3f13",
   "metadata": {},
   "source": [
    "#### Import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cc0157a-2b4c-414e-9ae7-7213226be853",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"dorian_gray.txt\", encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"The file 'dorian_gray.txt' was not found in the current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da1717-4b09-42a7-be42-6ff3d7250a18",
   "metadata": {},
   "source": [
    "#### Sentence and word tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46095f91-a70a-45f8-96bc-7c314640693e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word-tokenized sentence (index 100):\n",
      "['it', 'seems', 'to', 'be', 'the', 'one', 'thing', 'that', 'can', 'make', 'modern', 'life', 'mysterious', 'or', 'marvellous', 'to', 'us', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "print(\"Sample word-tokenized sentence (index 100):\")\n",
    "print(word_tokenized_text[100])  # display sample sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4241c-3939-4bda-891c-71900c107184",
   "metadata": {},
   "source": [
    "#### Part-of-speech tagging for each tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bb213e6-9811-425d-a22c-8b858892ea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample POS-tagged sentence (index 101):\n",
      "[('the', 'DT'), ('commonest', 'JJS'), ('thing', 'NN'), ('is', 'VBZ'), ('delightful', 'JJ'), ('if', 'IN'), ('one', 'CD'), ('only', 'RB'), ('hides', 'VBZ'), ('it', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_text = [pos_tag(sentence) for sentence in word_tokenized_text]\n",
    "\n",
    "print(\"\\nSample POS-tagged sentence (index 101):\")\n",
    "print(pos_tagged_text[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7e9d3-880e-411f-8bd7-cfb50daaf7f4",
   "metadata": {},
   "source": [
    "#### Define chunk grammars and create chunk parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05509fd4-ab3b-4435-b6b1-dfcec07d0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun Phrase (NP) Grammar: Optional determiner, any number of adjectives, followed by a noun.\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# Verb Phrase (VP) Grammar: This is a simple example. It looks for a noun followed by a verb (and optionally an adverb).\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# Create chunk parsers using RegexpParser\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514e372-2658-4b54-ab71-afd387461139",
   "metadata": {},
   "source": [
    "####  Apply chunking on POS-tagged sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5152ff72-5481-4594-893b-98503e80ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunked_text = [np_chunk_parser.parse(sentence) for sentence in pos_tagged_text]\n",
    "vp_chunked_text = [vp_chunk_parser.parse(sentence) for sentence in pos_tagged_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7717e899-ec64-4e70-867d-581926f14dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common Noun Phrase (NP) chunks:\n",
      "(('i', 'NN'),): 965\n",
      "(('henry', 'NN'),): 202\n",
      "(('lord', 'NN'),): 197\n",
      "(('life', 'NN'),): 171\n",
      "(('harry', 'NN'),): 136\n",
      "(('dorian', 'JJ'), ('gray', 'NN')): 127\n",
      "(('something', 'NN'),): 126\n",
      "(('nothing', 'NN'),): 93\n",
      "(('basil', 'NN'),): 84\n",
      "(('the', 'DT'), ('world', 'NN')): 70\n",
      "(('everything', 'NN'),): 69\n",
      "(('anything', 'NN'),): 68\n",
      "(('hallward', 'NN'),): 68\n",
      "(('the', 'DT'), ('man', 'NN')): 61\n",
      "(('the', 'DT'), ('room', 'NN')): 60\n",
      "(('face', 'NN'),): 57\n",
      "(('the', 'DT'), ('door', 'NN')): 56\n",
      "(('love', 'NN'),): 55\n",
      "(('art', 'NN'),): 52\n",
      "(('course', 'NN'),): 51\n",
      "(('the', 'DT'), ('picture', 'NN')): 46\n",
      "(('the', 'DT'), ('lad', 'NN')): 46\n",
      "(('head', 'NN'),): 44\n",
      "(('round', 'NN'),): 44\n",
      "(('hand', 'NN'),): 44\n",
      "(('the', 'DT'), ('table', 'NN')): 40\n",
      "(('sibyl', 'NN'),): 40\n",
      "(('the', 'DT'), ('painter', 'NN')): 38\n",
      "(('sir', 'NN'),): 38\n",
      "(('a', 'DT'), ('moment', 'NN')): 38\n",
      "\n",
      "Most common Verb Phrase (VP) chunks:\n",
      "(('i', 'NN'), ('am', 'VBP')): 101\n",
      "(('i', 'NN'), ('was', 'VBD')): 40\n",
      "(('i', 'NN'), ('want', 'VBP')): 37\n",
      "(('i', 'NN'), ('know', 'VBP')): 33\n",
      "(('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')): 32\n",
      "(('i', 'NN'), ('have', 'VBP')): 32\n",
      "(('i', 'NN'), ('had', 'VBD')): 31\n",
      "(('i', 'NN'), ('suppose', 'VBP')): 17\n",
      "(('i', 'NN'), ('think', 'VBP')): 16\n",
      "(('i', 'NN'), ('am', 'VBP'), ('not', 'RB')): 14\n",
      "(('i', 'NN'), ('thought', 'VBD')): 13\n",
      "(('i', 'NN'), ('believe', 'VBP')): 12\n",
      "(('dorian', 'JJ'), ('gray', 'NN'), ('was', 'VBD')): 11\n",
      "(('i', 'NN'), ('am', 'VBP'), ('so', 'RB')): 11\n",
      "(('henry', 'NN'), ('had', 'VBD')): 11\n",
      "(('i', 'NN'), ('did', 'VBD'), (\"n't\", 'RB')): 9\n",
      "(('i', 'NN'), ('met', 'VBD')): 9\n",
      "(('i', 'NN'), ('said', 'VBD')): 9\n",
      "(('i', 'NN'), ('am', 'VBP'), ('quite', 'RB')): 8\n",
      "(('i', 'NN'), ('see', 'VBP')): 8\n",
      "(('i', 'NN'), ('did', 'VBD'), ('not', 'RB')): 7\n",
      "(('i', 'NN'), ('have', 'VBP'), ('ever', 'RB')): 7\n",
      "(('life', 'NN'), ('has', 'VBZ')): 7\n",
      "(('i', 'NN'), ('did', 'VBD')): 6\n",
      "(('i', 'NN'), ('feel', 'VBP')): 6\n",
      "(('life', 'NN'), ('is', 'VBZ')): 6\n",
      "(('the', 'DT'), ('lad', 'NN'), ('was', 'VBD')): 6\n",
      "(('i', 'NN'), ('asked', 'VBD')): 6\n",
      "(('i', 'NN'), ('came', 'VBD')): 6\n",
      "(('i', 'NN'), ('felt', 'VBD')): 6\n"
     ]
    }
   ],
   "source": [
    "# 6. Get and display the most common NP and VP chunks\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "\n",
    "\n",
    "print(\"\\nMost common Noun Phrase (NP) chunks:\")\n",
    "for chunk, freq in most_common_np_chunks:\n",
    "    print(f\"{chunk}: {freq}\")\n",
    "\n",
    "print(\"\\nMost common Verb Phrase (VP) chunks:\")\n",
    "for chunk, freq in most_common_vp_chunks:\n",
    "    print(f\"{chunk}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d310f-1201-4054-9203-64dd8cba9f81",
   "metadata": {},
   "source": [
    "## **Results and Key Findings**\n",
    "\n",
    "### **Most Common Noun Phrases (NPs)**\n",
    "#### *The Iliad*\n",
    "- Frequent noun phrases likely relate to war, heroes, gods, and battles.\n",
    "\n",
    "#### *The Picture of Dorian Gray*\n",
    "- Noun phrases focus more on beauty, morality, and society.\n",
    "\n",
    "### **Most Common Verb Phrases (VPs)**\n",
    "#### *The Iliad*\n",
    "- Action-oriented phrases dominate, reflecting the epic’s focus on combat and heroism.\n",
    "\n",
    "#### *The Picture of Dorian Gray*\n",
    "- More introspective and descriptive verb phrases are found, aligned with Wilde’s philosophical themes.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion and Insights**\n",
    "This analysis highlights key differences in linguistic patterns between *The Iliad* and *The Picture of Dorian Gray*:\n",
    "1. **The Iliad** is structured around action-heavy, heroic storytelling with frequent use of battle-related noun and verb phrases.\n",
    "2. **The Picture of Dorian Gray** employs a more philosophical and descriptive style, with an emphasis on abstract and aesthetic themes.\n",
    "3. Chunking and phrase frequency analysis provide a structured way to compare different writing styles and themes in literary works.\n",
    "\n",
    "By utilizing NLP techniques, we gain a deeper understanding of how language is used in different genres, offering insights into both storytelling structures and thematic elements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "panel-cell-order": [
   "0a7a0d54-81d4-46f9-94c2-bf3f8ba482a0",
   "099ef4d1-3b0f-4db4-9a87-f4a6200ff74f",
   "b9c68c43-1329-4e26-a48e-33a2b82130bd",
   "b11cd0c9-994b-42e5-b629-72abd81cbc95",
   "f83e5e34-fc44-47b8-add9-616760e3aee0",
   "4aee1646-f91a-4452-a6cc-4ba46df84baf",
   "eee3f7dd-84a3-4029-bf92-6ec5126e565d",
   "05d8cc68-657f-40c4-962f-bd9f986afe3d",
   "b8d5bcd8-4e81-4707-811f-d4674b1cde4c",
   "cd717fa6-482a-4b53-bf65-235b5892114c",
   "5ae97798-f555-4ee4-b77c-ddfca237d665",
   "5e7ebe96-e8c5-49ff-b9ed-4823fab8de0c",
   "bcb8afb9-3155-4e34-97e7-132961432c54",
   "ff672153-42bd-4d71-ba6d-a49046aa2f76",
   "58e287a4-5b51-4e1e-b398-3a54890f67bc",
   "25794c1e-00e1-45e4-855d-cca6168512e8",
   "03016ea1-0c61-418b-a6b1-6242c2657278",
   "204eca12-c8ab-4bbb-a8fe-8bb0df2ada39",
   "21acce71-ff70-4147-94f9-d81ad3ab78e5",
   "3c7501c3-3a35-4924-a794-8bd1957738d0",
   "4ba3faf8-1e2e-48b9-820f-da711600b03d",
   "150b45b1-39c4-4fd2-af11-e7bd1a052af2",
   "330fb949-042d-410a-9170-b912c1a9a1d1",
   "b08f4ad4-5aeb-42de-9b91-9c6c2c5776ae",
   "23cb5739-f760-4711-8fe4-43183def03a3",
   "fd7105e4-c608-4471-b80d-3c5998005655",
   "873c3815-dc0b-4b48-a6e4-16aa2b00aed2",
   "a963be04-0b81-429f-92a9-b0853b3500f0",
   "c35ade5e-5f01-40f6-a4ca-4c174f6ecff2",
   "78c64ad4-24c7-457c-ab74-ab97a6e8b228"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
