{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6fa7b37-fe6d-439d-8f36-02c997008fcb",
   "metadata": {},
   "source": [
    "# Text Insights: Extracting Key Terms with NLP & TF-IDF\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Natural Language Processing (NLP) enables us to analyze, process, and extract meaningful insights from textual data. In this project, we explore the application of **TF-IDF (Term Frequency-Inverse Document Frequency)** and **text preprocessing** techniques to analyze a collection of articles. The primary goal is to convert raw text into a structured numerical format, allowing us to identify significant terms and patterns within each document.\n",
    "\n",
    "### **Objectives of the Project**\n",
    "- **Preprocess textual data** by cleaning, tokenizing, removing stopwords, and lemmatizing.\n",
    "- **Vectorize the text** using **CountVectorizer** and **TF-IDF** to transform words into numerical features.\n",
    "- **Analyze TF-IDF scores** to identify the most important terms in each article.\n",
    "- **Gain insights** into document importance and keyword significance.\n",
    "\n",
    "By implementing these NLP techniques, we can better understand how term importance varies across different texts, a fundamental step in various applications like search engines, text classification, and topic modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## **Workflow of the Project**\n",
    "1. **Text Preprocessing:**\n",
    "   - Convert text to lowercase and remove punctuation.\n",
    "   - Tokenize the text into words.\n",
    "   - Remove stopwords to reduce noise.\n",
    "   - Apply **lemmatization** to normalize words to their root forms.\n",
    "    \n",
    "   \n",
    "2. **Text Vectorization:**\n",
    "   - Use **CountVectorizer** to convert words into numerical frequency-based representations.\n",
    "   - Transform the frequency matrix using **TF-IDF** to assign importance to words based on their relevance.\n",
    "    \n",
    "\n",
    "3. **Data Analysis:**\n",
    "   - Construct **TF-IDF matrices** for each article.\n",
    "   - Identify the **most significant words** per document.\n",
    "   - Compare different vectorization methods to validate consistency.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4fff3e5-292e-4b0d-84fc-974ef953d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from articles import articles  # Import the articles list from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298bd32b-23b3-400c-8e94-fe00f2d575ac",
   "metadata": {},
   "source": [
    "#### Helper functions and Initialize stopwords and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1568bdb7-0ff2-47c9-9157-6aa4cffd0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf6f73f-3c27-4328-a746-385c0faf2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_speech(word): # this wil determine the most probable part of speech for a word.\n",
    "    probable_pos = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = sum(1 for item in probable_pos if item.pos() == \"n\")\n",
    "    pos_counts[\"v\"] = sum(1 for item in probable_pos if item.pos() == \"v\")\n",
    "    pos_counts[\"a\"] = sum(1 for item in probable_pos if item.pos() == \"a\")\n",
    "    pos_counts[\"r\"] = sum(1 for item in probable_pos if item.pos() == \"r\")\n",
    "    return pos_counts.most_common(1)[0][0] if pos_counts else \"n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79390ba3-c691-4fd3-a533-36e56b4f50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):  # this will lean, tokenize, remove stopwords, and lemmatize text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token, get_part_of_speech(token)) \n",
    "              for token in tokens if token not in stop_words and not re.match(r'\\d+', token)]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec35ad-bb7b-4ac5-bd5a-e8ae390f0392",
   "metadata": {},
   "source": [
    "#### Preprocess articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5cc6906-be4a-4297-bd6c-82c6d3cf6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_articles = [preprocess_text(article) for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afb037d-ae03-4a2d-8a28-5668191f27ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two articles:\n",
      " ['KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum product prices by the federal government, Geo News reported.Sources said reduction in fares will be applicable on public transport, rickshaw, taxi and other means of traveling. Meanwhile, Karachi Transport Ittehad (KTI) has refused to abide by the government decision.KTI President Irshad Bukhari said the commuters are charged the lowest fares in Karachi as compare to other parts of the country, adding that 80pc vehicles run on Compressed Natural Gas (CNG). Bukhari said Karachi transporters will cut fares when decrease in CNG prices will be made.', 'HONG KONG:  Hong Kong shares opened 0.66 percent lower Monday following a tepid lead from Wall Street, as the first full week of the new year kicked off. The benchmark Hang Seng Index dipped 158.63 points to 23,699.19.']\n"
     ]
    }
   ],
   "source": [
    "print(\"First two articles:\\n\", articles[:2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a6f3e8-5581-4a48-9322-dfaa0e1c9764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two articles after cleaning the text for our analysis:\n",
      " ['karachi sindh government decide bring public transport fare per cent due massive reduction petroleum product price federal government geo news reportedsources say reduction fare applicable public transport rickshaw taxi mean travel meanwhile karachi transport ittehad kti refuse abide government decisionkti president irshad bukhari say commuter charge low fare karachi compare part country add vehicle run compress natural gas cng bukhari say karachi transporter cut fare decrease cng price make', 'hong kong hong kong share open percent lower monday follow tepid lead wall street first full week new year kick benchmark hang seng index dip point']\n"
     ]
    }
   ],
   "source": [
    "print(\"First two articles after cleaning the text for our analysis:\\n\", processed_articles[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b279cd7-2458-40be-b6b9-25e327e1daec",
   "metadata": {},
   "source": [
    "#### Initialize vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e19c76a-9ad8-45fd-937a-6bb5d3c7ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer(norm=None)\n",
    "tfidf_vectorizer = TfidfVectorizer(norm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eaaa7c-0038-449e-a495-8b201c5027ed",
   "metadata": {},
   "source": [
    "#### Convert text to numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27878dce-5411-418a-a722-73992e18a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = vectorizer.fit_transform(processed_articles)\n",
    "tfidf_scores_transformed = tfidf_transformer.fit_transform(word_counts)\n",
    "tfidf_scores = tfidf_vectorizer.fit_transform(processed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bfc9e3-cbae-42f9-9f9f-6622326f5b2b",
   "metadata": {},
   "source": [
    "#### Validate TF-IDF consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9e278f-51e8-476c-a93c-5b0663bdd633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF scores match.\n"
     ]
    }
   ],
   "source": [
    "if np.allclose(tfidf_scores_transformed.todense(), tfidf_scores.todense()):\n",
    "    print(\"TF-IDF scores match.\")\n",
    "    pd.DataFrame({'Are the tf-idf scores the same?':['YES']})\n",
    "else:\n",
    "    print(\"Mismatch in TF-IDF scores.\")\n",
    "    pd.DataFrame({'Are the tf-idf scores the same?':['No, something is wrong :(']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a98d1-00ad-4357-885b-02aef41ad8bd",
   "metadata": {},
   "source": [
    "#### Extract feature names and construct DataFrames to visulize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9926177-30d3-4775-b88e-89315c6efed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article 1</th>\n",
       "      <th>Article 2</th>\n",
       "      <th>Article 3</th>\n",
       "      <th>Article 4</th>\n",
       "      <th>Article 5</th>\n",
       "      <th>Article 6</th>\n",
       "      <th>Article 7</th>\n",
       "      <th>Article 8</th>\n",
       "      <th>Article 9</th>\n",
       "      <th>Article 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abbasi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abide</th>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.114244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.409496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.409496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
       "abbasi   0.000000   0.000000   0.000000   2.704748        0.0   0.000000   \n",
       "abide    2.704748   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "accord   0.000000   0.000000   2.704748   0.000000        0.0   0.000000   \n",
       "add      2.299283   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "agency   0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "world    0.000000   0.000000   0.000000   0.000000        0.0   8.114244   \n",
       "would    0.000000   0.000000   0.000000   2.299283        0.0   0.000000   \n",
       "year     0.000000   2.704748   0.000000   0.000000        0.0   0.000000   \n",
       "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "\n",
       "        Article 7  Article 8  Article 9  Article 10  \n",
       "abbasi   0.000000        0.0   0.000000    0.000000  \n",
       "abide    0.000000        0.0   0.000000    0.000000  \n",
       "accord   0.000000        0.0   0.000000    0.000000  \n",
       "add      0.000000        0.0   2.299283    0.000000  \n",
       "agency   2.704748        0.0   0.000000    0.000000  \n",
       "...           ...        ...        ...         ...  \n",
       "world    0.000000        0.0   0.000000    0.000000  \n",
       "would    0.000000        0.0   2.299283    0.000000  \n",
       "year     0.000000        0.0   0.000000    0.000000  \n",
       "yi       0.000000        0.0   0.000000    5.409496  \n",
       "yuan     0.000000        0.0   0.000000    5.409496  \n",
       "\n",
       "[315 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Article 1   Article 2   Article 3   Article 4   Article 5   Article 6  \\\n",
      "count  315.000000  315.000000  315.000000  315.000000  315.000000  315.000000   \n",
      "mean     0.529467    0.217187    0.462144    0.518605    0.380584    0.421591   \n",
      "std      1.385332    0.788563    1.714269    1.172005    1.351378    1.294715   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "max     10.818992    5.409496   18.933237    8.114244   16.228489    8.114244   \n",
      "\n",
      "        Article 7   Article 8   Article 9  Article 10  \n",
      "count  315.000000  315.000000  315.000000  315.000000  \n",
      "mean     0.401589    0.452858    0.354990    0.246758  \n",
      "std      1.052310    1.331015    1.107808    0.888571  \n",
      "min      0.000000    0.000000    0.000000    0.000000  \n",
      "25%      0.000000    0.000000    0.000000    0.000000  \n",
      "50%      0.000000    0.000000    0.000000    0.000000  \n",
      "75%      0.000000    0.000000    0.000000    0.000000  \n",
      "max      5.409496   11.496415    8.114244    5.409496  \n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "article_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
    "df_word_counts = pd.DataFrame(word_counts.T.todense(), index=feature_names, columns=article_index)\n",
    "df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=article_index)\n",
    "\n",
    "display(df_tf_idf)\n",
    "print(df_tf_idf.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5bf533-48e7-444f-8a0c-99052e345f42",
   "metadata": {},
   "source": [
    "#### Display top TF-IDF term for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "964a9439-8fff-4062-8e41-1ccbe2b5fac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top term for Article 1: fare\n",
      "Top term for Article 2: hong\n",
      "Top term for Article 3: sugar\n",
      "Top term for Article 4: petrol\n",
      "Top term for Article 5: engine\n",
      "Top term for Article 6: australia\n",
      "Top term for Article 7: apple\n",
      "Top term for Article 8: railway\n",
      "Top term for Article 9: cabinet\n",
      "Top term for Article 10: china\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(articles)):\n",
    "    print(f\"Top term for Article {i+1}:\", df_tf_idf.iloc[:, i].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac680e-d3ae-4779-836a-2b275bf45de1",
   "metadata": {},
   "source": [
    "## **Key Findings & Conclusion**\n",
    "\n",
    "Through this analysis, we achieved a structured numerical representation of textual data, enabling us to highlight the **most relevant words per article**. Some key takeaways:\n",
    "- **TF-IDF effectively ranks words based on importance**, reducing the influence of common words while amplifying unique terms.\n",
    "- **Text preprocessing significantly impacts results**, as stopword removal and lemmatization improve clarity and reduce redundancy.\n",
    "- **The highest scoring TF-IDF words offer valuable insights** into document themes, providing a foundation for tasks like authorship attribution, keyword extraction, and topic classification.\n",
    "\n",
    "This project demonstrates the power of **NLP and text vectorization techniques** in transforming raw text into meaningful insights. This project will serve us as a strong foundation for future advancements and enhancements, such as **document classification, sentiment analysis, or machine learning applications**.\n",
    "\n",
    "*We will update and enhance this project very soon!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "panel-cell-order": [
   "0a7a0d54-81d4-46f9-94c2-bf3f8ba482a0",
   "099ef4d1-3b0f-4db4-9a87-f4a6200ff74f",
   "b9c68c43-1329-4e26-a48e-33a2b82130bd",
   "b11cd0c9-994b-42e5-b629-72abd81cbc95",
   "f83e5e34-fc44-47b8-add9-616760e3aee0",
   "4aee1646-f91a-4452-a6cc-4ba46df84baf",
   "eee3f7dd-84a3-4029-bf92-6ec5126e565d",
   "05d8cc68-657f-40c4-962f-bd9f986afe3d",
   "b8d5bcd8-4e81-4707-811f-d4674b1cde4c",
   "cd717fa6-482a-4b53-bf65-235b5892114c",
   "5ae97798-f555-4ee4-b77c-ddfca237d665",
   "5e7ebe96-e8c5-49ff-b9ed-4823fab8de0c",
   "bcb8afb9-3155-4e34-97e7-132961432c54",
   "ff672153-42bd-4d71-ba6d-a49046aa2f76",
   "58e287a4-5b51-4e1e-b398-3a54890f67bc",
   "25794c1e-00e1-45e4-855d-cca6168512e8",
   "03016ea1-0c61-418b-a6b1-6242c2657278",
   "204eca12-c8ab-4bbb-a8fe-8bb0df2ada39",
   "21acce71-ff70-4147-94f9-d81ad3ab78e5",
   "3c7501c3-3a35-4924-a794-8bd1957738d0",
   "4ba3faf8-1e2e-48b9-820f-da711600b03d",
   "150b45b1-39c4-4fd2-af11-e7bd1a052af2",
   "330fb949-042d-410a-9170-b912c1a9a1d1",
   "b08f4ad4-5aeb-42de-9b91-9c6c2c5776ae",
   "23cb5739-f760-4711-8fe4-43183def03a3",
   "fd7105e4-c608-4471-b80d-3c5998005655",
   "873c3815-dc0b-4b48-a6e4-16aa2b00aed2",
   "a963be04-0b81-429f-92a9-b0853b3500f0",
   "c35ade5e-5f01-40f6-a4ca-4c174f6ecff2",
   "78c64ad4-24c7-457c-ab74-ab97a6e8b228"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
