{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b58914-fdf1-4423-b405-2d8b8f91bd1b",
   "metadata": {},
   "source": [
    "# Tiny but Mighty: Aspect-Based Classification with TinyBERT, Hugging Face Transformers, and PyTorch\n",
    "\n",
    "## 🧠 Introduction\n",
    "\n",
    "In this project, we address the task of **text classification** within the movie domain, using a dataset of reviews from IMDB. The main goal is to **predict the thematic aspect** (Story, Characters, or Cinematography) that each review focuses on.\n",
    "\n",
    "To achieve this, we implement two main approaches:\n",
    "\n",
    "1. A **simple neural network** with trainable embeddings from scratch, used as a baseline.\n",
    "2. A **pretrained TinyBERT model**, partially fine-tuned to our specific task.\n",
    "\n",
    "## 🎯 Objective\n",
    "\n",
    "- Build a model capable of automatically classifying movie reviews by thematic aspect.\n",
    "- Compare the performance of a simple neural model with a pretrained Transformer.\n",
    "- Extract insights from text analysis, including most frequent words.\n",
    "- Visualize model performance through graphs to gain a better understanding of the results.\n",
    "\n",
    "## 🔧 Methodology and Workflow\n",
    "\n",
    "1. **Data loading and cleaning**: Reading training and test datasets, basic exploratory analysis, and tokenization.\n",
    "2. **Preprocessing**: Vocabulary construction, encoding, and padding.\n",
    "3. **Baseline training**: Simple neural network with embedding layers.\n",
    "4. **Model evaluation**: Classification metrics and confusion matrix.\n",
    "5. **TinyBERT fine-tuning**: Freezing most layers and adjusting to our domain-specific task.\n",
    "6. **Performance comparison**: Visual analysis and summary of both approaches.\n",
    "\n",
    "This pipeline enables us to evaluate how both traditional and state-of-the-art NLP models perform on a practical classification task, giving us experience with both foundational and modern tools.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90f6ef-0659-4afb-8b6e-dc40266b82b1",
   "metadata": {},
   "source": [
    "**Setup - Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275582c5-c2ca-43ae-85f0-5cdf88b36fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error() # we remove warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5c280-304f-4038-8cb4-1c3c795d5fbd",
   "metadata": {},
   "source": [
    "## Import and Inspect the Movie Reviews\n",
    "\n",
    "Let's first import and inspect the datasets containing the movie reviews!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f1f258-9aaa-4cc8-9150-99f010d06b88",
   "metadata": {},
   "source": [
    "In this project, we'll be classifying different _aspects_ within movie reviews from IMDB (Internet Movie Database), which is a free online database containing data on movies, TV shows, and other types of media.\n",
    "\n",
    "We've obtained the movie reviews from this [Hugging Face Dataset](https://huggingface.co/datasets/Lowerated/lm6-movies-reviews-aspects) that we've cleaned and pre-processed into the following csv files:\n",
    "- `\"imdb_movie_reviews_train.csv\"` - contains movie reviews in the training dataset\n",
    "- `\"imdb_movie_reviews_test.csv\"` - contains the movie reviews in the testing dataset\n",
    "\n",
    "Here's a quick summary of the columns in the dataset:\n",
    "\n",
    "- `review` - the text of the movie review\n",
    "- `aspect` - the thematic aspect of the movie the review targets\n",
    "- `aspect_encoded` an integer label encoding the `aspect` column\n",
    "\n",
    "We can start by importing the CSV file to a pandas DataFrame named `train_reviews_df`.\n",
    "\n",
    "Then we use the `.head()`  and `.info()`method to preview the first five rows and some basic statics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7219599-d548-4d36-9a76-73b861f9c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review          aspect  \\\n",
      "0       Ibiza filming location looks very enchanting  Cinematography   \n",
      "1  RANDOLPH SCOTT always played men you could loo...      Characters   \n",
      "2  interesting and promising basic idea', 'some p...           Story   \n",
      "3  the film could explore very powerful politics,...           Story   \n",
      "4  The animation is nice, and the use of color ma...  Cinematography   \n",
      "\n",
      "   aspect_encoded  \n",
      "0               0  \n",
      "1               1  \n",
      "2               2  \n",
      "3               2  \n",
      "4               0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>aspect_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ibiza filming location looks very enchanting</td>\n",
       "      <td>Cinematography</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RANDOLPH SCOTT always played men you could loo...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interesting and promising basic idea', 'some p...</td>\n",
       "      <td>Story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the film could explore very powerful politics,...</td>\n",
       "      <td>Story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The animation is nice, and the use of color ma...</td>\n",
       "      <td>Cinematography</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>obviously shot on video tape', 'very poorly at...</td>\n",
       "      <td>Cinematography</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>it is humorous how bad it is', \"i've seen porn...</td>\n",
       "      <td>Story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>Everything that is seen or heard on the screen...</td>\n",
       "      <td>Cinematography</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>the actors delivered up to standard', 'only on...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>Tony Randall works for the IRS', 'They dote on...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review          aspect  \\\n",
       "0         Ibiza filming location looks very enchanting  Cinematography   \n",
       "1    RANDOLPH SCOTT always played men you could loo...      Characters   \n",
       "2    interesting and promising basic idea', 'some p...           Story   \n",
       "3    the film could explore very powerful politics,...           Story   \n",
       "4    The animation is nice, and the use of color ma...  Cinematography   \n",
       "..                                                 ...             ...   \n",
       "364  obviously shot on video tape', 'very poorly at...  Cinematography   \n",
       "365  it is humorous how bad it is', \"i've seen porn...           Story   \n",
       "366  Everything that is seen or heard on the screen...  Cinematography   \n",
       "367  the actors delivered up to standard', 'only on...      Characters   \n",
       "368  Tony Randall works for the IRS', 'They dote on...      Characters   \n",
       "\n",
       "     aspect_encoded  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 2  \n",
       "3                 2  \n",
       "4                 0  \n",
       "..              ...  \n",
       "364               0  \n",
       "365               2  \n",
       "366               0  \n",
       "367               1  \n",
       "368               1  \n",
       "\n",
       "[369 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_reviews_df = pd.read_csv('imdb_movie_reviews_train.csv')\n",
    "print(train_reviews_df.head())\n",
    "display(train_reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f06982-e313-482c-b4ac-f81dd9a27fcd",
   "metadata": {},
   "source": [
    "Next, use `.info()` to inspect the training dataset. Make sure the data types for each column make sense and if there are any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a502b157-10a8-49a8-a83a-2020388fb013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 369 entries, 0 to 368\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   review          369 non-null    object\n",
      " 1   aspect          369 non-null    object\n",
      " 2   aspect_encoded  369 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 8.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8b644-3296-46de-bb91-40a21b133480",
   "metadata": {},
   "source": [
    "Let's explore the different aspect labels in the training dataset.\n",
    "\n",
    "We can use the `.value_counts()` method on the `aspect` column to count the number of aspect labels.\n",
    "\n",
    "After that, we do the same for the `aspect_encoded` column to verify that the number of aspect labels correspond equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a940209-b9f2-4390-be70-fa8c04d4b04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect\n",
      "Cinematography    125\n",
      "Characters        123\n",
      "Story             121\n",
      "Name: count, dtype: int64\n",
      "aspect_encoded\n",
      "0    125\n",
      "1    123\n",
      "2    121\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews_df.aspect.value_counts())\n",
    "\n",
    "print(train_reviews_df.aspect_encoded.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cb954-2249-4b7f-a4b9-a0d5c5de77db",
   "metadata": {},
   "source": [
    "Here's a quick summary of each aspect:\n",
    "\n",
    "- **Cinematography** focuses on the movie's visual elements like the locations, quality, lighting, and visual appeal\n",
    "- **Characters** addresses the portrayal of characters and their development throughout the movie, which can focus on their acting, personality, depth, and relatability\n",
    "- **Story** highlights the movie's themes, plots, originality, and quality of storytelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be883d99-a973-423c-9766-399fb6519604",
   "metadata": {},
   "source": [
    "Since each movie review is classified into one of the three aspects, we'll frame this task as a **multi-class** classification task. \n",
    "\n",
    "Let's save the number of aspects to the variable `n_aspects`, which we'll reference later when we build the neural network and BERT transformer using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f168ad-fe32-4897-bd84-d9f2fa9a7d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "n_aspects = train_reviews_df['aspect'].nunique()\n",
    "print(n_aspects) # this value represents the number of classes for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993202c-4a55-4d7a-9358-8529c9a9ac2c",
   "metadata": {},
   "source": [
    "#### Chek the Plot for Distribution of Aspects in Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "499ddae9-9c42-4be8-bb71-33b035341d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='aspect', data=train_reviews_df, hue='aspect', palette='pastel', legend=False)\n",
    "plt.title('Distribution of Aspects in Reviews')\n",
    "plt.xlabel('Aspect')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e20b1cb-527e-4e57-988a-117055232e63",
   "metadata": {},
   "source": [
    "Next, create the _training corpus_ by using the `.tolist()` function to convert the text sequence in each review and their corresponding aspect labels to the following lists:\n",
    "\n",
    "- `train_texts` contains each movie review separated by commas\n",
    "- `train_labels` contains each review aspect separated by commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17fa105-cdbd-406a-8cdd-961019222cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_reviews_df['review'].tolist()\n",
    "train_labels = train_reviews_df['aspect_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a602e-8f34-4276-9f8d-617a3046e957",
   "metadata": {},
   "source": [
    "## Pre-processing the Text Data\n",
    "\n",
    "Next, we'll need to pre-process the text data into a numerical representation that our text classification model will understand!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7779c2-e0e1-4447-af48-c67dd3b58392",
   "metadata": {},
   "source": [
    "Next, let's **tokenize** each review into word tokens following this instructions:\n",
    "\n",
    "A. Create a function named `tokenize_review` that uses each review text as input.\n",
    "- with the `re` module, use the `re.findall()` function to lowercase the text using `.lower()` and locate words (without punctuations and special characters) using the regular expression `r'\\b\\w+\\b'`\n",
    "- save the tokenized text to the variable `tokenized_review_text` within the function\n",
    "- make sure the function returns the tokenized text\n",
    "\n",
    "B. Apply the function to tokenize each review text in the training corpus.\n",
    "- use a `for` loop to apply the `tokenize` function to each review text in `train_texts`\n",
    "- save the list of tokenized reviews to the variable `tokenized_corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581436b8-1aea-444a-b8db-d4442b5bbafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take a look of the first tokenized sentence ['ibiza', 'filming', 'location', 'looks', 'very', 'enchanting']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# create tokenize function\n",
    "def tokenize_review(text):\n",
    "    tokenized_review_text = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokenized_review_text\n",
    "\n",
    "# we will call the function using list comprehension\n",
    "tokenized_corpus = [tokenize_review(text) for text in train_texts]\n",
    "\n",
    "# show output and print the first tokenized text sequence to check everithon works porperly so far\n",
    "print(\"Take a look of the first tokenized sentence\", tokenized_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102c8eb-5e49-490c-8e5d-b99533b212dd",
   "metadata": {},
   "source": [
    "From the `collections` module, use the `Counter()` function to count the number of occurrences of each token and save the counts to the variable `word_freqs`\n",
    "\n",
    "- First, we'll need to combine each tokenized text sequence together into a single, long list of tokens of the full training corpus. We'll save the list to the variable `combined_corpus`.\n",
    "- Next, you'll need to create a _nested loop_ that first loops through each tokenized text sequence within the training corpus\n",
    "- Then, the second loop iterates through each token within each text sequence and appends each token to `combined_corpus`\n",
    "- Lastly, apply the `Counter()` function to `combined_corpus` to count the number of occurrences of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9974f3-595a-4ab5-8a6f-c7f229875ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0634d392-947d-4afe-bdf8-d32f0b6c3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# here is the nested loop and we will append it to our empty list\n",
    "combined_corpus = []\n",
    "\n",
    "for text in tokenized_corpus:\n",
    "    for token in text:\n",
    "        combined_corpus.append(token)\n",
    "\n",
    "word_freqs = Counter(combined_corpus)\n",
    "# print(word_freqs) WORKS FINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e3d75-135e-40f9-83c7-8bf8836b2b41",
   "metadata": {},
   "source": [
    "Use the `.most_common()` function to obtain the top 1000 most commonly occurring tokens.\n",
    "- Set the value `1000` to the variable `MAX_VOCAB_SIZE`\n",
    "- Save the top 1000 most commonly occurring tokens to the variable `most_common_words`\n",
    "\n",
    "Print out the top 10 most common words in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7128789-c86d-4b6b-bce7-7973cc522009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Common Words:  [('the', 732), ('a', 307), ('and', 306), ('of', 296), ('is', 218), ('to', 213), ('in', 177), ('it', 134), ('s', 109), ('that', 105)]\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "most_common_words = word_freqs.most_common(MAX_VOCAB_SIZE)\n",
    "\n",
    "print(\"Top 10 Most Common Words: \", most_common_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0be82013-d36f-4760-b125-86aa00517e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Plot: Top 20 Most Frequent Words\\nmost_common_df = pd.DataFrame(most_common_words, columns=['word', 'count'])\\n\\nplt.figure(figsize=(10, 5))\\nsns.barplot(data=most_common_df[:20], x='count', y='word', hue='count', palette='pastel', legend = False)\\nplt.title('Top 20 Most Frequent Words in the Corpus')\\nplt.xlabel('Frequency')\\nplt.ylabel('Word')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot: Top 20 Most Frequent Words\n",
    "most_common_df = pd.DataFrame(most_common_words, columns=['word', 'count'])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=most_common_df[:20], x='count', y='word', hue='count', palette='pastel', legend = False)\n",
    "plt.title('Top 20 Most Frequent Words in the Corpus')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02877c-4d85-4d3a-b09a-0e025f07609e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We create the vocabulary as a dictionary using the top 1000 most commonly occurring word tokens.\n",
    "- use the `enumerate` function to assign token IDs for each word token in `most_common_words` based on its positional value **starting at the value 2**\n",
    "- save the vocabulary to the variable `vocab`\n",
    "\n",
    "Also we add the special tokens `<unk>` and `<pad>` to the vocabulary by:\n",
    "- assigning the special token key `<unk>` with the token ID value `0`\n",
    "- assigning the special token key `<pad>` with the token ID value `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03088d1d-7c54-422e-a460-a75692fb4440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1002\n",
      "First 10 words in the vocabulary:  [('the', 2), ('a', 3), ('and', 4), ('of', 5), ('is', 6), ('to', 7), ('in', 8), ('it', 9), ('s', 10), ('that', 11)]\n"
     ]
    }
   ],
   "source": [
    "vocab = {word: idx + 2 for idx, (word, freq) in enumerate(most_common_words)}\n",
    "\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "\n",
    "# print the vocabulary size and the first 10 items\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "print(\"First 10 words in the vocabulary: \", list(vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d69cb5-0770-4c3e-83cd-738498f60fe4",
   "metadata": {},
   "source": [
    "Using the vocabulary, we can create a function named `encode_text` that tokenizes and encodes each review text into a sequence of token IDs and then we:\n",
    "\n",
    "- specify the inputs `text` and `vocab`\n",
    "    - `text` will be the raw review text to be tokenized and encoded\n",
    "    - `vocab` is the vocabulary\n",
    "- apply the `tokenize` function to the input `text`\n",
    "- encode the tokenized text into a sequence of token IDs\n",
    "    - assign the token ID value for the special token `<unk>` for word tokens that are not in the vocabulary\n",
    "- the function should return the encoded review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d54afa-cf5f-4608-9a9d-c00acdcfa65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying the encode:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12, 0, 6, 46, 17, 0, 88, 0, 2, 0, 0, 0, 8, 113, 378, 5, 0, 14, 117, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_text(text, vocab):\n",
    "    tokenized_text = tokenize_review(text)\n",
    "    encoded_text = [vocab.get(word, vocab['<unk>']) for word in tokenized_text]\n",
    "    return encoded_text\n",
    "\n",
    "#  examples texts\n",
    "text = \"Good programmers write code humans can understand\"\n",
    "text2 = \"This snetneces is just for testing how weel the encoer function perform in any kind of sentences i will porvide\"\n",
    "\n",
    "print(\"\\nTrying the encode:\")\n",
    "encode_text(text, vocab)\n",
    "encode_text(text2, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8759be0-71e0-408a-8aec-62d5e813d81b",
   "metadata": {},
   "source": [
    "Let's create another function named `pad_or_truncate` that pre-processes each encoded text to have the same length specified by a maximum length value.\n",
    "- specify the inputs `encoded_text` and `max_len`\n",
    "    - `encoded_text` will be the input encoded review text\n",
    "    - `max_len` is the specified maximum length value\n",
    "- use an `if` statement to identify reviews longer than the maximum length and returns the review truncated to the maximum length value\n",
    "- use an `else` statement to identify reviews shorter than the maximum length and returns the review padded with `1` values (corresponding to the token ID for the special token `<pad>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3db3bfec-6d34-4220-82ca-202af4028256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(encoded_text, max_len):\n",
    "    if len(encoded_text) > max_len:\n",
    "        return encoded_text[:max_len]\n",
    "    else:\n",
    "        return encoded_text + [vocab['<pad>']] * (max_len - len(encoded_text))\n",
    "        # return encoded_text + [1] * (max_len - len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7720614-300f-4608-a067-0a26faaf49a9",
   "metadata": {},
   "source": [
    "Now, let's fully tokenize and encode each review text by applying the `encode_text` and `pad_or_truncate` functions:\n",
    "- specify a maximum length of `128` for each review text sequence and save the value to the variable `MAX_SEQ_LENGTH`\n",
    "- tokenize and encode each review text as follows:\n",
    "    - use a `for` loop to iterate through each training review text in `train_texts`\n",
    "    - apply the `encode_text` function to each review text (be sure to specify the vocabulary)\n",
    "    - apply the `pad_or_truncate` function to each encoded text (be sure to specify the maximum length value)\n",
    "- save the fully pre-processed review text sequences to the variable `padded_text_seqs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c5c039-bdbb-494f-86a5-8947e7ee84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "padded_text_seqs = [pad_or_truncate(encode_text(text, vocab), max_len=MAX_SEQ_LENGTH) for text in train_texts]\n",
    "\n",
    "# print(padded_text_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b242c14-bbb0-49a2-a336-104601566aa1",
   "metadata": {},
   "source": [
    "Let's prepare our text sequences to for training by converting them into PyTorch tensors:\n",
    "\n",
    "- create the input tensor `X_tensor` by converting the padded/truncated sequences\n",
    "- create the label tensor `y_tensor` by converting the training review labels (be sure to specify the datatype `torch.long`)\n",
    "\n",
    "Using the PyTorch utility module `torch.utils.data`, let's organize the training input and label tensors into a single dataset object and an iterable that will allow us to load the training data in batches:\n",
    "- create the variable `train_dataset` using the `TensorDataset` utility class to organize the input tensor and label tensor into a single dataset object\n",
    "- create the variable `train_dataloader` using the `DataLoader` utility class to create an iterable that loads the `train_dataset` in batches of `16` (be sure to set `shuffle` to `True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72fcb32d-e5da-4522-b143-93f4c31a0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_tensor = torch.tensor(padded_text_seqs)\n",
    "y_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10404cf-c27f-412f-aca9-c7f3a4ac813a",
   "metadata": {},
   "source": [
    "## Training a Simple Neural Network\n",
    "\n",
    "The first text classification model we'll build and train to classify movie reviews is a **simple neural network with an embedding layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f761e-513c-4993-a57d-42f82f6e02c5",
   "metadata": {},
   "source": [
    "Let's construct the neural network architecture that will be trained to classify movie reviews!\n",
    "\n",
    "We create a class named `SimpleNNWithEmbedding` for our neural network (using the PyTorch's `nn.Module`) with the following:\n",
    "\n",
    "A. Create the `init` method that takes in the following attributes:\n",
    "- `vocab_size` is the number of tokens in the vocabulary \n",
    "- `embed_size` is the embedding size\n",
    "- `hidden_size` is the number of neurons in the linear layer\n",
    "- `output_size` is the number of output classes\n",
    "\n",
    "B. Build the `init` method by initializing the following layers:\n",
    "- `self.embedding` is an embedding layer that creates embeddings equal to the vocabulary size with embedding sizes specified by the `embed_size` input\n",
    "- `self.fc1` is the first linear layer with an input size equal to the embedding size in the embedding layer and an output size equal to the number of neurons in the hidden layer specified by the `hidden_size` input\n",
    "- `self.fc2` is the second linear layer with an input size equal to the hidden size of the first linear layer and an output size equal to the number of classes specified by the `output_size` input\n",
    "\n",
    "C. Build the `forward` method and create the forward operations in the following order:\n",
    "1. Start by passing the input `x` into the embedding layer\n",
    "2. Average the embeddings into a single representation\n",
    "3. Pass the averaged embedding into the first linear layer\n",
    "4. Apply the ReLU activation function \n",
    "5. Pass the activated output to the second linear layer\n",
    "6. Return the output of the second linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5557d49-7af6-4999-bdc2-e7d41b31d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(SimpleNNWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "              \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a4d0b-ebe2-48e3-afc9-1586229d93e8",
   "metadata": {},
   "source": [
    "Next, let's create an instance of our `SimpleNNWithEmbedding` neural network class.\n",
    "\n",
    "Start by defining the following variables:\n",
    "- `vocab_size` - integer containing the vocabulary size (vocabulary length)\n",
    "- `embed_size` - an embedding size of `50` dimensions for each token\n",
    "- `hidden_size` - specifies the hidden layer in the neural network with `100` neurons\n",
    "- `output_size` - specifies the number of class labels in our **multi-class** classification task\n",
    "    - hint: each review will be classified as one of the following aspects: Cinematography, Characters, or Story\n",
    " \n",
    "Instantiate the model with the variable parameters to the variable `text_classifier_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4df1e1a7-7232-4301-a5f8-6e16459e0490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lets  check and see our model:\n",
      " SimpleNNWithEmbedding(\n",
      "  (embedding): Embedding(1002, 50)\n",
      "  (fc1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 50\n",
    "hidden_size = 100\n",
    "output_size = 3 # because each review will be classified as one of the following aspects: Cinematography, Characters, or Story\n",
    "\n",
    "text_classifier_nn = SimpleNNWithEmbedding(vocab_size, embed_size, hidden_size, output_size)\n",
    "print(\"\\nLets  check and see our model:\\n\", text_classifier_nn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8df372-41f0-4fdb-9ebe-f977a32ec942",
   "metadata": {},
   "source": [
    "Next, let's initialize the **loss function** and the  **optimizer** for training:\n",
    "- create an instance of the **cross-entropy** loss function in PyTorch (from the `torch.nn` module) and save it to the variable `criterion`\n",
    "- create an instance of the **Adam** optimizer in PyTorch (from the `torch.optim` module) with a learning rate of **0.005** and save it to the variable `optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1926e79-6ac8-43a6-a0e5-e63315b09657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, text_classifier_nn.parameters()), lr=0.005) OTRA FORMA DE ESCRIBIRLO\n",
    "optimizer = optim.Adam(text_classifier_nn.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4ff2c-5dd1-495b-982a-d3ade22b8d18",
   "metadata": {},
   "source": [
    "Now let's train our neural network to classify movie reviews!\n",
    "\n",
    "A. Create a function named `train_model` that takes in the following inputs:\n",
    "- `model` is the text classification model\n",
    "- `train_loader` is the training data contained in a PyTorch `DataLoader` object\n",
    "- `criterion` is the loss function used to train the model\n",
    "- `optimizer` is the optimizer used to train the model\n",
    "- `num_epochs` is the number of training epochs\n",
    "\n",
    "B. In the function, create a `for` loop that loops through the number of epochs specified by the input `num_epochs`. Within the loop:\n",
    "- set the model to training mode\n",
    "- track the loss per epoch by initializing the variable `epoch_loss` to `0.0`\n",
    "\n",
    "C. Create a nested `for` loop within the first `for` loop such that:\n",
    "\n",
    "1. Loops through the inputs and labels of each training batch in the input `train_loader` \n",
    "2. Reset the gradients at each iteration\n",
    "3. Input the training batch through the forward pass\n",
    "4. Calculate the cross-entropy loss\n",
    "5. Backpropagate the loss through the network\n",
    "6. Adjust the weights and biases\n",
    "7. Update the total loss within the current training epoch\n",
    "\n",
    "D. Run the function to train the `text_classifier_nn` model on the training dataset stored in the iterable `train_dataloader` using the loss-function saved in `criterion` and the optimizer saved in `optimizer` for `50` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63f3bea0-ab65-468d-b968-5d3fcf8f6a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we will cal our function to see it in actionn\n",
      "\n",
      "[Epoch 5/50], Average CE Loss: 0.98320\n",
      "[Epoch 10/50], Average CE Loss: 0.74018\n",
      "[Epoch 15/50], Average CE Loss: 0.40746\n",
      "[Epoch 20/50], Average CE Loss: 0.15354\n",
      "[Epoch 25/50], Average CE Loss: 0.07105\n",
      "[Epoch 30/50], Average CE Loss: 0.03629\n",
      "[Epoch 35/50], Average CE Loss: 0.02796\n",
      "[Epoch 40/50], Average CE Loss: 0.01198\n",
      "[Epoch 45/50], Average CE Loss: 0.01633\n",
      "[Epoch 50/50], Average CE Loss: 0.01190\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"[Epoch {epoch + 1}/{num_epochs}], Average CE Loss: {avg_loss:.5f}\")\n",
    "        \n",
    "print(\"Now we will cal our function to see it in actionn\\n\")\n",
    "\n",
    "train_model(text_classifier_nn, train_dataloader, criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8b196-6de4-4d6d-9257-88adb9bb9a94",
   "metadata": {},
   "source": [
    "Let's evaluate the trained neural network on an out-of-sample testing dataset.\n",
    "\n",
    "Import the testing dataset stored in the CSV file `\"datasets/imdb_movie_reviews_test.csv\"` to a pandas DataFrame named `test_reviews_df`.\n",
    "\n",
    "Then, create the _testing corpus_ by using the `.tolist()` function to convert the text sequence in each review and their corresponding aspect labels to the following lists:\n",
    "\n",
    "- `test_texts` contains each movie review separated by commas\n",
    "- `test_labels` contains each review aspect separated by commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f61ab47-9a7a-41f2-99f4-4e513a46e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check test_reviews_df basic information:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>aspect_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the cinematography is commonplace it at least ...</td>\n",
       "      <td>Cinematography</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The cinematography is OK', 'there are some pre...</td>\n",
       "      <td>Cinematography</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the strangely annoying and unnecessary George ...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alyssa Milano', 'Tina Yothers', \"Edie McClurg ...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I loved the way the minor characters intervene...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>acting is bland and uninspired', 'completely l...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>himesh showing his generosity character throug...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>gorgeous and inventive camera work</td>\n",
       "      <td>Cinematography</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>The story was well knitted', 'Most of the conv...</td>\n",
       "      <td>Story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>The panel is funny and the contestants so far ...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review          aspect  \\\n",
       "0    the cinematography is commonplace it at least ...  Cinematography   \n",
       "1    The cinematography is OK', 'there are some pre...  Cinematography   \n",
       "2    the strangely annoying and unnecessary George ...      Characters   \n",
       "3    Alyssa Milano', 'Tina Yothers', \"Edie McClurg ...      Characters   \n",
       "4    I loved the way the minor characters intervene...      Characters   \n",
       "..                                                 ...             ...   \n",
       "127  acting is bland and uninspired', 'completely l...      Characters   \n",
       "128  himesh showing his generosity character throug...      Characters   \n",
       "129                 gorgeous and inventive camera work  Cinematography   \n",
       "130  The story was well knitted', 'Most of the conv...           Story   \n",
       "131  The panel is funny and the contestants so far ...      Characters   \n",
       "\n",
       "     aspect_encoded  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "..              ...  \n",
       "127               1  \n",
       "128               1  \n",
       "129               0  \n",
       "130               2  \n",
       "131               1  \n",
       "\n",
       "[132 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132 entries, 0 to 131\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   review          132 non-null    object\n",
      " 1   aspect          132 non-null    object\n",
      " 2   aspect_encoded  132 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "test_reviews_df = pd.read_csv('imdb_movie_reviews_test.csv')\n",
    "\n",
    "print(\"\\nCheck test_reviews_df basic information:\")\n",
    "display(test_reviews_df)\n",
    "print(test_reviews_df.info())\n",
    "\n",
    "test_texts = test_reviews_df['review'].tolist()\n",
    "test_labels = test_reviews_df['aspect_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590de0b-e30e-48b1-878d-541fe518cb0b",
   "metadata": {},
   "source": [
    "Let's pre-process (tokenize, encode, pad, and truncate) the text sequences in the testing dataset using the previous functions:\n",
    "- `encode_text` - uses the vocabulary to tokenize and encode text into token IDs\n",
    "- `pad_or_truncate` - pads or truncates the encoded text to a specified maximum sequence length\n",
    "\n",
    "Remember, we need to be sure to use the same maximum sequence length we used for the training dataset.\n",
    "\n",
    "Save the pre-processed test sequences to the variable `padded_text_seqs_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0692fdec-1946-412d-907f-93176f3fd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text_seqs_test = [pad_or_truncate(encode_text(test_seq, vocab), MAX_SEQ_LENGTH) for test_seq in test_texts]\n",
    "# print(padded_text_seqs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5996fb-3d53-4cb7-b6f9-589af49e55eb",
   "metadata": {},
   "source": [
    "Next, let's convert the pre-processed test sequences into PyTorch tensors:\n",
    "- create the input tensor `X_tensor_test` by converting the padded/truncated test sequences\n",
    "- create the label tensor `y_tensor_test` by converting the testing labels (be sure to specify thet datatype `torch.long`\n",
    "\n",
    "\n",
    "Using the PyTorch utility module `torch.utils.data`, let's organize the testing input and label tensors into a single dataset object and an iterable that will allow us to load the testing data in batches:\n",
    "- create the variable `test_dataset` using the `TensorDataset` utility class to organize the input tensor and label tensor into a single dataset object\n",
    "- create the variable `test_dataloader` using the `DataLoader` utility class to create an iterable that loads the `test_dataset` in batches of `8` (be sure to set `shuffle` to `False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d49c132a-ce3a-4309-80e3-26ef315bb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_test = torch.tensor(padded_text_seqs_test)\n",
    "y_tensor_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "test_dataset = TensorDataset(X_tensor_test, y_tensor_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4021084-f5da-46fc-9367-0a03e0138358",
   "metadata": {},
   "source": [
    "Let's generate predictions from the trained neural network on the reviews in the testing dataset!\n",
    "\n",
    "A. Create the function `get_predictions_and_probabilities` that takes in the trained model and testing dataloader as input.\n",
    "\n",
    "B. Within the function:\n",
    "- set the model to evaluation mode\n",
    "- initialize the following empty lists:\n",
    "    - `all_probs` - stores all of the predicted probabilities for the testing dataset\n",
    "    - `all_labels` - stores all of the predicted labels for the testing dataset\n",
    "\n",
    "- using `with torch.no_grad()`, loop through each batch in the testing dataloader and:\n",
    "    -  generate outputs from the forward pass\n",
    "    -  use the **softmax** function to generate predicted probabilities (be sure to add the probabilities to the `all_probs` list using `extend()`\n",
    "    -  use the **argmax** function to select the class label with the highest probabilities (be sure to add the labels to the `all_labels` list using `extend()`\n",
    "- the function should return the predicted probabilities `all_probs` and predicted labels `all_labels`\n",
    "\n",
    "C. Apply the `get_predictions_and_probabilities` to generate predictions. Save the predicted probabilities to the variable `pred_probs` and predicted labels to the variable `pred_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbe3fcaa-7c53-40df-b801-78469db1e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_predictions_and_probabilities(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            probs = F.softmax(outputs, dim=1)  \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            all_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "        return all_probs, all_labels\n",
    "\n",
    "pred_probs, pred_labels = get_predictions_and_probabilities(text_classifier_nn, test_loader)\n",
    "# print(pred_probs, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7edf8-386a-425c-af8f-b2c36669d223",
   "metadata": {},
   "source": [
    "Let's evaluate the model's predictions (`pred_labels`) with the true labels (`test_labels`).\n",
    "\n",
    "A. Generate a **confusion matrix** to count the number of true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP). Save the confusion matrix to the variable `conf_matrix`.\n",
    "\n",
    "B. Generate a **classification report** to calculate the accuracy, precision, recall, and F1-score metrics for each label. Save the classification report to the variable `report`.\n",
    "\n",
    "Print the confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbce5406-f843-40c3-9078-3e4e32c71955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix Results:\n",
      " [[27 18  4]\n",
      " [ 1 35  2]\n",
      " [ 2 15 28]]\n",
      "\n",
      "Report Result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.55      0.68        49\n",
      "           1       0.51      0.92      0.66        38\n",
      "           2       0.82      0.62      0.71        45\n",
      "\n",
      "    accuracy                           0.68       132\n",
      "   macro avg       0.75      0.70      0.68       132\n",
      "weighted avg       0.76      0.68      0.69       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "conf_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "print(\"\\nConfusion Matrix Results:\\n\", conf_matrix)\n",
    "\n",
    "report = classification_report(test_labels, pred_labels)\n",
    "print(\"\\nReport Result:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ed6e0-0ba9-44df-9d41-2eb9788b7655",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">How well does the simple neural network perform when classifying movie reviews?</summary>\n",
    "\n",
    "    \n",
    "**Confusion Matrix**\n",
    "\n",
    "We can interpret the confusion matrix by this structure:\n",
    "    \n",
    "```py\n",
    "    [[TP0 FN0  FN0]\n",
    "     [FN1 TP1  FN1]\n",
    "     [FN2 FN2  TP2]]\n",
    "```\n",
    "\n",
    "Each row corresponds to the TPs and FNs for each review aspect: \n",
    "    \n",
    "- row 1 corresponds to the TPs and FNs for Cinematography reviews (label 0)\n",
    "    - 27 Cinematography reviews were correctly classified as Cinematography reviews (label 0)\n",
    "    - 18 Cinematography reviews were incorrectly classified as Character reviews (label 1)\n",
    "    - 4 Cinematography reviews were incorrectly classified as Story reviews (label 2)\n",
    "- row 2 corresponds to the TPs and FNs for Character reviews (label 1)\n",
    "    - 35 Character reviews were correctly classified as Character reviews (label 1)\n",
    "    - 1 Character review was incorrectly classified as a Cinematography review (label 0)\n",
    "    - 2 Character reviews were incorrectly classified as Story reviews (label 2)\n",
    "- row 3 corresponds to the TPs and FNs for Story reviews (label 2)\n",
    "    - 28 Story reviews were correctly classified as Story reviews (label 2)\n",
    "    - 2 Story review was incorrectly classified as a Cinematography review (label 0)\n",
    "    - 15 Story reviews were incorrectly classified as Character reviews (label 1)    \n",
    "\n",
    "**Classification Report**\n",
    "\n",
    "When classifying movie reviews, the simple neural network (with an embedding layer) had an overall accuracy of 68%, which is decent but not too great. \n",
    "    \n",
    "For Cinematography reviews (label 0):\n",
    "- the model has a high precision score of 90%, meaning that when it classified a review as a Cinematography review, it was right 90% of the time\n",
    "- however, the model has a low recall score of 55%, meaning that it could only identify 55% of all Cinematography reviews, which suggests that it is overpredicting \n",
    "    \n",
    "For Character reviews (label 1):\n",
    "- the model has a low precision score of 51%, meaning that when it classified a review as a Character review, it was right 51% of the time (pretty much a 50/50 coin flip)\n",
    "- however, the model has a high recall score of 92%, meaning that it successfully identified 92% of all Character reviews\n",
    "- this suggests that the model rarely misses a Character review, but seems to misclassify the other review aspects often\n",
    "    \n",
    "For Story reviews (label 2):\n",
    "- the model has a decent precision score of 82%, meaning that when it classified a review as a Story review, it was right 82% of the time\n",
    "- however, the model has a low recall score of 62%, meaning that it could only identify 62% of all Story reviews\n",
    "    \n",
    "Overall, the model performs moderately well in classifying movie review aspects, but there is definitely room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934abae6-9de7-42b4-8c8b-5c0df23e84a6",
   "metadata": {},
   "source": [
    "## Fine-tuning a TinyBERT Transformer\n",
    "\n",
    "Let's now train a more advanced BERT transformer language model! Specifically, let's fine-tune a TinyBERT model to classify movie reviews! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da5a137d-a225-4586-8ef2-041468fbcd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x259347d89b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837afff7-9f73-4e2a-b538-4cf51b338805",
   "metadata": {},
   "source": [
    "Load the pre-trained BERT model using the `transformers` library from Hugging Face.\n",
    "\n",
    "Due to memory and hardware constraints, we won't be able to use the full pre-trained BERT model that contains 12 layers and 110 million parameters. Instead, we'll use a distilled, smaller version of BERT called **TinyBERT** that contains 4 layers and 14.5 million parameters developed by Huawei Noah's Ark Lab (https://huggingface.co/huawei-noah).\n",
    "\n",
    "A. First, save the TinyBERT model name `'huawei-noah/TinyBERT_General_4L_312D'` as a string to the variable `model_name`.\n",
    "\n",
    "B. Download and load the TinyBERT tokenizer from the `BertTokenizer` module and save the tokenizer to the variable `tinybert_tokenizer`.\n",
    "\n",
    "C. Download and load the TinyBERT models (weights) with the `BertForSequenceClassification` module. Be sure to specify `num_labels=` with the correct number of class labels (# of movie aspects). Save the TinyBERT model to the variable `text_classifier_tinybert`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fda1b125-2c19-4b9a-b5fb-84166724daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "tinybert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "text_classifier_tinybert = BertForSequenceClassification.from_pretrained(model_name, num_labels = n_aspects) # Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b100d-a34d-4ecf-91d4-f3318c3255e8",
   "metadata": {},
   "source": [
    "Before fine-tuning the TinyBERT, let's **freeze** and **unfreeze** the following layers:\n",
    "\n",
    "- first, freeze all of the parameters in the pre-trained TinyBERT\n",
    "- second, unfreeze the classification layer added on top of the pre-trained model\n",
    "- third, unfreeze the encoder layer specified at `layer[3]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1882aba4-632b-4043-bd85-47e13e4e47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the pre-trained layers\n",
    "for param in text_classifier_tinybert.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the feed-forward classification layer\n",
    "for param in text_classifier_tinybert.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze an encoder layer\n",
    "for param in text_classifier_tinybert.bert.encoder.layer[3].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3823a4-d934-411b-b375-63fcd4f7c6be",
   "metadata": {},
   "source": [
    "Let's pre-process the training text reviews (`train_texts`) with the TinyBERT tokenizer:\n",
    "- specify a maximum sequence length of `124` and save the value to the variable `MAX_SEQ_LENGTH_TINYBERT`\n",
    "- create the tensor `X_train` that contains the tokenized training text tokenized by the TinyBERT tokenizer:\n",
    "    - the sequences have a maximum length of **128** tokens\n",
    "    - the sequences are **padded**\n",
    "    - the sequences are **truncated**\n",
    "    - the padded and truncated sequences are returned as PyTorch tensors\n",
    "- create the tensor `y_train` that contains the corresponding training labels in `train_labels` converted to a PyTorch tensor (be sure to specify the `torch.long` datatype)\n",
    "\n",
    "Using the PyTorch utility module `torch.utils.data`, let's organize the training input and label tensors into a single dataset object and an iterable that will allow us to load the training data in batches:\n",
    "- create the variable `train_dataset` using the `TensorDataset` utility class to organize the input tensor `X_train` and label tensor `y_train` into a single dataset object (be sure to specify the attention mask in the input tensor created by the TinyBERT tokenizer)\n",
    "- create the variable `train_dataloader` using the `DataLoader` utility class to create an iterable that loads the `train_dataset` in batches of `16` (be sure to set `shuffle` to `True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7004e87-7a64-4507-8da5-6f914388a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH_TINYBERT = 124\n",
    "\n",
    "X_train = tinybert_tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length = MAX_SEQ_LENGTH_TINYBERT)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset # was already imported, buts as a remember\n",
    "train_dataset = TensorDataset(X_train['input_ids'], X_train['attention_mask'], y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd42e2-dbdc-4d9f-843d-4426a96ce30a",
   "metadata": {},
   "source": [
    "Next, let's initialize the optimizer and loss function to fine-tune our TinyBERT model (`text_classifier_tinybert`).\n",
    "\n",
    "A. Initialize the Adam optimizer with **weight decay** with a learning rate of `0.0025`. Ensure that only the unfrozen layers are optimized during training. Save the optimizer to the variable `optimizer`.\n",
    "\n",
    "B. Initialize the cross-entropy loss function. Save the loss function to the variable `criterion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "952d9821-1780-4d4c-82d6-1d3edb48fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn    # was already imported, buts as a remember\n",
    "import torch.optim as optim  # was already imported, buts as a remember\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, text_classifier_tinybert.parameters()), lr = 0.0025)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4dd75-1371-4757-bf65-79b350221a6c",
   "metadata": {},
   "source": [
    "Let's create a training loop to fine-tune the TinyBERT model for `10` epochs!\n",
    "\n",
    "A. Set the value of `10` to the variable `num_epochs`.\n",
    "\n",
    "B. Create a training loop first loops through each epoch where in each epoch:\n",
    "- set the TinyBERT model to training mode\n",
    "- initialize the empty variable `total_loss` with `0.0` to keep track of the total loss per epoch\n",
    "- create a nested loop that loops through each batch (be sure to specify the batch's attention mask) in the training dataloader where for each batch:\n",
    "    1. reset the gradients\n",
    "    2. apply the forward pass to the training batch (be sure to pass the current batch's attention mask)\n",
    "    3. extract the logits from the forward pass output\n",
    "    4. calculate the cross-entropy loss\n",
    "    5. update the `total_loss`\n",
    "    6. backpropagate the loss through the network\n",
    "    7. adjust the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0737017b-f254-4da6-a291-cd0ac2685be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9607686599095663\n",
      "Epoch 2, Loss: 0.5334098692983389\n",
      "Epoch 3, Loss: 0.3706777536620696\n",
      "Epoch 4, Loss: 0.42953731181720894\n",
      "Epoch 5, Loss: 0.3311362837751706\n",
      "Epoch 6, Loss: 0.25495298237850267\n",
      "Epoch 7, Loss: 0.2601506874586145\n",
      "Epoch 8, Loss: 0.3355211243033409\n",
      "Epoch 9, Loss: 0.2327814206170539\n",
      "Epoch 10, Loss: 0.17160291612769166\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    text_classifier_tinybert.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_attention_mask, batch_y in train_dataloader:  # es train_loader o train_dataloader??\n",
    "        optimizer.zero_grad()\n",
    "        outputs = text_classifier_tinybert(input_ids=batch_X, attention_mask=batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, batch_y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ee652-d91c-464d-ae02-4adc1a22554e",
   "metadata": {},
   "source": [
    "Now that the TinyBERT model is fine-tuned, let's evaluate how the transformer performs on the movie reviews in the testing set and compare its performance with the simple neural network from before!\n",
    "\n",
    "Pre-process the testing dataset into the following PyTorch tensors:\n",
    "\n",
    "- create the tensor `X_test` using the TinyBERT tokenizer to tokenize and testing dataset and save the pre-processed tensor to the variable `X_test`:\n",
    "    - the maximum sequence length should be same as the length used during fine-tuning the TinyBERT\n",
    "    - apply padding and truncating\n",
    "    - return the tokenized text as a PyTorch tensor\n",
    "- create the tensor `y_test` that contains the testing labels `test_labels` as a PyTorch tensor (be sure to specify the `torch.long` datatype).\n",
    "\n",
    "Using the PyTorch utility module `torch.utils.data`, let's organize the training input and label tensors into a single dataset object and an iterable that will allow us to load the training data in batches:\n",
    "- create the variable `test_dataset` using the `TensorDataset` utility class to organize the input tensor `X_test` and label tensor `y_test` into a single dataset object (be sure to specify the attention mask in the input tensor created by the TinyBERT tokenizer)\n",
    "- create the variable `test_dataloader` using the `DataLoader` utility class to create an iterable that loads the `test_dataset` in batches of `8` (be sure to set `shuffle` to `False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a7403a5-7c72-47dc-9444-2314cecf16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tinybert_tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_SEQ_LENGTH_TINYBERT)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset # was already imported, buts as a remember\n",
    "\n",
    "test_dataset = TensorDataset(X_test['input_ids'], X_test['attention_mask'], y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31d98a-b28a-46e2-9b6b-b52559e8f58d",
   "metadata": {},
   "source": [
    "Next, let's generate predictions from the fine-tuned TinyBERT!\n",
    "\n",
    "A. Set the fine-tuned TinyBERT model to evaluation mode.\n",
    "\n",
    "B. Initialize the following empty lists:\n",
    "- `pred_probs` - stores the predicted probabilities for the reviews in the testing dataset\n",
    "- `pred_labels` - stores the predicted labels for the reviews in the testing dataset\n",
    "\n",
    "C. Within `with torch.no_grad()`:\n",
    "- loop through each batch in the testing dataloader (be sure to specify the attention masks)\n",
    "- apply the forward pass to each batch\n",
    "- obtain the logits from the outputs from the forward pass\n",
    "-  use the **softmax** function to generate predicted probabilities (be sure to add the probabilities to the `pred_probs` list using `extend()`\n",
    "-  use the **argmax** function to select the class label with the highest probabilities (be sure to add the labels to the `pred_labels` list using `extend()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "336a679d-ab59-4632-a7db-5f746955e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "text_classifier_tinybert.eval() # this put our model in evaluation mode\n",
    "\n",
    "pred_probs = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_attention_mask, batch_y in test_dataloader:\n",
    "        outputs = text_classifier_tinybert(input_ids= batch_X, attention_mask= batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        pred_labels.extend(predicted_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48957b-4d22-40da-8a5f-921fe3a5fc7c",
   "metadata": {},
   "source": [
    "Let's see how well our fine-tuned TinyBERT model classifies movie reviews!\n",
    "\n",
    "Evaluate the TinyBERT model's predictions (`pred_labels`) with the true labels (`test_labels`) with the following:\n",
    "\n",
    "A. Generate a **confusion matrix** to count the number of true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP). Save the confusion matrix to the variable `conf_matrix`.\n",
    "\n",
    "B. Generate a **classification report** to calculate the accuracy, precision, recall, and F1-score metrics for each label. Save the classification report to the variable `report`.\n",
    "\n",
    "Print the confusion matrix and classification report.\n",
    "\n",
    "How does the fine-tuned TinyBERT model performance compare to the simple neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb920469-ad9c-4010-ade7-f46a201aaf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  0  0]\n",
      " [ 2 32  4]\n",
      " [ 4  4 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        49\n",
      "           1       0.89      0.84      0.86        38\n",
      "           2       0.90      0.82      0.86        45\n",
      "\n",
      "    accuracy                           0.89       132\n",
      "   macro avg       0.89      0.89      0.89       132\n",
      "weighted avg       0.89      0.89      0.89       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "print(confusion_matrix)\n",
    "\n",
    "report = classification_report(test_labels, pred_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d1b3b-9922-449c-ae53-bb07b828237c",
   "metadata": {},
   "source": [
    "#### Let´s  plot the Confusion Matrix for our TinyBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2666a429-3651-4bd4-889f-bf12f35a5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlabels = train_reviews_df['aspect'].unique()\\n\\nplt.figure(figsize=(6, 5))\\nsns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', \\n            xticklabels=labels, yticklabels=labels)\\nplt.title('Confusion Matrix - TinyBERT Model')\\nplt.xlabel('Predicted Label')\\nplt.ylabel('True Label')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "labels = train_reviews_df['aspect'].unique()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix - TinyBERT Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a972f76-4413-477c-b58b-9e55458ec412",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">How does the fine-tuned TinyBERT model performance compare to the simple neural network?</summary>\n",
    "   \n",
    "**Confusion Matrix**\n",
    "\n",
    "We can interpret the confusion matrix by this structure:\n",
    "    \n",
    "```py\n",
    "    [[TP0 FN0  FN0]\n",
    "     [FN1 TP1  FN1]\n",
    "     [FN2 FN2  TP2]]\n",
    "```\n",
    "\n",
    "Each row corresponds to the TPs and FNs for each review aspect: \n",
    "    \n",
    "- row 1 corresponds to the TPs and FNs for Cinematography reviews (label 0)\n",
    "    - 49 Cinematography reviews were correctly classified as Cinematography reviews (label 0)\n",
    "    - 0 Cinematography reviews were incorrectly classified as Character reviews (label 1)\n",
    "    - 0 Cinematography reviews were incorrectly classified as Story reviews (label 2)\n",
    "- row 2 corresponds to the TPs and FNs for Character reviews (label 1)\n",
    "    - 32 Character reviews were correctly classified as Character reviews (label 1)\n",
    "    - 2 Character review was incorrectly classified as a Cinematography review (label 0)\n",
    "    - 4 Character reviews were incorrectly classified as Story reviews (label 2)\n",
    "- row 3 corresponds to the TPs and FNs for Story reviews (label 2)\n",
    "    - 37 Story reviews were correctly classified as Story reviews (label 2)\n",
    "    - 4 Story review was incorrectly classified as a Cinematography review (label 0)\n",
    "    - 4 Story reviews were incorrectly classified as Character reviews (label 1)    \n",
    "\n",
    "**Classification Report**\n",
    "\n",
    "When classifying movie reviews, the fine-tuned TinyBERT achieved an overall accuracy of 89%, which outperformed the simple neural network that had an overall accuracy of 68%. \n",
    "    \n",
    "For Cinematography reviews (label 0):\n",
    "- the model has a high precision score of 89%, meaning that when it classified a review as a Cinematography review, it was right 89% of the time\n",
    "- the model has a perfect recall score of 100%, meaning that it correctly identified all Cinematography reviews\n",
    "    \n",
    "For Character reviews (label 1):\n",
    "- the model has a high precision score of 89%, meaning that when it classified a review as a Character review, it was right 89% of the time\n",
    "- the model has a decent recall score of 84%, meaning that it correctly identified most of all Character reviews\n",
    "    \n",
    "For Story reviews (label 2):\n",
    "- the model has a perfect precision score of 89%, meaning that when it classified a review as a Story review, it was right 89% of the time\n",
    "- the model has a decent recall score of 89%, meaning that it identified 89% of all Story reviews\n",
    "\n",
    "    \n",
    "The macro and weighted average F1 Score of almost 90% tell us that the model performs well across all label aspects.\n",
    "    \n",
    "Overall, the model performs exceptionally well in classifying movie review aspects, confirming that fine-tuning a TinyBERT model outperforms a simple neural network for this text classification task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a43fd2-880d-4b00-a509-1f555501da06",
   "metadata": {},
   "source": [
    "That's the end of our project on building text classification models to classify different aspects of movie reviews! \n",
    "\n",
    "Here are some areas for improvement:\n",
    "- increase the maximum sequence length\n",
    "- increase the training batch sizes\n",
    "- freeze/unfreeze different layers in the TinyBERT architecture\n",
    "- increase the number of training epochs\n",
    "- try different optimizers and learning rates\n",
    "- try a different transformer model\n",
    "- increase the size of training dataset ([Full Hugging Face Dataset](https://huggingface.co/datasets/Lowerated/lm6-movies-reviews-aspects))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc914e9-3728-45f5-b9e7-f5836fd65a8f",
   "metadata": {},
   "source": [
    "# 📊 Conclusion and Results\n",
    "\n",
    "##### After training and comparing two distinct approaches for text classification on IMDB movie reviews, we reached the following conclusions:\n",
    "\n",
    "## 🔍 Key Findings\n",
    "\n",
    "- **The TinyBERT model significantly outperformed the simple neural network**, thanks to its prior training on a massive corpus and its ability to understand contextual word relationships.\n",
    "- The dataset is relatively balanced across classes, which simplifies the learning process without requiring additional balancing techniques.\n",
    "- The most frequent words are domain-relevant (e.g., film-related), confirming the quality and consistency of the corpus.\n",
    "\n",
    "## 📈 Model Performance\n",
    "\n",
    "- The simple neural network achieved modest performance, serving as a useful initial baseline.\n",
    "- TinyBERT achieved **high precision, recall, and F1 scores**, particularly in identifying \"Story\" and \"Characters\" aspects.\n",
    "- The confusion matrix shows a **clear improvement in class distinction** when using a Transformer-based model.\n",
    "\n",
    "## 🚀 Future Work\n",
    "\n",
    "- Apply data augmentation techniques to expand the training set.\n",
    "- Experiment with more robust models like DistilBERT or full BERT-base.\n",
    "- Deploy the model using an inference API (e.g., with FastAPI or Flask).\n",
    "\n",
    "This project demonstrates the strength of **Transformer-based models** in Natural Language Processing and contributes to the development of core skills in text classification, embeddings, and transfer learning with pretrained language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e4745-6960-49f5-a732-b528792c7fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
